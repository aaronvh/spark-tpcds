2018-10-08 00:40:12,908 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test' ...
2018-10-08 00:40:12,922 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:40:13,129 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:40:13,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:40:14,031 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:40:14,032 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test'.
2018-10-08 00:40:15,101 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test1' ...
2018-10-08 00:40:15,109 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:40:15,182 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:40:15,556 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:40:15,846 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:40:15,846 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test1'.
2018-10-08 00:40:16,679 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test2' ...
2018-10-08 00:40:16,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '2' ... 
2018-10-08 00:40:16,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-08 00:40:16,981 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '2'.
2018-10-08 00:40:16,981 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test2'.
2018-10-08 00:40:18,349 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:40:19,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:40:21,306 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:40:21,976 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:40:21,983 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:40:22,040 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:40:22,317 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:40:22,488 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:40:22,488 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:40:23,171 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:40:23,176 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:40:23,229 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:40:23,526 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:40:23,723 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:40:23,724 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:40:26,241 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:40:26,241 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-08 00:40:26,242 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,242 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,242 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,242 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,243 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,244 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,244 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,244 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,244 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,244 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,244 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,246 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,250 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,251 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,252 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,253 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,254 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:40:26,255 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:40:26,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test5' ...
2018-10-08 00:40:26,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:40:26,917 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:40:27,164 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:40:27,344 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:40:27,344 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test5'.
2018-10-08 00:40:28,030 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test6' ...
2018-10-08 00:40:28,036 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '2' ... 
2018-10-08 00:40:28,086 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-08 00:40:28,290 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '2'.
2018-10-08 00:40:28,290 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test6'.
2018-10-08 00:40:30,346 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:40:30,346 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:60]   No result found in run 2.
2018-10-08 00:40:30,346 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 2
2018-10-08 00:40:30,346 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:59]   No result found in run 1.
2018-10-08 00:41:05,201 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test' ...
2018-10-08 00:41:05,214 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:41:05,392 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:41:05,989 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:41:06,258 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:41:06,259 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test'.
2018-10-08 00:41:07,360 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test1' ...
2018-10-08 00:41:07,368 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:41:07,439 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:41:07,796 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:41:08,028 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:41:08,029 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test1'.
2018-10-08 00:41:08,855 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test2' ...
2018-10-08 00:41:08,863 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '2' ... 
2018-10-08 00:41:08,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-08 00:41:09,166 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '2'.
2018-10-08 00:41:09,166 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test2'.
2018-10-08 00:41:10,648 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:41:12,315 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:41:13,801 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:41:14,453 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:41:14,458 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:41:14,507 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:41:14,848 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:41:15,039 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:41:15,039 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:41:15,724 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:41:15,728 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:41:15,775 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:41:16,040 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:41:16,227 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:41:16,228 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:41:18,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:41:18,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-08 00:41:18,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:41:18,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,765 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,765 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,765 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,765 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:18,765 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:41:18,766 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:41:18,766 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:41:18,766 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:41:19,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test5' ...
2018-10-08 00:41:19,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:41:19,447 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:41:19,698 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:41:19,899 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:41:19,899 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test5'.
2018-10-08 00:41:20,533 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test6' ...
2018-10-08 00:41:20,538 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '2' ... 
2018-10-08 00:41:20,597 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-08 00:41:20,784 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '2'.
2018-10-08 00:41:20,784 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test6'.
2018-10-08 00:41:22,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:41:22,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:60]   No result found in run 2.
2018-10-08 00:41:22,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 2
2018-10-08 00:41:22,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:59]   No result found in run 1.
2018-10-08 00:43:28,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test' ...
2018-10-08 00:43:28,780 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:43:29,007 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:43:29,659 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:43:29,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:43:29,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test'.
2018-10-08 00:43:31,113 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test1' ...
2018-10-08 00:43:31,120 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:43:31,189 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:43:31,575 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:43:31,816 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:43:31,816 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test1'.
2018-10-08 00:43:32,626 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test2' ...
2018-10-08 00:43:32,631 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '2' ... 
2018-10-08 00:43:32,712 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-08 00:43:32,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '2'.
2018-10-08 00:43:32,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test2'.
2018-10-08 00:43:34,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:43:36,049 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:43:37,630 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-08 00:43:38,333 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:43:38,339 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:43:38,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:43:38,717 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:43:38,903 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:43:38,903 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:43:39,545 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:43:39,550 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:43:39,603 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:43:39,889 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:43:40,116 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:43:40,117 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:43:42,862 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:43:42,862 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-08 00:43:42,863 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,863 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,863 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,865 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,865 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,865 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,865 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,865 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,865 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,866 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,866 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,866 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,866 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,866 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,868 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,868 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,868 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,868 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,868 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,869 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,869 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,869 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,869 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,869 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:43:42,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,872 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:43:42,872 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,872 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,872 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,872 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,873 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,873 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,873 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-08 00:43:42,873 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,873 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,873 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,874 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,877 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:42,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-08 00:43:42,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-08 00:43:42,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-08 00:43:42,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-08 00:43:43,503 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test5' ...
2018-10-08 00:43:43,509 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:43:43,560 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:43:43,821 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:43:44,038 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:43:44,038 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test5'.
2018-10-08 00:43:44,664 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test6' ...
2018-10-08 00:43:44,668 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '2' ... 
2018-10-08 00:43:44,714 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-08 00:43:44,889 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '2'.
2018-10-08 00:43:44,890 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test6'.
2018-10-08 00:43:47,044 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:43:47,044 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:60]   No result found in run 2.
2018-10-08 00:43:47,044 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 2
2018-10-08 00:43:47,045 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:59]   No result found in run 1.
2018-10-08 00:45:05,763 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:45:05,775 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:45:06,012 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:45:06,802 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:45:07,128 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:45:07,128 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:45:08,212 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:45:08,220 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:45:08,297 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:45:08,712 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:45:08,983 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:45:08,983 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:45:13,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:45:13,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-08 00:48:35,914 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:48:35,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:49:34,261 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:49:51,062 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:49:53,147 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:49:53,148 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:49:54,175 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:49:54,182 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:50:04,913 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:50:06,909 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:50:07,906 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:50:07,906 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:50:12,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:50:12,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-08 00:50:50,308 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:50:50,322 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:51:25,289 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:51:32,034 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:51:33,266 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:51:33,266 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:51:34,268 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:51:34,275 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:51:49,072 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:51:52,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:51:53,477 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:51:53,477 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:51:58,183 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:51:58,185 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-08 00:52:17,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-08 00:52:17,937 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:52:54,819 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:53:08,526 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:53:09,785 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:53:09,786 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-08 00:53:10,729 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-08 00:53:10,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Run query '1' ... 
2018-10-08 00:53:22,547 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-08 00:53:26,419 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-08 00:53:27,378 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:21] Finished query '1'.
2018-10-08 00:53:27,378 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-08 00:53:32,036 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-08 00:53:32,038 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 21:35:55,520 WARN [ScalaTest-run] o.a.s.u.Utils [Logging.scala:66] Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.0.206 instead (on interface wlo1)
2018-10-11 21:35:55,537 WARN [ScalaTest-run] o.a.s.u.Utils [Logging.scala:66] Set SPARK_LOCAL_IP if you need to bind to another address
2018-10-11 21:36:00,605 INFO [ScalaTest-run] o.a.s.SparkContext [Logging.scala:54] Running Spark version 2.3.1
2018-10-11 21:36:00,765 WARN [ScalaTest-run] o.a.h.u.NativeCodeLoader [NativeCodeLoader.java:62] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-10-11 21:36:00,880 INFO [ScalaTest-run] o.a.s.SparkContext [Logging.scala:54] Submitted application: SparkPerformanceTester
2018-10-11 21:36:00,948 INFO [ScalaTest-run] o.a.s.SecurityManager [Logging.scala:54] Changing view acls to: aaronvanhecken
2018-10-11 21:36:00,949 INFO [ScalaTest-run] o.a.s.SecurityManager [Logging.scala:54] Changing modify acls to: aaronvanhecken
2018-10-11 21:36:00,950 INFO [ScalaTest-run] o.a.s.SecurityManager [Logging.scala:54] Changing view acls groups to: 
2018-10-11 21:36:00,950 INFO [ScalaTest-run] o.a.s.SecurityManager [Logging.scala:54] Changing modify acls groups to: 
2018-10-11 21:36:00,951 INFO [ScalaTest-run] o.a.s.SecurityManager [Logging.scala:54] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaronvanhecken); groups with view permissions: Set(); users  with modify permissions: Set(aaronvanhecken); groups with modify permissions: Set()
2018-10-11 21:36:01,162 INFO [ScalaTest-run] o.a.s.u.Utils [Logging.scala:54] Successfully started service 'sparkDriver' on port 42563.
2018-10-11 21:36:01,190 INFO [ScalaTest-run] o.a.s.SparkEnv [Logging.scala:54] Registering MapOutputTracker
2018-10-11 21:36:01,207 INFO [ScalaTest-run] o.a.s.SparkEnv [Logging.scala:54] Registering BlockManagerMaster
2018-10-11 21:36:01,210 INFO [ScalaTest-run] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-10-11 21:36:01,210 INFO [ScalaTest-run] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] BlockManagerMasterEndpoint up
2018-10-11 21:36:01,225 INFO [ScalaTest-run] o.a.s.s.DiskBlockManager [Logging.scala:54] Created local directory at /tmp/blockmgr-c3d48550-eebc-4167-af06-b06e89c928b6
2018-10-11 21:36:01,241 INFO [ScalaTest-run] o.a.s.s.m.MemoryStore [Logging.scala:54] MemoryStore started with capacity 1391.4 MB
2018-10-11 21:36:01,254 INFO [ScalaTest-run] o.a.s.SparkEnv [Logging.scala:54] Registering OutputCommitCoordinator
2018-10-11 21:36:01,340 INFO [ScalaTest-run] o.s.j.u.log [Log.java:192] Logging initialized @7275ms
2018-10-11 21:36:01,402 INFO [ScalaTest-run] o.s.j.s.Server [Server.java:346] jetty-9.3.z-SNAPSHOT
2018-10-11 21:36:01,423 INFO [ScalaTest-run] o.s.j.s.Server [Server.java:414] Started @7358ms
2018-10-11 21:36:01,443 INFO [ScalaTest-run] o.s.j.s.AbstractConnector [AbstractConnector.java:278] Started ServerConnector@4a183d02{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-10-11 21:36:01,444 INFO [ScalaTest-run] o.a.s.u.Utils [Logging.scala:54] Successfully started service 'SparkUI' on port 4040.
2018-10-11 21:36:01,474 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@26a94fa5{/jobs,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,476 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@5eccd3b9{/jobs/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,477 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@4d6f197e{/jobs/job,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,484 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@5c089b2f{/jobs/job/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,486 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@6999cd39{/stages,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,488 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@14bae047{/stages/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,490 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@7ed9ae94{/stages/stage,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,492 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@2bc12da{/stages/stage/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,493 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@3122b117{/stages/pool,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,494 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@534ca02b{/stages/pool/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,495 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@29a23c3d{/storage,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,497 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@4b6ac111{/storage/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,498 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@6fe46b62{/storage/rdd,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,499 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@591fd34d{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,500 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@61e45f87{/environment,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,502 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@7c9b78e3{/environment/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,503 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@3068b369{/executors,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,504 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@17ca8b92{/executors/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,505 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@5491f68b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,506 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@736ac09a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,514 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@6ecd665{/static,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,516 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@bcb09a6{/,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,518 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@7c2a69b4{/api,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,519 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@1813f3e9{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,520 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@28cb9120{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-10-11 21:36:01,522 INFO [ScalaTest-run] o.a.s.u.SparkUI [Logging.scala:54] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.206:4040
2018-10-11 21:36:01,646 INFO [ScalaTest-run] o.a.s.e.Executor [Logging.scala:54] Starting executor ID driver on host localhost
2018-10-11 21:36:01,682 INFO [ScalaTest-run] o.a.s.u.Utils [Logging.scala:54] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34485.
2018-10-11 21:36:01,683 INFO [ScalaTest-run] o.a.s.n.n.NettyBlockTransferService [Logging.scala:54] Server created on 192.168.0.206:34485
2018-10-11 21:36:01,685 INFO [ScalaTest-run] o.a.s.s.BlockManager [Logging.scala:54] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-10-11 21:36:01,721 INFO [ScalaTest-run] o.a.s.s.BlockManagerMaster [Logging.scala:54] Registering BlockManager BlockManagerId(driver, 192.168.0.206, 34485, None)
2018-10-11 21:36:01,725 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerMasterEndpoint [Logging.scala:54] Registering block manager 192.168.0.206:34485 with 1391.4 MB RAM, BlockManagerId(driver, 192.168.0.206, 34485, None)
2018-10-11 21:36:01,730 INFO [ScalaTest-run] o.a.s.s.BlockManagerMaster [Logging.scala:54] Registered BlockManager BlockManagerId(driver, 192.168.0.206, 34485, None)
2018-10-11 21:36:01,731 INFO [ScalaTest-run] o.a.s.s.BlockManager [Logging.scala:54] Initialized BlockManager: BlockManagerId(driver, 192.168.0.206, 34485, None)
2018-10-11 21:36:01,932 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@48535004{/metrics/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:02,088 INFO [ScalaTest-run] o.a.s.s.i.SharedState [Logging.scala:54] loading hive config file: file:/home/aaronvanhecken/tpc-ds/target/scala-2.11/test-classes/hive-site.xml
2018-10-11 21:36:02,113 INFO [ScalaTest-run] o.a.s.s.i.SharedState [Logging.scala:54] spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').
2018-10-11 21:36:02,114 INFO [ScalaTest-run] o.a.s.s.i.SharedState [Logging.scala:54] Warehouse path is '/user/hive/warehouse'.
2018-10-11 21:36:02,122 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@7c1c5936{/SQL,null,AVAILABLE,@Spark}
2018-10-11 21:36:02,123 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@7aead3af{/SQL/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:02,124 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@37ce3644{/SQL/execution,null,AVAILABLE,@Spark}
2018-10-11 21:36:02,125 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@4b869331{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-10-11 21:36:02,127 INFO [ScalaTest-run] o.s.j.s.h.ContextHandler [ContextHandler.java:781] Started o.s.j.s.ServletContextHandler@15d0849{/static/sql,null,AVAILABLE,@Spark}
2018-10-11 21:36:02,639 INFO [ScalaTest-run] o.a.s.s.e.s.s.StateStoreCoordinatorRef [Logging.scala:54] Registered StateStoreCoordinator endpoint
2018-10-11 21:36:03,032 INFO [ScalaTest-run] o.a.s.s.h.HiveUtils [Logging.scala:54] Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-10-11 21:36:03,726 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:589] 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2018-10-11 21:36:03,748 INFO [ScalaTest-run] o.a.h.h.m.ObjectStore [ObjectStore.java:289] ObjectStore, initialize called
2018-10-11 21:36:04,841 INFO [ScalaTest-run] o.a.h.h.m.ObjectStore [ObjectStore.java:370] Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2018-10-11 21:36:05,745 INFO [ScalaTest-run] o.a.h.h.m.MetaStoreDirectSql [MetaStoreDirectSql.java:139] Using direct SQL, underlying DB is MYSQL
2018-10-11 21:36:05,751 INFO [ScalaTest-run] o.a.h.h.m.ObjectStore [ObjectStore.java:272] Initialized ObjectStore
2018-10-11 21:36:06,061 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:663] Added admin role in metastore
2018-10-11 21:36:06,063 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:672] Added public role in metastore
2018-10-11 21:36:06,111 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:712] No user is added in admin role, since config is empty
2018-10-11 21:36:06,200 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_all_databases
2018-10-11 21:36:06,201 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_all_databases	
2018-10-11 21:36:06,214 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_functions: db=default pat=*
2018-10-11 21:36:06,214 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2018-10-11 21:36:06,260 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_functions: db=tpcds pat=*
2018-10-11 21:36:06,260 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_functions: db=tpcds pat=*	
2018-10-11 21:36:06,289 INFO [ScalaTest-run] o.a.h.h.q.s.SessionState [SessionState.java:641] Created local directory: /tmp/43a7fc81-8758-4306-bb3f-50ad4af96a81_resources
2018-10-11 21:36:06,294 INFO [ScalaTest-run] o.a.h.h.q.s.SessionState [SessionState.java:641] Created HDFS directory: /tmp/hive/aaronvanhecken/43a7fc81-8758-4306-bb3f-50ad4af96a81
2018-10-11 21:36:06,299 INFO [ScalaTest-run] o.a.h.h.q.s.SessionState [SessionState.java:641] Created local directory: /tmp/aaronvanhecken/43a7fc81-8758-4306-bb3f-50ad4af96a81
2018-10-11 21:36:06,303 INFO [ScalaTest-run] o.a.h.h.q.s.SessionState [SessionState.java:641] Created HDFS directory: /tmp/hive/aaronvanhecken/43a7fc81-8758-4306-bb3f-50ad4af96a81/_tmp_space.db
2018-10-11 21:36:06,306 INFO [ScalaTest-run] o.a.s.s.h.c.HiveClientImpl [Logging.scala:54] Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
2018-10-11 21:36:06,317 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: default
2018-10-11 21:36:06,318 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: default	
2018-10-11 21:36:06,323 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: global_temp
2018-10-11 21:36:06,324 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: global_temp	
2018-10-11 21:36:06,327 WARN [ScalaTest-run] o.a.h.h.m.ObjectStore [ObjectStore.java:568] Failed to get database global_temp, returning NoSuchObjectException
2018-10-11 21:36:07,634 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: create_database: Database(name:tpcds, description:, locationUri:file:/user/hive/warehouse/tpcds.db, parameters:{})
2018-10-11 21:36:07,635 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=create_database: Database(name:tpcds, description:, locationUri:file:/user/hive/warehouse/tpcds.db, parameters:{})	
2018-10-11 21:36:07,642 ERROR [ScalaTest-run] o.a.h.h.m.RetryingHMSHandler [RetryingHMSHandler.java:159] AlreadyExistsException(message:Database tpcds already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy17.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy18.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:303)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:272)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:210)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:209)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:255)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:302)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:164)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateDatabase$1.apply(HiveExternalCatalog.scala:164)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$doCreateDatabase$1.apply(HiveExternalCatalog.scala:164)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.doCreateDatabase(HiveExternalCatalog.scala:163)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.createDatabase(ExternalCatalog.scala:69)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:207)
	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
	at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:190)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:190)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:75)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)
	at org.avanhecken.tpcds.SparkTPCDSTest.beforeAll(SparkTPCDSTest.scala:15)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:212)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)
	at org.avanhecken.tpcds.SparkTPCDSTest.run(SparkTPCDSTest.scala:5)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1346)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$1.apply(Runner.scala:1340)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1340)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1011)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1010)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1506)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010)
	at org.scalatest.tools.Runner$.run(Runner.scala:850)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:131)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)

2018-10-11 21:36:07,679 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:07,680 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:07,786 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:07,787 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:07,791 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:07,791 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:07,816 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:07,817 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:07,986 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:07,987 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:08,027 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:08,028 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:08,039 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:08,041 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:08,088 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:08,089 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:08,384 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:08,384 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:08,388 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:08,388 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:08,406 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:08,407 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:08,414 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:08,415 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:08,431 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: drop_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:08,431 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=drop_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:08,781 INFO [ScalaTest-run] h.m.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] deleting  file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs
2018-10-11 21:36:08,783 INFO [ScalaTest-run] o.a.h.c.C.deprecation [Configuration.java:1129] io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2018-10-11 21:36:08,786 INFO [ScalaTest-run] o.a.h.f.TrashPolicyDefault [TrashPolicyDefault.java:92] Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2018-10-11 21:36:08,788 INFO [ScalaTest-run] h.m.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] Deleted the diretory file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs
2018-10-11 21:36:08,817 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs_summary
2018-10-11 21:36:08,818 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs_summary	
2018-10-11 21:36:08,821 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs_summary
2018-10-11 21:36:08,822 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs_summary	
2018-10-11 21:36:08,837 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:08,837 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:08,863 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:08,863 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:08,866 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:08,867 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:08,888 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:08,888 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:08,920 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:08,921 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:08,946 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:08,947 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:08,953 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:08,953 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:08,973 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:08,974 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:09,021 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:09,023 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:09,029 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:09,029 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:09,050 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:09,050 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:09,057 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:09,058 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:09,079 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: drop_table : db=tpcds tbl=test1
2018-10-11 21:36:09,080 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=drop_table : db=tpcds tbl=test1	
2018-10-11 21:36:09,316 INFO [ScalaTest-run] h.m.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] deleting  file:/user/hive/warehouse/tpcds.db/test1
2018-10-11 21:36:09,318 INFO [ScalaTest-run] o.a.h.f.TrashPolicyDefault [TrashPolicyDefault.java:92] Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2018-10-11 21:36:09,319 INFO [ScalaTest-run] h.m.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] Deleted the diretory file:/user/hive/warehouse/tpcds.db/test1
2018-10-11 21:36:09,893 INFO [ScalaTest-run] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 185.394797 ms
2018-10-11 21:36:09,938 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:09,939 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:10,205 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:10,205 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:10,208 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:10,209 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:10,214 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:10,214 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:10,259 INFO [ScalaTest-run] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:10,320 INFO [ScalaTest-run] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:10,321 INFO [ScalaTest-run] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:10,474 INFO [ScalaTest-run] o.a.s.SparkContext [Logging.scala:54] Starting job: saveAsTable at SparkTPCDSTest.scala:21
2018-10-11 21:36:10,501 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 0 (saveAsTable at SparkTPCDSTest.scala:21) with 1 output partitions
2018-10-11 21:36:10,502 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 0 (saveAsTable at SparkTPCDSTest.scala:21)
2018-10-11 21:36:10,503 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:10,505 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:10,511 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTable at SparkTPCDSTest.scala:21), which has no missing parents
2018-10-11 21:36:10,576 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_0 stored as values in memory (estimated size 129.5 KB, free 1391.3 MB)
2018-10-11 21:36:10,605 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_0_piece0 stored as bytes in memory (estimated size 45.6 KB, free 1391.2 MB)
2018-10-11 21:36:10,608 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_0_piece0 in memory on 192.168.0.206:34485 (size: 45.6 KB, free: 1391.4 MB)
2018-10-11 21:36:10,610 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 0 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:10,622 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTable at SparkTPCDSTest.scala:21) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:10,623 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 0.0 with 1 tasks
2018-10-11 21:36:10,667 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 11265 bytes)
2018-10-11 21:36:10,675 INFO [Executor task launch worker for task 0] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 0.0 (TID 0)
2018-10-11 21:36:10,797 INFO [Executor task launch worker for task 0] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:10,798 INFO [Executor task launch worker for task 0] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:10,960 INFO [Executor task launch worker for task 0] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:10,961 INFO [Executor task launch worker for task 0] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:10,973 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:10,973 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:10,973 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:10,974 INFO [Executor task launch worker for task 0] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:10,990 INFO [Executor task launch worker for task 0] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "col1",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 col1;
}

       
2018-10-11 21:36:11,029 INFO [Executor task launch worker for task 0] o.a.h.i.c.CodecPool [CodecPool.java:151] Got brand-new compressor [.snappy]
2018-10-11 21:36:11,087 INFO [Executor task launch worker for task 0] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 792
2018-10-11 21:36:11,305 INFO [Executor task launch worker for task 0] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213610_0000_m_000000_0' to file:/user/hive/warehouse/tpcds.db/test1/_temporary/0/task_20181011213610_0000_m_000000
2018-10-11 21:36:11,305 INFO [Executor task launch worker for task 0] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213610_0000_m_000000_0: Committed
2018-10-11 21:36:11,321 INFO [Executor task launch worker for task 0] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 0.0 (TID 0). 2039 bytes result sent to driver
2018-10-11 21:36:11,327 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 0.0 (TID 0) in 678 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:11,330 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-10-11 21:36:11,334 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 0 (saveAsTable at SparkTPCDSTest.scala:21) finished in 0.812 s
2018-10-11 21:36:11,338 INFO [ScalaTest-run] o.a.s.s.DAGScheduler [Logging.scala:54] Job 0 finished: saveAsTable at SparkTPCDSTest.scala:21, took 0.863175 s
2018-10-11 21:36:11,352 INFO [ScalaTest-run] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:11,358 INFO [ScalaTest-run] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:11,371 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:11,372 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:11,377 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:11,377 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:11,383 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:11,383 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:11,394 INFO [ScalaTest-run] o.a.s.s.h.HiveExternalCatalog [Logging.scala:54] Persisting file based data source table `tpcds`.`test1` into Hive metastore in Hive compatible format.
2018-10-11 21:36:11,487 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: create_table: Table(tableName:test1, dbName:tpcds, owner:aaronvanhecken, createTime:1539286569, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/user/hive/warehouse/tpcds.db/test1, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"col1","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet, spark.sql.create.version=2.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2018-10-11 21:36:11,487 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test1, dbName:tpcds, owner:aaronvanhecken, createTime:1539286569, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/user/hive/warehouse/tpcds.db/test1, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"col1","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet, spark.sql.create.version=2.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2018-10-11 21:36:11,494 INFO [ScalaTest-run] hive.log [MetaStoreUtils.java:217] Updating table stats fast for test1
2018-10-11 21:36:11,495 INFO [ScalaTest-run] hive.log [MetaStoreUtils.java:219] Updated size of table test1 to 767
2018-10-11 21:36:11,703 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,703 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,716 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:11,716 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:11,719 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,720 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,732 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,732 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,746 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,746 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,760 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:11,760 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:11,763 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,764 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,779 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,780 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,803 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:11,804 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:11,808 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,808 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,818 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:11,819 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:11,822 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,823 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,834 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: drop_table : db=tpcds tbl=test2
2018-10-11 21:36:11,834 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=drop_table : db=tpcds tbl=test2	
2018-10-11 21:36:11,935 INFO [ScalaTest-run] h.m.hivemetastoressimpl [HiveMetaStoreFsImpl.java:41] deleting  file:/user/hive/warehouse/tpcds.db/test2
2018-10-11 21:36:11,936 INFO [ScalaTest-run] o.a.h.c.C.deprecation [Configuration.java:1129] io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum
2018-10-11 21:36:11,936 INFO [ScalaTest-run] o.a.h.f.TrashPolicyDefault [TrashPolicyDefault.java:92] Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.
2018-10-11 21:36:11,937 INFO [ScalaTest-run] h.m.hivemetastoressimpl [HiveMetaStoreFsImpl.java:53] Deleted the diretory file:/user/hive/warehouse/tpcds.db/test2
2018-10-11 21:36:11,997 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:11,998 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:12,128 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:12,128 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:12,130 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:12,131 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:12,134 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:12,135 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:12,147 INFO [ScalaTest-run] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:12,150 INFO [ScalaTest-run] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:12,152 INFO [ScalaTest-run] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:12,194 INFO [ScalaTest-run] o.a.s.SparkContext [Logging.scala:54] Starting job: saveAsTable at SparkTPCDSTest.scala:23
2018-10-11 21:36:12,195 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 1 (saveAsTable at SparkTPCDSTest.scala:23) with 1 output partitions
2018-10-11 21:36:12,196 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 1 (saveAsTable at SparkTPCDSTest.scala:23)
2018-10-11 21:36:12,196 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:12,196 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:12,197 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at SparkTPCDSTest.scala:23), which has no missing parents
2018-10-11 21:36:12,227 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_1 stored as values in memory (estimated size 129.5 KB, free 1391.1 MB)
2018-10-11 21:36:12,230 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_1_piece0 stored as bytes in memory (estimated size 45.7 KB, free 1391.1 MB)
2018-10-11 21:36:12,235 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_1_piece0 in memory on 192.168.0.206:34485 (size: 45.7 KB, free: 1391.3 MB)
2018-10-11 21:36:12,237 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:12,244 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at saveAsTable at SparkTPCDSTest.scala:23) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:12,244 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 1.0 with 1 tasks
2018-10-11 21:36:12,291 WARN [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:66] Stage 1 contains a task of very large size (330 KB). The maximum recommended task size is 100 KB.
2018-10-11 21:36:12,292 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 337965 bytes)
2018-10-11 21:36:12,292 INFO [Executor task launch worker for task 1] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 1.0 (TID 1)
2018-10-11 21:36:12,362 INFO [Executor task launch worker for task 1] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:12,363 INFO [Executor task launch worker for task 1] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:12,363 INFO [Executor task launch worker for task 1] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:12,364 INFO [Executor task launch worker for task 1] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:12,364 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:12,364 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:12,365 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:12,366 INFO [Executor task launch worker for task 1] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:12,369 INFO [Executor task launch worker for task 1] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "col1",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 col1;
}

       
2018-10-11 21:36:12,493 INFO [Executor task launch worker for task 1] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 79992
2018-10-11 21:36:12,516 INFO [Executor task launch worker for task 1] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213612_0001_m_000000_0' to file:/user/hive/warehouse/tpcds.db/test2/_temporary/0/task_20181011213612_0001_m_000000
2018-10-11 21:36:12,517 INFO [Executor task launch worker for task 1] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213612_0001_m_000000_0: Committed
2018-10-11 21:36:12,524 INFO [Executor task launch worker for task 1] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 1.0 (TID 1). 2039 bytes result sent to driver
2018-10-11 21:36:12,528 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 1.0 (TID 1) in 259 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:12,528 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-10-11 21:36:12,529 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 1 (saveAsTable at SparkTPCDSTest.scala:23) finished in 0.331 s
2018-10-11 21:36:12,530 INFO [ScalaTest-run] o.a.s.s.DAGScheduler [Logging.scala:54] Job 1 finished: saveAsTable at SparkTPCDSTest.scala:23, took 0.335914 s
2018-10-11 21:36:12,556 INFO [ScalaTest-run] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:12,557 INFO [ScalaTest-run] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:12,569 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:12,569 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:12,576 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:12,577 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:12,581 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:12,582 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:12,585 INFO [ScalaTest-run] o.a.s.s.h.HiveExternalCatalog [Logging.scala:54] Persisting file based data source table `tpcds`.`test2` into Hive metastore in Hive compatible format.
2018-10-11 21:36:12,586 INFO [ScalaTest-run] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: create_table: Table(tableName:test2, dbName:tpcds, owner:aaronvanhecken, createTime:1539286572, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/user/hive/warehouse/tpcds.db/test2, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"col1","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet, spark.sql.create.version=2.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2018-10-11 21:36:12,588 INFO [ScalaTest-run] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=create_table: Table(tableName:test2, dbName:tpcds, owner:aaronvanhecken, createTime:1539286572, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col1, type:int, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/user/hive/warehouse/tpcds.db/test2, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"col1","type":"integer","nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet, spark.sql.create.version=2.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2018-10-11 21:36:12,597 INFO [ScalaTest-run] hive.log [MetaStoreUtils.java:217] Updating table stats fast for test2
2018-10-11 21:36:12,597 INFO [ScalaTest-run] hive.log [MetaStoreUtils.java:219] Updated size of table test2 to 40371
2018-10-11 21:36:12,897 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:12,956 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 14
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 46
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 30
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 33
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 50
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 62
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 15
2018-10-11 21:36:12,957 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 51
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 20
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 7
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 39
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 26
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 37
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 45
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 0
2018-10-11 21:36:12,958 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 32
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 5
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 65
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 57
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 48
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 47
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 12
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 59
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 21
2018-10-11 21:36:12,959 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 13
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 67
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 40
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 23
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 27
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 38
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 8
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 4
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 35
2018-10-11 21:36:12,960 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 61
2018-10-11 21:36:12,961 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 29
2018-10-11 21:36:12,961 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 18
2018-10-11 21:36:12,961 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 63
2018-10-11 21:36:12,961 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2
2018-10-11 21:36:12,961 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 16
2018-10-11 21:36:12,977 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_0_piece0 on 192.168.0.206:34485 in memory (size: 45.6 KB, free: 1391.4 MB)
2018-10-11 21:36:12,981 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 28
2018-10-11 21:36:12,981 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 17
2018-10-11 21:36:12,982 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 25
2018-10-11 21:36:12,982 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 22
2018-10-11 21:36:12,982 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 55
2018-10-11 21:36:12,982 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 31
2018-10-11 21:36:12,983 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_1_piece0 on 192.168.0.206:34485 in memory (size: 45.7 KB, free: 1391.4 MB)
2018-10-11 21:36:12,984 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 36
2018-10-11 21:36:12,984 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 60
2018-10-11 21:36:12,984 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1
2018-10-11 21:36:12,984 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 42
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 49
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 41
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 6
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 52
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 34
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 58
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 43
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 24
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 64
2018-10-11 21:36:12,985 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 44
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 56
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 9
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 11
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 66
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 19
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 10
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 54
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 3
2018-10-11 21:36:12,986 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 53
2018-10-11 21:36:13,186 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:13,187 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:13,214 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:13,214 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:13,217 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:13,218 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:13,222 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:13,223 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:13,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:13,235 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:13,236 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:13,275 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: saveAsTable at SparkDataManager.scala:40
2018-10-11 21:36:13,277 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 2 (saveAsTable at SparkDataManager.scala:40) with 1 output partitions
2018-10-11 21:36:13,277 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 2 (saveAsTable at SparkDataManager.scala:40)
2018-10-11 21:36:13,277 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:13,277 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:13,278 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 2 (MapPartitionsRDD[7] at saveAsTable at SparkDataManager.scala:40), which has no missing parents
2018-10-11 21:36:13,297 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_2 stored as values in memory (estimated size 131.7 KB, free 1391.3 MB)
2018-10-11 21:36:13,300 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_2_piece0 stored as bytes in memory (estimated size 46.7 KB, free 1391.2 MB)
2018-10-11 21:36:13,301 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_2_piece0 in memory on 192.168.0.206:34485 (size: 46.7 KB, free: 1391.4 MB)
2018-10-11 21:36:13,302 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:13,303 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at saveAsTable at SparkDataManager.scala:40) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:13,303 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 2.0 with 1 tasks
2018-10-11 21:36:13,304 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7880 bytes)
2018-10-11 21:36:13,305 INFO [Executor task launch worker for task 2] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 2.0 (TID 2)
2018-10-11 21:36:13,320 INFO [Executor task launch worker for task 2] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:13,321 INFO [Executor task launch worker for task 2] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:13,321 INFO [Executor task launch worker for task 2] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:13,322 INFO [Executor task launch worker for task 2] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:13,323 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:13,323 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:13,323 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:13,323 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:13,323 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:13,324 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:13,324 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:13,327 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:13,327 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:13,327 INFO [Executor task launch worker for task 2] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:13,338 INFO [Executor task launch worker for task 2] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "database",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "executionDateTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "sparkConfig",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "queries",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "id",
          "type" : "short",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "businessQuestion",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "queryClass",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "statements",
          "type" : {
            "type" : "array",
            "elementType" : {
              "type" : "struct",
              "fields" : [ {
                "name" : "id",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              }, {
                "name" : "text",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              } ]
            },
            "containsNull" : true
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

       
2018-10-11 21:36:13,381 INFO [Executor task launch worker for task 2] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 0
2018-10-11 21:36:13,383 INFO [Executor task launch worker for task 2] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213613_0002_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs/_temporary/0/task_20181011213613_0002_m_000000
2018-10-11 21:36:13,383 INFO [Executor task launch worker for task 2] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213613_0002_m_000000_0: Committed
2018-10-11 21:36:13,385 INFO [Executor task launch worker for task 2] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 2.0 (TID 2). 1953 bytes result sent to driver
2018-10-11 21:36:13,387 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 2.0 (TID 2) in 83 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:13,387 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-10-11 21:36:13,388 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 2 (saveAsTable at SparkDataManager.scala:40) finished in 0.109 s
2018-10-11 21:36:13,388 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 2 finished: saveAsTable at SparkDataManager.scala:40, took 0.112578 s
2018-10-11 21:36:13,410 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:13,411 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:13,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:13,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:13,430 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:13,431 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:13,438 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:13,438 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:13,444 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.h.HiveExternalCatalog [Logging.scala:54] Persisting file based data source table `tpcds`.`spark_tpcds_runs` into Hive metastore in Hive compatible format.
2018-10-11 21:36:13,457 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: create_table: Table(tableName:spark_tpcds_runs, dbName:tpcds, owner:aaronvanhecken, createTime:1539286573, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:description, type:string, comment:null), FieldSchema(name:database, type:string, comment:null), FieldSchema(name:executionDateTime, type:bigint, comment:null), FieldSchema(name:sparkConfig, type:map<string,string>, comment:null), FieldSchema(name:queries, type:array<struct<id:smallint,businessQuestion:string,queryClass:int,statements:array<struct<id:string,text:string>>>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"description","type":"string","nullable":true,"metadata":{}},{"name":"database","type":"string","nullable":true,"metadata":{}},{"name":"executionDateTime","type":"long","nullable":true,"metadata":{}},{"name":"sparkConfig","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"queries","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"id","type":"short","nullable":true,"metadata":{}},{"name":"businessQuestion","type":"string","nullable":true,"metadata":{}},{"name":"queryClass","type":"integer","nullable":true,"metadata":{}},{"name":"statements","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"text","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet, spark.sql.create.version=2.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))
2018-10-11 21:36:13,458 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=create_table: Table(tableName:spark_tpcds_runs, dbName:tpcds, owner:aaronvanhecken, createTime:1539286573, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:name, type:string, comment:null), FieldSchema(name:description, type:string, comment:null), FieldSchema(name:database, type:string, comment:null), FieldSchema(name:executionDateTime, type:bigint, comment:null), FieldSchema(name:sparkConfig, type:map<string,string>, comment:null), FieldSchema(name:queries, type:array<struct<id:smallint,businessQuestion:string,queryClass:int,statements:array<struct<id:string,text:string>>>>, comment:null)], location:null, inputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, parameters:{path=file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs, serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={"type":"struct","fields":[{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"description","type":"string","nullable":true,"metadata":{}},{"name":"database","type":"string","nullable":true,"metadata":{}},{"name":"executionDateTime","type":"long","nullable":true,"metadata":{}},{"name":"sparkConfig","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"queries","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"id","type":"short","nullable":true,"metadata":{}},{"name":"businessQuestion","type":"string","nullable":true,"metadata":{}},{"name":"queryClass","type":"integer","nullable":true,"metadata":{}},{"name":"statements","type":{"type":"array","elementType":{"type":"struct","fields":[{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"text","type":"string","nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}}]},"containsNull":true},"nullable":true,"metadata":{}}]}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=parquet, spark.sql.create.version=2.3.1}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))	
2018-10-11 21:36:13,463 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_runs
2018-10-11 21:36:13,464 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_runs to 1501
2018-10-11 21:36:13,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:13,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:13,775 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:13,829 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:13,829 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:13,830 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:13,831 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:13,832 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:13,835 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:13,835 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:13,848 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:13,848 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:14,218 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:14,220 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:14,223 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:14,233 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:14,320 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 26.190211 ms
2018-10-11 21:36:14,505 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 102
2018-10-11 21:36:14,505 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 85
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 86
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 73
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 80
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 90
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 74
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 88
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 101
2018-10-11 21:36:14,506 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 94
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 81
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 79
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 97
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 75
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 89
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 71
2018-10-11 21:36:14,507 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 87
2018-10-11 21:36:14,509 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_2_piece0 on 192.168.0.206:34485 in memory (size: 46.7 KB, free: 1391.4 MB)
2018-10-11 21:36:14,510 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 84
2018-10-11 21:36:14,510 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 76
2018-10-11 21:36:14,510 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 95
2018-10-11 21:36:14,510 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 72
2018-10-11 21:36:14,510 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 83
2018-10-11 21:36:14,511 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 93
2018-10-11 21:36:14,511 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 92
2018-10-11 21:36:14,511 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 82
2018-10-11 21:36:14,511 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 98
2018-10-11 21:36:14,511 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 100
2018-10-11 21:36:14,511 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 70
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 78
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 69
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 96
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 91
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 99
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 68
2018-10-11 21:36:14,512 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 77
2018-10-11 21:36:14,554 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 188.358375 ms
2018-10-11 21:36:14,574 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_3 stored as values in memory (estimated size 230.0 KB, free 1391.2 MB)
2018-10-11 21:36:14,583 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_3_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1391.2 MB)
2018-10-11 21:36:14,584 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_3_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.4 MB)
2018-10-11 21:36:14,585 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 3 from count at SparkDataManager.scala:47
2018-10-11 21:36:14,600 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195805 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:14,673 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:14,676 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 11 (count at SparkDataManager.scala:47)
2018-10-11 21:36:14,678 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 3 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:14,678 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 4 (count at SparkDataManager.scala:47)
2018-10-11 21:36:14,678 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 3)
2018-10-11 21:36:14,679 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 3)
2018-10-11 21:36:14,680 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 3 (MapPartitionsRDD[11] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:14,700 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_4 stored as values in memory (estimated size 26.3 KB, free 1391.1 MB)
2018-10-11 21:36:14,702 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1391.1 MB)
2018-10-11 21:36:14,702 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_4_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.4 MB)
2018-10-11 21:36:14,703 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 4 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:14,705 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[11] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:14,705 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 3.0 with 1 tasks
2018-10-11 21:36:14,710 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 8375 bytes)
2018-10-11 21:36:14,710 INFO [Executor task launch worker for task 3] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 3.0 (TID 3)
2018-10-11 21:36:14,735 INFO [Executor task launch worker for task 3] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:14,780 INFO [Executor task launch worker for task 3] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:14,851 INFO [Executor task launch worker for task 3] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 49.657753 ms
2018-10-11 21:36:14,881 INFO [Executor task launch worker for task 3] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 15.9422 ms
2018-10-11 21:36:14,893 INFO [Executor task launch worker for task 3] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 9.991434 ms
2018-10-11 21:36:14,895 INFO [Executor task launch worker for task 3] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:14,923 INFO [Executor task launch worker for task 3] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 3.0 (TID 3). 1651 bytes result sent to driver
2018-10-11 21:36:14,927 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 3.0 (TID 3) in 221 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:14,928 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-10-11 21:36:14,929 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 3 (count at SparkDataManager.scala:47) finished in 0.242 s
2018-10-11 21:36:14,930 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:14,930 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:14,931 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 4)
2018-10-11 21:36:14,932 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:14,934 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 4 (MapPartitionsRDD[14] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:14,942 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_5 stored as values in memory (estimated size 7.4 KB, free 1391.1 MB)
2018-10-11 21:36:14,944 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1391.1 MB)
2018-10-11 21:36:14,945 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_5_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.4 MB)
2018-10-11 21:36:14,946 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 5 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:14,948 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:14,948 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 4.0 with 1 tasks
2018-10-11 21:36:14,953 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:14,953 INFO [Executor task launch worker for task 4] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 4.0 (TID 4)
2018-10-11 21:36:14,969 INFO [Executor task launch worker for task 4] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:14,971 INFO [Executor task launch worker for task 4] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 5 ms
2018-10-11 21:36:14,993 INFO [Executor task launch worker for task 4] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 4.0 (TID 4). 1775 bytes result sent to driver
2018-10-11 21:36:14,994 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 4.0 (TID 4) in 43 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:14,994 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-10-11 21:36:14,995 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 4 (count at SparkDataManager.scala:47) finished in 0.055 s
2018-10-11 21:36:14,995 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 3 finished: count at SparkDataManager.scala:47, took 0.321605 s
2018-10-11 21:36:15,001 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:15,002 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:15,004 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:15,005 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:15,016 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:15,016 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:15,237 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:15,238 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:15,238 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:15,239 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:15,305 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 39.137023 ms
2018-10-11 21:36:15,318 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_6 stored as values in memory (estimated size 230.0 KB, free 1390.9 MB)
2018-10-11 21:36:15,327 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_6_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.9 MB)
2018-10-11 21:36:15,328 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_6_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:15,329 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 6 from count at SparkDataManager.scala:47
2018-10-11 21:36:15,330 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195805 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:15,348 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:15,349 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 17 (count at SparkDataManager.scala:47)
2018-10-11 21:36:15,349 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 4 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:15,349 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 6 (count at SparkDataManager.scala:47)
2018-10-11 21:36:15,349 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 5)
2018-10-11 21:36:15,350 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 5)
2018-10-11 21:36:15,350 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 5 (MapPartitionsRDD[17] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:15,353 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_7 stored as values in memory (estimated size 26.3 KB, free 1390.8 MB)
2018-10-11 21:36:15,355 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1390.8 MB)
2018-10-11 21:36:15,356 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_7_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.3 MB)
2018-10-11 21:36:15,357 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 7 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:15,357 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[17] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:15,358 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 5.0 with 1 tasks
2018-10-11 21:36:15,359 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 8375 bytes)
2018-10-11 21:36:15,359 INFO [Executor task launch worker for task 5] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 5.0 (TID 5)
2018-10-11 21:36:15,365 INFO [Executor task launch worker for task 5] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:15,369 INFO [Executor task launch worker for task 5] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:15,379 INFO [Executor task launch worker for task 5] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:15,386 INFO [Executor task launch worker for task 5] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 5.0 (TID 5). 1608 bytes result sent to driver
2018-10-11 21:36:15,387 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 5.0 (TID 5) in 29 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:15,387 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 5.0, whose tasks have all completed, from pool 
2018-10-11 21:36:15,388 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 5 (count at SparkDataManager.scala:47) finished in 0.037 s
2018-10-11 21:36:15,389 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:15,389 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:15,389 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 6)
2018-10-11 21:36:15,389 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:15,389 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 6 (MapPartitionsRDD[20] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:15,392 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_8 stored as values in memory (estimated size 7.4 KB, free 1390.8 MB)
2018-10-11 21:36:15,393 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.8 MB)
2018-10-11 21:36:15,394 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_8_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:15,395 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 8 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:15,396 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[20] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:15,396 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 6.0 with 1 tasks
2018-10-11 21:36:15,397 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:15,397 INFO [Executor task launch worker for task 6] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 6.0 (TID 6)
2018-10-11 21:36:15,400 INFO [Executor task launch worker for task 6] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:15,401 INFO [Executor task launch worker for task 6] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:15,402 INFO [Executor task launch worker for task 6] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 6.0 (TID 6). 1775 bytes result sent to driver
2018-10-11 21:36:15,404 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 6.0 (TID 6) in 8 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:15,404 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 6.0, whose tasks have all completed, from pool 
2018-10-11 21:36:15,406 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 6 (count at SparkDataManager.scala:47) finished in 0.015 s
2018-10-11 21:36:15,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 4 finished: count at SparkDataManager.scala:47, took 0.058011 s
2018-10-11 21:36:15,588 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 85.817717 ms
2018-10-11 21:36:15,660 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:15,660 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:15,723 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:15,726 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:15,726 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:15,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 25.250822 ms
2018-10-11 21:36:15,776 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:55
2018-10-11 21:36:15,777 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 5 (insertInto at SparkDataManager.scala:55) with 1 output partitions
2018-10-11 21:36:15,777 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 7 (insertInto at SparkDataManager.scala:55)
2018-10-11 21:36:15,777 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:15,777 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:15,778 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 7 (MapPartitionsRDD[22] at insertInto at SparkDataManager.scala:55), which has no missing parents
2018-10-11 21:36:15,796 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_9 stored as values in memory (estimated size 131.8 KB, free 1390.7 MB)
2018-10-11 21:36:15,799 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_9_piece0 stored as bytes in memory (estimated size 46.7 KB, free 1390.6 MB)
2018-10-11 21:36:15,800 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_9_piece0 in memory on 192.168.0.206:34485 (size: 46.7 KB, free: 1391.3 MB)
2018-10-11 21:36:15,801 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 9 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:15,802 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at insertInto at SparkDataManager.scala:55) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:15,802 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 7.0 with 1 tasks
2018-10-11 21:36:15,804 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 9026 bytes)
2018-10-11 21:36:15,805 INFO [Executor task launch worker for task 7] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 7.0 (TID 7)
2018-10-11 21:36:15,818 INFO [Executor task launch worker for task 7] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:15,818 INFO [Executor task launch worker for task 7] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:15,818 INFO [Executor task launch worker for task 7] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:15,818 INFO [Executor task launch worker for task 7] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:15,819 INFO [Executor task launch worker for task 7] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:15,822 INFO [Executor task launch worker for task 7] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "database",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "executionDateTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "sparkConfig",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "queries",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "id",
          "type" : "short",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "businessQuestion",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "queryClass",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "statements",
          "type" : {
            "type" : "array",
            "elementType" : {
              "type" : "struct",
              "fields" : [ {
                "name" : "id",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              }, {
                "name" : "text",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              } ]
            },
            "containsNull" : true
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

       
2018-10-11 21:36:15,850 INFO [Executor task launch worker for task 7] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 898
2018-10-11 21:36:15,867 INFO [Executor task launch worker for task 7] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213615_0007_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs/_temporary/0/task_20181011213615_0007_m_000000
2018-10-11 21:36:15,868 INFO [Executor task launch worker for task 7] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213615_0007_m_000000_0: Committed
2018-10-11 21:36:15,869 INFO [Executor task launch worker for task 7] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 7.0 (TID 7). 1996 bytes result sent to driver
2018-10-11 21:36:15,870 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 7.0 (TID 7) in 66 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:15,870 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 7.0, whose tasks have all completed, from pool 
2018-10-11 21:36:15,871 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 7 (insertInto at SparkDataManager.scala:55) finished in 0.093 s
2018-10-11 21:36:15,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 5 finished: insertInto at SparkDataManager.scala:55, took 0.095263 s
2018-10-11 21:36:15,880 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:15,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:15,891 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:15,892 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:15,895 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:15,896 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:15,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:15,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:15,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:15,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:15,976 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs
2018-10-11 21:36:15,977 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs	
2018-10-11 21:36:15,997 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_runs
2018-10-11 21:36:15,997 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_runs to 5837
2018-10-11 21:36:16,037 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test' ...
2018-10-11 21:36:16,050 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:16,051 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:16,055 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:36:16,113 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:16,114 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:16,119 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:16,120 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:16,167 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:16,167 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:16,168 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:16,168 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:16,190 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 8.081936 ms
2018-10-11 21:36:16,212 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 14.15254 ms
2018-10-11 21:36:16,218 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_10 stored as values in memory (estimated size 226.1 KB, free 1390.4 MB)
2018-10-11 21:36:16,227 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_10_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.4 MB)
2018-10-11 21:36:16,228 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_10_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:16,229 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 10 from collect at Statement.scala:24
2018-10-11 21:36:16,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:16,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:16,250 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 26 (collect at Statement.scala:24)
2018-10-11 21:36:16,251 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 6 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:16,251 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 9 (collect at Statement.scala:24)
2018-10-11 21:36:16,251 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 8)
2018-10-11 21:36:16,251 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 8)
2018-10-11 21:36:16,252 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 8 (MapPartitionsRDD[26] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:16,261 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_11 stored as values in memory (estimated size 11.1 KB, free 1390.4 MB)
2018-10-11 21:36:16,263 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.4 KB, free 1390.4 MB)
2018-10-11 21:36:16,264 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_11_piece0 in memory on 192.168.0.206:34485 (size: 5.4 KB, free: 1391.3 MB)
2018-10-11 21:36:16,265 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 11 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:16,266 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[26] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:16,266 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 8.0 with 1 tasks
2018-10-11 21:36:16,268 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:16,269 INFO [Executor task launch worker for task 8] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 8.0 (TID 8)
2018-10-11 21:36:16,274 INFO [Executor task launch worker for task 8] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:16,309 INFO [Executor task launch worker for task 8] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 8.0 (TID 8). 1680 bytes result sent to driver
2018-10-11 21:36:16,310 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 8.0 (TID 8) in 42 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:16,310 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 8.0, whose tasks have all completed, from pool 
2018-10-11 21:36:16,311 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 8 (collect at Statement.scala:24) finished in 0.058 s
2018-10-11 21:36:16,311 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:16,311 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:16,312 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 9)
2018-10-11 21:36:16,313 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:16,313 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 9 (MapPartitionsRDD[29] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:16,317 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_12 stored as values in memory (estimated size 7.4 KB, free 1390.4 MB)
2018-10-11 21:36:16,320 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.4 MB)
2018-10-11 21:36:16,321 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_12_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:16,322 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 12 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:16,322 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:16,322 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 9.0 with 1 tasks
2018-10-11 21:36:16,328 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:16,328 INFO [Executor task launch worker for task 9] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 9.0 (TID 9)
2018-10-11 21:36:16,332 INFO [Executor task launch worker for task 9] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:16,332 INFO [Executor task launch worker for task 9] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:16,334 INFO [Executor task launch worker for task 9] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 9.0 (TID 9). 1782 bytes result sent to driver
2018-10-11 21:36:16,335 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 9.0 (TID 9) in 8 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:16,335 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 9.0, whose tasks have all completed, from pool 
2018-10-11 21:36:16,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 9 (collect at Statement.scala:24) finished in 0.020 s
2018-10-11 21:36:16,336 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 6 finished: collect at Statement.scala:24, took 0.087089 s
2018-10-11 21:36:16,341 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:36:16,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 27.038015 ms
2018-10-11 21:36:16,427 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:16,428 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:16,543 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:16,545 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:16,546 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:16,563 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 12.625908 ms
2018-10-11 21:36:16,584 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:16,585 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 7 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:16,585 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 10 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:16,585 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:16,585 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:16,585 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 10 (MapPartitionsRDD[31] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:16,600 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_13 stored as values in memory (estimated size 131.0 KB, free 1390.2 MB)
2018-10-11 21:36:16,602 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_13_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.2 MB)
2018-10-11 21:36:16,603 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_13_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:16,604 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 13 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:16,605 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[31] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:16,606 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 10.0 with 1 tasks
2018-10-11 21:36:16,607 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 10305 bytes)
2018-10-11 21:36:16,608 INFO [Executor task launch worker for task 10] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 10.0 (TID 10)
2018-10-11 21:36:16,622 INFO [Executor task launch worker for task 10] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:16,623 INFO [Executor task launch worker for task 10] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:16,623 INFO [Executor task launch worker for task 10] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:16,624 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:16,625 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:16,625 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:16,625 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:16,625 INFO [Executor task launch worker for task 10] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:16,627 INFO [Executor task launch worker for task 10] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:16,633 INFO [Executor task launch worker for task 10] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2244
2018-10-11 21:36:16,637 INFO [Executor task launch worker for task 10] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213616_0010_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213616_0010_m_000000
2018-10-11 21:36:16,637 INFO [Executor task launch worker for task 10] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213616_0010_m_000000_0: Committed
2018-10-11 21:36:16,640 INFO [Executor task launch worker for task 10] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 10.0 (TID 10). 2039 bytes result sent to driver
2018-10-11 21:36:16,641 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 10.0 (TID 10) in 34 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:16,641 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 10.0, whose tasks have all completed, from pool 
2018-10-11 21:36:16,642 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 10 (insertInto at SparkDataManager.scala:61) finished in 0.056 s
2018-10-11 21:36:16,643 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 7 finished: insertInto at SparkDataManager.scala:61, took 0.058870 s
2018-10-11 21:36:16,658 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:16,658 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:16,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:16,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:16,689 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:16,690 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:16,705 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:16,705 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:16,718 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:16,719 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:16,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:16,749 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 183
2018-10-11 21:36:16,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:16,750 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 124
2018-10-11 21:36:16,750 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 251
2018-10-11 21:36:16,750 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 190
2018-10-11 21:36:16,752 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_7_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.2 MB)
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 171
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 232
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 249
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 261
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 314
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 265
2018-10-11 21:36:16,754 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 276
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 350
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 283
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 297
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 223
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 294
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 161
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 141
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 156
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 279
2018-10-11 21:36:16,755 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 290
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 321
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 244
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 235
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 357
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 309
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 325
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 300
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 333
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 195
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 172
2018-10-11 21:36:16,756 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 330
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 302
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 336
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 328
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 166
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 243
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 348
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 173
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 358
2018-10-11 21:36:16,757 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 153
2018-10-11 21:36:16,759 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_5_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:16,760 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 198
2018-10-11 21:36:16,761 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 273
2018-10-11 21:36:16,761 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 270
2018-10-11 21:36:16,761 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 252
2018-10-11 21:36:16,763 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_12_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:16,764 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 197
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 129
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 242
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 329
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 320
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 326
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 135
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 264
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 289
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 206
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 306
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 345
2018-10-11 21:36:16,765 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 164
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 366
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 360
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 245
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 213
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 356
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 168
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 162
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 208
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 247
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 257
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 151
2018-10-11 21:36:16,766 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 154
2018-10-11 21:36:16,767 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 238
2018-10-11 21:36:16,767 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 353
2018-10-11 21:36:16,767 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 303
2018-10-11 21:36:16,767 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 175
2018-10-11 21:36:16,767 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 250
2018-10-11 21:36:16,767 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 179
2018-10-11 21:36:16,769 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_6_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:16,770 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 361
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 214
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 225
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 158
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 267
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 365
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 186
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 192
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 159
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 234
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 266
2018-10-11 21:36:16,771 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 218
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 316
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 174
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 178
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 180
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 227
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 149
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 167
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 155
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 187
2018-10-11 21:36:16,772 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 355
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 150
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 362
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 146
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 271
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 308
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 237
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 367
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 262
2018-10-11 21:36:16,773 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 319
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 246
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 211
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 295
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 347
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 354
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 256
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 304
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 305
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 292
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 275
2018-10-11 21:36:16,774 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 182
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 212
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 240
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 121
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 144
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 177
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 189
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 254
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 258
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 277
2018-10-11 21:36:16,775 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 148
2018-10-11 21:36:16,776 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 205
2018-10-11 21:36:16,777 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_8_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:16,778 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 263
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 193
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 278
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 287
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 291
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 165
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 253
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 224
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 269
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 137
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 160
2018-10-11 21:36:16,779 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 157
2018-10-11 21:36:16,780 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 145
2018-10-11 21:36:16,780 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 281
2018-10-11 21:36:16,781 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_11_piece0 on 192.168.0.206:34485 in memory (size: 5.4 KB, free: 1391.3 MB)
2018-10-11 21:36:16,782 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 220
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 301
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 142
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 327
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 122
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 140
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 323
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 202
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 181
2018-10-11 21:36:16,783 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 228
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 136
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 346
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 217
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 229
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 248
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 203
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 184
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 216
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 334
2018-10-11 21:36:16,784 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 176
2018-10-11 21:36:16,791 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 1
2018-10-11 21:36:16,791 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 204
2018-10-11 21:36:16,791 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 215
2018-10-11 21:36:16,791 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 272
2018-10-11 21:36:16,793 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_9_piece0 on 192.168.0.206:34485 in memory (size: 46.7 KB, free: 1391.3 MB)
2018-10-11 21:36:16,794 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 310
2018-10-11 21:36:16,794 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 313
2018-10-11 21:36:16,794 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 152
2018-10-11 21:36:16,794 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 188
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 332
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 127
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 230
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 210
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 231
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 221
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 260
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 364
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 284
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 318
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 344
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 147
2018-10-11 21:36:16,795 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 194
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 259
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 131
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 312
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 282
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 337
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 125
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 169
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 191
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 307
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 163
2018-10-11 21:36:16,796 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 280
2018-10-11 21:36:16,798 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_13_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.3 MB)
2018-10-11 21:36:16,803 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 2
2018-10-11 21:36:16,804 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 288
2018-10-11 21:36:16,804 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 200
2018-10-11 21:36:16,804 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 219
2018-10-11 21:36:16,804 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 239
2018-10-11 21:36:16,804 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 274
2018-10-11 21:36:16,804 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 241
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 130
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 359
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 351
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 343
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 143
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 134
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 170
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 315
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 268
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 349
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 285
2018-10-11 21:36:16,805 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 298
2018-10-11 21:36:16,806 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 128
2018-10-11 21:36:16,806 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 293
2018-10-11 21:36:16,806 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 123
2018-10-11 21:36:16,806 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 133
2018-10-11 21:36:16,806 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 317
2018-10-11 21:36:16,806 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 226
2018-10-11 21:36:16,807 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 335
2018-10-11 21:36:16,807 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 352
2018-10-11 21:36:16,807 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 139
2018-10-11 21:36:16,807 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 126
2018-10-11 21:36:16,809 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_4_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.4 MB)
2018-10-11 21:36:16,809 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 255
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 311
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 299
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 222
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 199
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 209
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 185
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 286
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 324
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 322
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 196
2018-10-11 21:36:16,810 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 233
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 207
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 132
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 331
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 236
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 296
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 363
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 201
2018-10-11 21:36:16,811 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 138
2018-10-11 21:36:16,812 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_10_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.4 MB)
2018-10-11 21:36:16,820 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:16,821 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2915137
2018-10-11 21:36:16,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:16,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:16,867 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:16,868 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:16,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:16,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:16,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:16,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:16,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:16,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:16,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:16,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:16,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:16,950 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:16,977 INFO [broadcast-exchange-0] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 11.754907 ms
2018-10-11 21:36:16,979 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 14.581986 ms
2018-10-11 21:36:16,982 INFO [broadcast-exchange-0] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_14 stored as values in memory (estimated size 226.1 KB, free 1390.9 MB)
2018-10-11 21:36:16,992 INFO [broadcast-exchange-0] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_14_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.9 MB)
2018-10-11 21:36:16,993 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_14_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.4 MB)
2018-10-11 21:36:16,994 INFO [broadcast-exchange-0] o.a.s.SparkContext [Logging.scala:54] Created broadcast 14 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:16,995 INFO [broadcast-exchange-0] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:17,014 INFO [broadcast-exchange-0] o.a.s.SparkContext [Logging.scala:54] Starting job: run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:17,015 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 8 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2018-10-11 21:36:17,015 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 11 (run at ThreadPoolExecutor.java:1149)
2018-10-11 21:36:17,015 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:17,015 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 16.861764 ms
2018-10-11 21:36:17,016 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:17,016 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 11 (MapPartitionsRDD[35] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2018-10-11 21:36:17,018 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_15 stored as values in memory (estimated size 7.5 KB, free 1390.9 MB)
2018-10-11 21:36:17,020 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1390.9 MB)
2018-10-11 21:36:17,020 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_15_piece0 in memory on 192.168.0.206:34485 (size: 3.9 KB, free: 1391.4 MB)
2018-10-11 21:36:17,021 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 15 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:17,022 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[35] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:17,023 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 11.0 with 1 tasks
2018-10-11 21:36:17,024 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 11.0 (TID 11, localhost, executor driver, partition 0, PROCESS_LOCAL, 8375 bytes)
2018-10-11 21:36:17,024 INFO [Executor task launch worker for task 11] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 11.0 (TID 11)
2018-10-11 21:36:17,029 INFO [Executor task launch worker for task 11] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:17,034 INFO [Executor task launch worker for task 11] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 11.0 (TID 11). 1460 bytes result sent to driver
2018-10-11 21:36:17,035 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 11.0 (TID 11) in 11 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:17,035 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 11.0, whose tasks have all completed, from pool 
2018-10-11 21:36:17,036 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 11 (run at ThreadPoolExecutor.java:1149) finished in 0.019 s
2018-10-11 21:36:17,037 INFO [broadcast-exchange-0] o.a.s.s.DAGScheduler [Logging.scala:54] Job 8 finished: run at ThreadPoolExecutor.java:1149, took 0.022214 s
2018-10-11 21:36:17,043 INFO [broadcast-exchange-0] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_16 stored as values in memory (estimated size 5.8 KB, free 1390.9 MB)
2018-10-11 21:36:17,045 INFO [broadcast-exchange-0] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_16_piece0 stored as bytes in memory (estimated size 201.0 B, free 1390.9 MB)
2018-10-11 21:36:17,046 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_16_piece0 in memory on 192.168.0.206:34485 (size: 201.0 B, free: 1391.4 MB)
2018-10-11 21:36:17,047 INFO [broadcast-exchange-0] o.a.s.SparkContext [Logging.scala:54] Created broadcast 16 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:17,062 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 12.632797 ms
2018-10-11 21:36:17,066 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_17 stored as values in memory (estimated size 226.1 KB, free 1390.7 MB)
2018-10-11 21:36:17,075 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_17_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.7 MB)
2018-10-11 21:36:17,076 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_17_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:17,077 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 17 from collect at Statement.scala:24
2018-10-11 21:36:17,078 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:17,104 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:17,105 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 41 (collect at Statement.scala:24)
2018-10-11 21:36:17,105 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 9 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:17,105 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 13 (collect at Statement.scala:24)
2018-10-11 21:36:17,105 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 12)
2018-10-11 21:36:17,105 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 12)
2018-10-11 21:36:17,106 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 12 (MapPartitionsRDD[41] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:17,133 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_18 stored as values in memory (estimated size 17.5 KB, free 1390.6 MB)
2018-10-11 21:36:17,135 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.1 KB, free 1390.6 MB)
2018-10-11 21:36:17,136 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_18_piece0 in memory on 192.168.0.206:34485 (size: 8.1 KB, free: 1391.3 MB)
2018-10-11 21:36:17,137 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 18 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:17,138 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[41] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:17,138 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 12.0 with 1 tasks
2018-10-11 21:36:17,140 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 12.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:17,140 INFO [Executor task launch worker for task 12] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 12.0 (TID 12)
2018-10-11 21:36:17,153 INFO [Executor task launch worker for task 12] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 6.098583 ms
2018-10-11 21:36:17,156 INFO [Executor task launch worker for task 12] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:17,167 INFO [Executor task launch worker for task 12] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 12.0 (TID 12). 2669 bytes result sent to driver
2018-10-11 21:36:17,168 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 12.0 (TID 12) in 29 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:17,168 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 12.0, whose tasks have all completed, from pool 
2018-10-11 21:36:17,169 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 12 (collect at Statement.scala:24) finished in 0.062 s
2018-10-11 21:36:17,170 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:17,170 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:17,170 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 13)
2018-10-11 21:36:17,170 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:17,171 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 13 (MapPartitionsRDD[44] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:17,173 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_19 stored as values in memory (estimated size 7.4 KB, free 1390.6 MB)
2018-10-11 21:36:17,176 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_19_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.6 MB)
2018-10-11 21:36:17,176 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_19_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:17,177 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 19 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:17,177 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[44] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:17,178 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 13.0 with 1 tasks
2018-10-11 21:36:17,179 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 13.0 (TID 13, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:17,179 INFO [Executor task launch worker for task 13] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 13.0 (TID 13)
2018-10-11 21:36:17,183 INFO [Executor task launch worker for task 13] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:17,183 INFO [Executor task launch worker for task 13] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:17,187 INFO [Executor task launch worker for task 13] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 13.0 (TID 13). 1782 bytes result sent to driver
2018-10-11 21:36:17,188 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 13.0 (TID 13) in 10 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:17,188 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 13.0, whose tasks have all completed, from pool 
2018-10-11 21:36:17,190 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 13 (collect at Statement.scala:24) finished in 0.018 s
2018-10-11 21:36:17,191 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 9 finished: collect at Statement.scala:24, took 0.086918 s
2018-10-11 21:36:17,193 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:36:17,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:17,249 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:17,333 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:17,337 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:17,338 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:17,362 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:17,363 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 10 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:17,363 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 14 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:17,364 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:17,364 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:17,364 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 14 (MapPartitionsRDD[46] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:17,382 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_20 stored as values in memory (estimated size 131.0 KB, free 1390.5 MB)
2018-10-11 21:36:17,385 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_20_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.4 MB)
2018-10-11 21:36:17,386 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_20_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.3 MB)
2018-10-11 21:36:17,386 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 20 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:17,387 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[46] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:17,387 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 14.0 with 1 tasks
2018-10-11 21:36:17,388 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 14.0 (TID 14, localhost, executor driver, partition 0, PROCESS_LOCAL, 10836 bytes)
2018-10-11 21:36:17,389 INFO [Executor task launch worker for task 14] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 14.0 (TID 14)
2018-10-11 21:36:17,400 INFO [Executor task launch worker for task 14] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:17,401 INFO [Executor task launch worker for task 14] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:17,401 INFO [Executor task launch worker for task 14] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:17,401 INFO [Executor task launch worker for task 14] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:17,402 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:17,403 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:17,403 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:17,403 INFO [Executor task launch worker for task 14] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:17,405 INFO [Executor task launch worker for task 14] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:17,414 INFO [Executor task launch worker for task 14] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2782
2018-10-11 21:36:17,419 INFO [Executor task launch worker for task 14] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213617_0014_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213617_0014_m_000000
2018-10-11 21:36:17,419 INFO [Executor task launch worker for task 14] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213617_0014_m_000000_0: Committed
2018-10-11 21:36:17,421 INFO [Executor task launch worker for task 14] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 14.0 (TID 14). 1996 bytes result sent to driver
2018-10-11 21:36:17,422 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 14.0 (TID 14) in 34 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:17,423 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 14.0, whose tasks have all completed, from pool 
2018-10-11 21:36:17,424 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 14 (insertInto at SparkDataManager.scala:61) finished in 0.059 s
2018-10-11 21:36:17,428 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 10 finished: insertInto at SparkDataManager.scala:61, took 0.065547 s
2018-10-11 21:36:17,457 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:17,458 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:17,491 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:17,492 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:17,507 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:17,508 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:17,527 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:17,527 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:17,544 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:17,545 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:17,560 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:17,560 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:17,592 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:17,592 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2928798
2018-10-11 21:36:17,631 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:36:17,631 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test'.
2018-10-11 21:36:17,632 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:17,632 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:17,636 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:17,636 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:17,647 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:17,647 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:17,705 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:17,706 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:17,706 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:17,707 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:17,735 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 11.415903 ms
2018-10-11 21:36:17,768 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 19.784842 ms
2018-10-11 21:36:17,772 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_21 stored as values in memory (estimated size 230.0 KB, free 1390.2 MB)
2018-10-11 21:36:17,779 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_21_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.2 MB)
2018-10-11 21:36:17,780 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_21_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:17,781 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 21 from show at SparkTPCDSTest.scala:35
2018-10-11 21:36:17,781 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 8394445 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:17,791 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: show at SparkTPCDSTest.scala:35
2018-10-11 21:36:17,792 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 11 (show at SparkTPCDSTest.scala:35) with 1 output partitions
2018-10-11 21:36:17,792 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 15 (show at SparkTPCDSTest.scala:35)
2018-10-11 21:36:17,792 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:17,792 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:17,792 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 15 (MapPartitionsRDD[51] at show at SparkTPCDSTest.scala:35), which has no missing parents
2018-10-11 21:36:17,797 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_22 stored as values in memory (estimated size 17.7 KB, free 1390.2 MB)
2018-10-11 21:36:17,798 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_22_piece0 stored as bytes in memory (estimated size 6.6 KB, free 1390.2 MB)
2018-10-11 21:36:17,799 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_22_piece0 in memory on 192.168.0.206:34485 (size: 6.6 KB, free: 1391.2 MB)
2018-10-11 21:36:17,799 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 22 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:17,801 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[51] at show at SparkTPCDSTest.scala:35) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:17,801 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 15.0 with 1 tasks
2018-10-11 21:36:17,802 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 15.0 (TID 15, localhost, executor driver, partition 0, PROCESS_LOCAL, 8537 bytes)
2018-10-11 21:36:17,802 INFO [Executor task launch worker for task 15] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 15.0 (TID 15)
2018-10-11 21:36:17,807 INFO [Executor task launch worker for task 15] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:17,812 INFO [Executor task launch worker for task 15] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:17,819 INFO [Executor task launch worker for task 15] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:17,824 INFO [Executor task launch worker for task 15] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:17,830 INFO [Executor task launch worker for task 15] o.a.h.i.c.CodecPool [CodecPool.java:179] Got brand-new decompressor [.snappy]
2018-10-11 21:36:17,833 INFO [Executor task launch worker for task 15] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 9 ms. row count = 1
2018-10-11 21:36:17,863 INFO [Executor task launch worker for task 15] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:17,867 INFO [Executor task launch worker for task 15] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:17,875 INFO [Executor task launch worker for task 15] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:17,881 INFO [Executor task launch worker for task 15] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 15.0 (TID 15). 1753 bytes result sent to driver
2018-10-11 21:36:17,883 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 15.0 (TID 15) in 82 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:17,883 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 15.0, whose tasks have all completed, from pool 
2018-10-11 21:36:17,884 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 15 (show at SparkTPCDSTest.scala:35) finished in 0.091 s
2018-10-11 21:36:17,885 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 11 finished: show at SparkTPCDSTest.scala:35, took 0.093706 s
2018-10-11 21:36:17,906 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:17,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:17,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:17,997 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:17,997 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:18,007 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,008 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,008 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,008 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:18,009 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:18,011 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:18,011 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:18,024 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:18,025 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:18,175 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:18,176 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:18,176 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:18,177 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:18,221 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 27.021244 ms
2018-10-11 21:36:18,227 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_23 stored as values in memory (estimated size 230.0 KB, free 1389.9 MB)
2018-10-11 21:36:18,237 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_23_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1389.9 MB)
2018-10-11 21:36:18,238 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_23_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:18,239 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 23 from count at SparkDataManager.scala:47
2018-10-11 21:36:18,240 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 8394445 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:18,258 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:18,259 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 54 (count at SparkDataManager.scala:47)
2018-10-11 21:36:18,260 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 12 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:18,260 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 17 (count at SparkDataManager.scala:47)
2018-10-11 21:36:18,260 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 16)
2018-10-11 21:36:18,260 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 16)
2018-10-11 21:36:18,260 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 16 (MapPartitionsRDD[54] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:18,263 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_24 stored as values in memory (estimated size 26.4 KB, free 1389.9 MB)
2018-10-11 21:36:18,264 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_24_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1389.9 MB)
2018-10-11 21:36:18,265 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_24_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.2 MB)
2018-10-11 21:36:18,265 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 24 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:18,266 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[54] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:18,266 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 16.0 with 1 tasks
2018-10-11 21:36:18,267 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 16.0 (TID 16, localhost, executor driver, partition 0, PROCESS_LOCAL, 8526 bytes)
2018-10-11 21:36:18,267 INFO [Executor task launch worker for task 16] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 16.0 (TID 16)
2018-10-11 21:36:18,271 INFO [Executor task launch worker for task 16] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:18,275 INFO [Executor task launch worker for task 16] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:18,281 INFO [Executor task launch worker for task 16] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:18,284 INFO [Executor task launch worker for task 16] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:18,285 INFO [Executor task launch worker for task 16] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:18,289 WARN [Executor task launch worker for task 16] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,289 WARN [Executor task launch worker for task 16] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,289 WARN [Executor task launch worker for task 16] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,290 INFO [Executor task launch worker for task 16] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:18,292 INFO [Executor task launch worker for task 16] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:18,297 INFO [Executor task launch worker for task 16] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:18,302 INFO [Executor task launch worker for task 16] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 16.0 (TID 16). 1651 bytes result sent to driver
2018-10-11 21:36:18,303 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 16.0 (TID 16) in 36 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:18,303 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 16.0, whose tasks have all completed, from pool 
2018-10-11 21:36:18,304 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 16 (count at SparkDataManager.scala:47) finished in 0.043 s
2018-10-11 21:36:18,304 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:18,304 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:18,304 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 17)
2018-10-11 21:36:18,304 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:18,305 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 17 (MapPartitionsRDD[57] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:18,306 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_25 stored as values in memory (estimated size 7.4 KB, free 1389.9 MB)
2018-10-11 21:36:18,308 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_25_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1389.9 MB)
2018-10-11 21:36:18,308 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_25_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:18,309 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 25 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:18,310 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[57] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:18,310 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 17.0 with 1 tasks
2018-10-11 21:36:18,311 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 17.0 (TID 17, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:18,311 INFO [Executor task launch worker for task 17] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 17.0 (TID 17)
2018-10-11 21:36:18,313 INFO [Executor task launch worker for task 17] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:18,313 INFO [Executor task launch worker for task 17] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:18,315 INFO [Executor task launch worker for task 17] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 17.0 (TID 17). 1775 bytes result sent to driver
2018-10-11 21:36:18,315 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 17.0 (TID 17) in 5 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:18,315 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 17.0, whose tasks have all completed, from pool 
2018-10-11 21:36:18,316 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 17 (count at SparkDataManager.scala:47) finished in 0.011 s
2018-10-11 21:36:18,316 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 12 finished: count at SparkDataManager.scala:47, took 0.057301 s
2018-10-11 21:36:18,317 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:18,318 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:18,321 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:18,321 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:18,331 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:18,331 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:18,509 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:18,510 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:18,510 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:18,511 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:18,559 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 28.658748 ms
2018-10-11 21:36:18,563 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_26 stored as values in memory (estimated size 230.0 KB, free 1389.7 MB)
2018-10-11 21:36:18,573 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_26_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1389.6 MB)
2018-10-11 21:36:18,574 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_26_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:18,576 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 26 from count at SparkDataManager.scala:47
2018-10-11 21:36:18,577 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 8394445 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:18,590 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:18,591 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 60 (count at SparkDataManager.scala:47)
2018-10-11 21:36:18,591 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 13 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:18,591 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 19 (count at SparkDataManager.scala:47)
2018-10-11 21:36:18,591 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 18)
2018-10-11 21:36:18,591 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 18)
2018-10-11 21:36:18,592 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 18 (MapPartitionsRDD[60] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:18,593 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_27 stored as values in memory (estimated size 26.4 KB, free 1389.6 MB)
2018-10-11 21:36:18,595 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_27_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1389.6 MB)
2018-10-11 21:36:18,596 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_27_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.2 MB)
2018-10-11 21:36:18,597 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 27 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:18,597 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[60] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:18,597 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 18.0 with 1 tasks
2018-10-11 21:36:18,598 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 18.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 8526 bytes)
2018-10-11 21:36:18,598 INFO [Executor task launch worker for task 18] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 18.0 (TID 18)
2018-10-11 21:36:18,602 INFO [Executor task launch worker for task 18] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:18,605 INFO [Executor task launch worker for task 18] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:18,610 INFO [Executor task launch worker for task 18] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:18,613 INFO [Executor task launch worker for task 18] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:18,614 INFO [Executor task launch worker for task 18] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:18,616 WARN [Executor task launch worker for task 18] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,616 WARN [Executor task launch worker for task 18] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,616 WARN [Executor task launch worker for task 18] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:18,616 INFO [Executor task launch worker for task 18] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:18,618 INFO [Executor task launch worker for task 18] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:18,625 INFO [Executor task launch worker for task 18] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:18,630 INFO [Executor task launch worker for task 18] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 18.0 (TID 18). 1651 bytes result sent to driver
2018-10-11 21:36:18,630 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 18.0 (TID 18) in 32 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:18,630 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 18.0, whose tasks have all completed, from pool 
2018-10-11 21:36:18,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 18 (count at SparkDataManager.scala:47) finished in 0.039 s
2018-10-11 21:36:18,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:18,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:18,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 19)
2018-10-11 21:36:18,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:18,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 19 (MapPartitionsRDD[63] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:18,633 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_28 stored as values in memory (estimated size 7.4 KB, free 1389.6 MB)
2018-10-11 21:36:18,635 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1389.6 MB)
2018-10-11 21:36:18,635 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_28_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:18,636 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 28 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:18,637 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[63] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:18,638 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 19.0 with 1 tasks
2018-10-11 21:36:18,639 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 19.0 (TID 19, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:18,639 INFO [Executor task launch worker for task 19] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 19.0 (TID 19)
2018-10-11 21:36:18,642 INFO [Executor task launch worker for task 19] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:18,642 INFO [Executor task launch worker for task 19] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:18,644 INFO [Executor task launch worker for task 19] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 19.0 (TID 19). 1775 bytes result sent to driver
2018-10-11 21:36:18,645 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 19.0 (TID 19) in 7 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:18,645 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 19.0, whose tasks have all completed, from pool 
2018-10-11 21:36:18,646 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 19 (count at SparkDataManager.scala:47) finished in 0.014 s
2018-10-11 21:36:18,646 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 13 finished: count at SparkDataManager.scala:47, took 0.055923 s
2018-10-11 21:36:18,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 40.959488 ms
2018-10-11 21:36:18,786 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:18,786 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:18,845 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:18,847 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:18,848 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:18,876 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:55
2018-10-11 21:36:18,877 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 14 (insertInto at SparkDataManager.scala:55) with 1 output partitions
2018-10-11 21:36:18,877 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 20 (insertInto at SparkDataManager.scala:55)
2018-10-11 21:36:18,877 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:18,877 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:18,878 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 20 (MapPartitionsRDD[65] at insertInto at SparkDataManager.scala:55), which has no missing parents
2018-10-11 21:36:18,898 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_29 stored as values in memory (estimated size 131.8 KB, free 1389.5 MB)
2018-10-11 21:36:18,905 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_29_piece0 stored as bytes in memory (estimated size 46.7 KB, free 1389.4 MB)
2018-10-11 21:36:18,906 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_29_piece0 in memory on 192.168.0.206:34485 (size: 46.7 KB, free: 1391.1 MB)
2018-10-11 21:36:18,906 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 29 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:18,907 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[65] at insertInto at SparkDataManager.scala:55) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:18,907 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 20.0 with 1 tasks
2018-10-11 21:36:18,908 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 20.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 9044 bytes)
2018-10-11 21:36:18,909 INFO [Executor task launch worker for task 20] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 20.0 (TID 20)
2018-10-11 21:36:18,922 INFO [Executor task launch worker for task 20] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:18,922 INFO [Executor task launch worker for task 20] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:18,923 INFO [Executor task launch worker for task 20] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:18,923 INFO [Executor task launch worker for task 20] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:18,924 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:18,925 INFO [Executor task launch worker for task 20] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:18,927 INFO [Executor task launch worker for task 20] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "database",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "executionDateTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "sparkConfig",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "queries",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "id",
          "type" : "short",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "businessQuestion",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "queryClass",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "statements",
          "type" : {
            "type" : "array",
            "elementType" : {
              "type" : "struct",
              "fields" : [ {
                "name" : "id",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              }, {
                "name" : "text",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              } ]
            },
            "containsNull" : true
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

       
2018-10-11 21:36:18,972 INFO [Executor task launch worker for task 20] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 902
2018-10-11 21:36:18,978 INFO [Executor task launch worker for task 20] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213618_0020_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs/_temporary/0/task_20181011213618_0020_m_000000
2018-10-11 21:36:18,978 INFO [Executor task launch worker for task 20] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213618_0020_m_000000_0: Committed
2018-10-11 21:36:18,979 INFO [Executor task launch worker for task 20] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 20.0 (TID 20). 1996 bytes result sent to driver
2018-10-11 21:36:18,980 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 20.0 (TID 20) in 71 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:18,980 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 20.0, whose tasks have all completed, from pool 
2018-10-11 21:36:18,980 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 20 (insertInto at SparkDataManager.scala:55) finished in 0.102 s
2018-10-11 21:36:18,980 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 14 finished: insertInto at SparkDataManager.scala:55, took 0.104023 s
2018-10-11 21:36:18,997 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:18,998 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:19,000 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:19,001 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:19,005 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:19,006 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:19,021 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:19,022 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:19,036 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:19,037 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:19,069 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs
2018-10-11 21:36:19,069 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs	
2018-10-11 21:36:19,101 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_runs
2018-10-11 21:36:19,101 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_runs to 10189
2018-10-11 21:36:19,145 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test1' ...
2018-10-11 21:36:19,177 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:19,178 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:19,180 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 509
2018-10-11 21:36:19,180 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 554
2018-10-11 21:36:19,180 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 638
2018-10-11 21:36:19,182 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 455
2018-10-11 21:36:19,182 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 502
2018-10-11 21:36:19,182 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 380
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 653
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 528
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 436
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 588
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 619
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 397
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 416
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 647
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 614
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 513
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 640
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 525
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 556
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 651
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 552
2018-10-11 21:36:19,183 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 649
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 386
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 390
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 622
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 413
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 412
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 696
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 688
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 524
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 381
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 684
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 504
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 662
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 438
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 667
2018-10-11 21:36:19,184 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 535
2018-10-11 21:36:19,185 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 602
2018-10-11 21:36:19,185 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 407
2018-10-11 21:36:19,186 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_17_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.2 MB)
2018-10-11 21:36:19,186 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 371
2018-10-11 21:36:19,186 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 534
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 607
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 375
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 474
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 403
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 414
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 499
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 592
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 579
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 482
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 559
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 603
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 615
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 629
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 689
2018-10-11 21:36:19,187 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 635
2018-10-11 21:36:19,188 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_22_piece0 on 192.168.0.206:34485 in memory (size: 6.6 KB, free: 1391.2 MB)
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 545
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 431
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 553
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 417
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 488
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 580
2018-10-11 21:36:19,189 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 396
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 570
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 408
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 433
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 515
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 387
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 620
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 623
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 342
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 698
2018-10-11 21:36:19,190 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 597
2018-10-11 21:36:19,190 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:36:19,191 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_28_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:19,192 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 476
2018-10-11 21:36:19,192 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 479
2018-10-11 21:36:19,192 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 555
2018-10-11 21:36:19,192 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:19,192 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 661
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 453
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 464
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 642
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 340
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 501
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 423
2018-10-11 21:36:19,193 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:19,193 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 596
2018-10-11 21:36:19,194 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 374
2018-10-11 21:36:19,194 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 593
2018-10-11 21:36:19,194 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 652
2018-10-11 21:36:19,194 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 547
2018-10-11 21:36:19,194 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 503
2018-10-11 21:36:19,194 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 519
2018-10-11 21:36:19,195 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_15_piece0 on 192.168.0.206:34485 in memory (size: 3.9 KB, free: 1391.2 MB)
2018-10-11 21:36:19,197 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 432
2018-10-11 21:36:19,197 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 609
2018-10-11 21:36:19,197 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 566
2018-10-11 21:36:19,198 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_16_piece0 on 192.168.0.206:34485 in memory (size: 201.0 B, free: 1391.2 MB)
2018-10-11 21:36:19,199 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 587
2018-10-11 21:36:19,199 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 493
2018-10-11 21:36:19,199 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 656
2018-10-11 21:36:19,199 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 546
2018-10-11 21:36:19,200 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 670
2018-10-11 21:36:19,200 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 424
2018-10-11 21:36:19,200 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 470
2018-10-11 21:36:19,200 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 675
2018-10-11 21:36:19,200 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 4
2018-10-11 21:36:19,201 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 498
2018-10-11 21:36:19,201 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 631
2018-10-11 21:36:19,198 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:19,201 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 395
2018-10-11 21:36:19,201 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 617
2018-10-11 21:36:19,201 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:19,201 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 537
2018-10-11 21:36:19,202 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 384
2018-10-11 21:36:19,202 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 624
2018-10-11 21:36:19,202 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 428
2018-10-11 21:36:19,204 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_21_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:19,205 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 445
2018-10-11 21:36:19,207 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_27_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.2 MB)
2018-10-11 21:36:19,210 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 604
2018-10-11 21:36:19,210 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 664
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 582
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 690
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 452
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 418
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 610
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 687
2018-10-11 21:36:19,211 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 543
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 645
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 339
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 565
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 378
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 693
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 601
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 606
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 581
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 425
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 691
2018-10-11 21:36:19,212 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 441
2018-10-11 21:36:19,213 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 404
2018-10-11 21:36:19,213 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 481
2018-10-11 21:36:19,216 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_19_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:19,217 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 447
2018-10-11 21:36:19,217 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 434
2018-10-11 21:36:19,218 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 458
2018-10-11 21:36:19,218 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 578
2018-10-11 21:36:19,219 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_23_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:19,220 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 589
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 616
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 505
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 658
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 460
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 370
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 446
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 341
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 430
2018-10-11 21:36:19,221 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 422
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 605
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 448
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 420
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 669
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 621
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 549
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 511
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 409
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 544
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 491
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 666
2018-10-11 21:36:19,222 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 376
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 406
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 612
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 663
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 450
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 548
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 506
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 451
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 657
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 440
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 695
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 449
2018-10-11 21:36:19,223 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 393
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 477
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 382
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 533
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 391
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 654
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 419
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 600
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 426
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 518
2018-10-11 21:36:19,224 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 487
2018-10-11 21:36:19,226 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 5
2018-10-11 21:36:19,227 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 644
2018-10-11 21:36:19,227 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 508
2018-10-11 21:36:19,227 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 551
2018-10-11 21:36:19,227 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 494
2018-10-11 21:36:19,227 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 468
2018-10-11 21:36:19,228 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 512
2018-10-11 21:36:19,228 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 462
2018-10-11 21:36:19,228 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 489
2018-10-11 21:36:19,228 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 562
2018-10-11 21:36:19,228 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 628
2018-10-11 21:36:19,229 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 383
2018-10-11 21:36:19,230 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 475
2018-10-11 21:36:19,233 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_14_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.2 MB)
2018-10-11 21:36:19,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 389
2018-10-11 21:36:19,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 442
2018-10-11 21:36:19,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 639
2018-10-11 21:36:19,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 673
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 507
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 671
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 483
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 542
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 595
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 571
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 594
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 636
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 531
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 400
2018-10-11 21:36:19,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 456
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 372
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 401
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 536
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 399
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 697
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 575
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 694
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 454
2018-10-11 21:36:19,237 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 500
2018-10-11 21:36:19,240 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_24_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.3 MB)
2018-10-11 21:36:19,241 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 598
2018-10-11 21:36:19,241 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 550
2018-10-11 21:36:19,241 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 655
2018-10-11 21:36:19,241 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 338
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 576
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 681
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 530
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 497
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 514
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 572
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 379
2018-10-11 21:36:19,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 411
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 461
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 627
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 683
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 510
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 444
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 394
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 625
2018-10-11 21:36:19,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 527
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 471
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 480
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 586
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 398
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 473
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 486
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 516
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 539
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 492
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 485
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 633
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 388
2018-10-11 21:36:19,244 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 472
2018-10-11 21:36:19,246 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_25_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:19,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 648
2018-10-11 21:36:19,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 529
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 558
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 679
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 574
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 665
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 541
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 392
2018-10-11 21:36:19,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 517
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 3
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 599
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 459
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 410
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 373
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 569
2018-10-11 21:36:19,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 495
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 680
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 427
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 686
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 415
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 608
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 677
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 540
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 585
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 685
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 523
2018-10-11 21:36:19,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 641
2018-10-11 21:36:19,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 496
2018-10-11 21:36:19,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 557
2018-10-11 21:36:19,256 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_26_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:19,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 618
2018-10-11 21:36:19,260 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 682
2018-10-11 21:36:19,260 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 520
2018-10-11 21:36:19,261 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 668
2018-10-11 21:36:19,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 465
2018-10-11 21:36:19,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 561
2018-10-11 21:36:19,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 567
2018-10-11 21:36:19,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 626
2018-10-11 21:36:19,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 699
2018-10-11 21:36:19,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 564
2018-10-11 21:36:19,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 676
2018-10-11 21:36:19,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 584
2018-10-11 21:36:19,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 429
2018-10-11 21:36:19,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 369
2018-10-11 21:36:19,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 692
2018-10-11 21:36:19,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 590
2018-10-11 21:36:19,265 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 659
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 385
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 678
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 457
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 532
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 563
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 405
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 439
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 467
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 469
2018-10-11 21:36:19,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 368
2018-10-11 21:36:19,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 484
2018-10-11 21:36:19,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 613
2018-10-11 21:36:19,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 568
2018-10-11 21:36:19,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 443
2018-10-11 21:36:19,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 573
2018-10-11 21:36:19,277 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_29_piece0 on 192.168.0.206:34485 in memory (size: 46.7 KB, free: 1391.3 MB)
2018-10-11 21:36:19,278 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:19,279 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:19,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 466
2018-10-11 21:36:19,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 577
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 650
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 660
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 421
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 490
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 646
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 435
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 674
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 672
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 478
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 538
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 591
2018-10-11 21:36:19,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 377
2018-10-11 21:36:19,282 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_18_piece0 on 192.168.0.206:34485 in memory (size: 8.1 KB, free: 1391.3 MB)
2018-10-11 21:36:19,283 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:19,283 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:19,283 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 630
2018-10-11 21:36:19,284 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 526
2018-10-11 21:36:19,285 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_20_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.4 MB)
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 632
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 637
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 560
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 583
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 437
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 611
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 522
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 463
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 402
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 643
2018-10-11 21:36:19,287 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 521
2018-10-11 21:36:19,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 634
2018-10-11 21:36:19,304 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_30 stored as values in memory (estimated size 226.1 KB, free 1390.9 MB)
2018-10-11 21:36:19,314 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_30_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.9 MB)
2018-10-11 21:36:19,315 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_30_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.4 MB)
2018-10-11 21:36:19,316 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 30 from collect at Statement.scala:24
2018-10-11 21:36:19,317 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:19,334 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:19,335 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 69 (collect at Statement.scala:24)
2018-10-11 21:36:19,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 15 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:19,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 22 (collect at Statement.scala:24)
2018-10-11 21:36:19,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 21)
2018-10-11 21:36:19,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 21)
2018-10-11 21:36:19,337 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 21 (MapPartitionsRDD[69] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:19,342 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_31 stored as values in memory (estimated size 11.1 KB, free 1390.9 MB)
2018-10-11 21:36:19,344 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_31_piece0 stored as bytes in memory (estimated size 5.4 KB, free 1390.9 MB)
2018-10-11 21:36:19,345 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_31_piece0 in memory on 192.168.0.206:34485 (size: 5.4 KB, free: 1391.4 MB)
2018-10-11 21:36:19,346 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 31 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:19,346 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[69] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:19,347 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 21.0 with 1 tasks
2018-10-11 21:36:19,348 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 21.0 (TID 21, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:19,348 INFO [Executor task launch worker for task 21] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 21.0 (TID 21)
2018-10-11 21:36:19,351 INFO [Executor task launch worker for task 21] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:19,356 INFO [Executor task launch worker for task 21] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 21.0 (TID 21). 1680 bytes result sent to driver
2018-10-11 21:36:19,357 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 21.0 (TID 21) in 10 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:19,357 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 21.0, whose tasks have all completed, from pool 
2018-10-11 21:36:19,357 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 21 (collect at Statement.scala:24) finished in 0.019 s
2018-10-11 21:36:19,358 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:19,358 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:19,358 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 22)
2018-10-11 21:36:19,358 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:19,358 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 22 (MapPartitionsRDD[72] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:19,360 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_32 stored as values in memory (estimated size 7.4 KB, free 1390.9 MB)
2018-10-11 21:36:19,361 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_32_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.9 MB)
2018-10-11 21:36:19,362 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_32_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:19,362 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 32 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:19,363 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[72] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:19,363 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 22.0 with 1 tasks
2018-10-11 21:36:19,364 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 22.0 (TID 22, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:19,364 INFO [Executor task launch worker for task 22] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 22.0 (TID 22)
2018-10-11 21:36:19,367 INFO [Executor task launch worker for task 22] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:19,368 INFO [Executor task launch worker for task 22] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:19,369 INFO [Executor task launch worker for task 22] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 22.0 (TID 22). 1782 bytes result sent to driver
2018-10-11 21:36:19,370 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 22.0 (TID 22) in 6 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:19,370 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 22.0, whose tasks have all completed, from pool 
2018-10-11 21:36:19,371 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 22 (collect at Statement.scala:24) finished in 0.012 s
2018-10-11 21:36:19,372 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 15 finished: collect at Statement.scala:24, took 0.037347 s
2018-10-11 21:36:19,373 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:36:19,412 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:19,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:19,470 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:19,473 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:19,473 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:19,506 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:19,507 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 16 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:19,507 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 23 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:19,508 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:19,508 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:19,508 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 23 (MapPartitionsRDD[74] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:19,525 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_33 stored as values in memory (estimated size 131.0 KB, free 1390.8 MB)
2018-10-11 21:36:19,526 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_33_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.7 MB)
2018-10-11 21:36:19,528 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_33_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.3 MB)
2018-10-11 21:36:19,532 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 33 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:19,533 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[74] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:19,533 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 23.0 with 1 tasks
2018-10-11 21:36:19,534 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 23.0 (TID 23, localhost, executor driver, partition 0, PROCESS_LOCAL, 10313 bytes)
2018-10-11 21:36:19,534 INFO [Executor task launch worker for task 23] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 23.0 (TID 23)
2018-10-11 21:36:19,544 INFO [Executor task launch worker for task 23] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:19,545 INFO [Executor task launch worker for task 23] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:19,545 INFO [Executor task launch worker for task 23] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:19,545 INFO [Executor task launch worker for task 23] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:19,546 INFO [Executor task launch worker for task 23] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:19,548 INFO [Executor task launch worker for task 23] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:19,554 INFO [Executor task launch worker for task 23] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2245
2018-10-11 21:36:19,557 INFO [Executor task launch worker for task 23] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213619_0023_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213619_0023_m_000000
2018-10-11 21:36:19,558 INFO [Executor task launch worker for task 23] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213619_0023_m_000000_0: Committed
2018-10-11 21:36:19,558 INFO [Executor task launch worker for task 23] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 23.0 (TID 23). 1996 bytes result sent to driver
2018-10-11 21:36:19,559 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 23.0 (TID 23) in 25 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:19,559 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 23.0, whose tasks have all completed, from pool 
2018-10-11 21:36:19,560 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 23 (insertInto at SparkDataManager.scala:61) finished in 0.051 s
2018-10-11 21:36:19,560 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 16 finished: insertInto at SparkDataManager.scala:61, took 0.053698 s
2018-10-11 21:36:19,568 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:19,569 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:19,583 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:19,583 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:19,586 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:19,587 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:19,598 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:19,598 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:19,612 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:19,612 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:19,626 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:19,626 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:19,669 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:19,669 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2940159
2018-10-11 21:36:19,778 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:19,778 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:19,781 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:19,782 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:19,792 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:19,793 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:19,814 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:19,814 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:19,815 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:19,815 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:19,815 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:19,815 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:19,816 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:19,816 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:19,828 INFO [broadcast-exchange-1] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_34 stored as values in memory (estimated size 226.1 KB, free 1390.5 MB)
2018-10-11 21:36:19,834 INFO [broadcast-exchange-1] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_34_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.5 MB)
2018-10-11 21:36:19,835 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_34_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:19,835 INFO [broadcast-exchange-1] o.a.s.SparkContext [Logging.scala:54] Created broadcast 34 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:19,836 INFO [broadcast-exchange-1] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:19,846 INFO [broadcast-exchange-1] o.a.s.SparkContext [Logging.scala:54] Starting job: run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:19,846 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 17 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2018-10-11 21:36:19,847 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 24 (run at ThreadPoolExecutor.java:1149)
2018-10-11 21:36:19,847 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:19,847 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:19,847 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 24 (MapPartitionsRDD[78] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2018-10-11 21:36:19,849 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_35 stored as values in memory (estimated size 7.5 KB, free 1390.5 MB)
2018-10-11 21:36:19,850 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1390.5 MB)
2018-10-11 21:36:19,850 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_35_piece0 in memory on 192.168.0.206:34485 (size: 3.9 KB, free: 1391.3 MB)
2018-10-11 21:36:19,851 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 35 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:19,851 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[78] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:19,852 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 24.0 with 1 tasks
2018-10-11 21:36:19,853 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 24.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 8375 bytes)
2018-10-11 21:36:19,853 INFO [Executor task launch worker for task 24] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 24.0 (TID 24)
2018-10-11 21:36:19,856 INFO [Executor task launch worker for task 24] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:19,859 INFO [Executor task launch worker for task 24] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 24.0 (TID 24). 1503 bytes result sent to driver
2018-10-11 21:36:19,860 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 24.0 (TID 24) in 8 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:19,860 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 24.0, whose tasks have all completed, from pool 
2018-10-11 21:36:19,861 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 24 (run at ThreadPoolExecutor.java:1149) finished in 0.013 s
2018-10-11 21:36:19,861 INFO [broadcast-exchange-1] o.a.s.s.DAGScheduler [Logging.scala:54] Job 17 finished: run at ThreadPoolExecutor.java:1149, took 0.015246 s
2018-10-11 21:36:19,863 INFO [broadcast-exchange-1] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_36 stored as values in memory (estimated size 5.8 KB, free 1390.5 MB)
2018-10-11 21:36:19,864 INFO [broadcast-exchange-1] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_36_piece0 stored as bytes in memory (estimated size 201.0 B, free 1390.5 MB)
2018-10-11 21:36:19,865 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_36_piece0 in memory on 192.168.0.206:34485 (size: 201.0 B, free: 1391.3 MB)
2018-10-11 21:36:19,866 INFO [broadcast-exchange-1] o.a.s.SparkContext [Logging.scala:54] Created broadcast 36 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:19,870 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_37 stored as values in memory (estimated size 226.1 KB, free 1390.2 MB)
2018-10-11 21:36:19,881 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_37_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.2 MB)
2018-10-11 21:36:19,881 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_37_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:19,882 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 37 from collect at Statement.scala:24
2018-10-11 21:36:19,883 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:19,907 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:19,908 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 84 (collect at Statement.scala:24)
2018-10-11 21:36:19,909 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 18 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:19,909 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 26 (collect at Statement.scala:24)
2018-10-11 21:36:19,909 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 25)
2018-10-11 21:36:19,909 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 25)
2018-10-11 21:36:19,910 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 25 (MapPartitionsRDD[84] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:19,912 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_38 stored as values in memory (estimated size 17.5 KB, free 1390.2 MB)
2018-10-11 21:36:19,914 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_38_piece0 stored as bytes in memory (estimated size 8.1 KB, free 1390.2 MB)
2018-10-11 21:36:19,915 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_38_piece0 in memory on 192.168.0.206:34485 (size: 8.1 KB, free: 1391.2 MB)
2018-10-11 21:36:19,915 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 38 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:19,916 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[84] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:19,916 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 25.0 with 1 tasks
2018-10-11 21:36:19,917 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 25.0 (TID 25, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:19,917 INFO [Executor task launch worker for task 25] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 25.0 (TID 25)
2018-10-11 21:36:19,920 INFO [Executor task launch worker for task 25] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:19,927 INFO [Executor task launch worker for task 25] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 25.0 (TID 25). 2669 bytes result sent to driver
2018-10-11 21:36:19,928 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 25.0 (TID 25) in 11 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:19,928 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 25.0, whose tasks have all completed, from pool 
2018-10-11 21:36:19,929 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 25 (collect at Statement.scala:24) finished in 0.019 s
2018-10-11 21:36:19,929 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:19,929 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:19,929 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 26)
2018-10-11 21:36:19,929 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:19,930 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 26 (MapPartitionsRDD[87] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:19,931 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_39 stored as values in memory (estimated size 7.4 KB, free 1390.2 MB)
2018-10-11 21:36:19,933 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_39_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.2 MB)
2018-10-11 21:36:19,933 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_39_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:19,934 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 39 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:19,935 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[87] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:19,935 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 26.0 with 1 tasks
2018-10-11 21:36:19,936 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 26.0 (TID 26, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:19,937 INFO [Executor task launch worker for task 26] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 26.0 (TID 26)
2018-10-11 21:36:19,939 INFO [Executor task launch worker for task 26] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:19,939 INFO [Executor task launch worker for task 26] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:19,940 INFO [Executor task launch worker for task 26] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 26.0 (TID 26). 1782 bytes result sent to driver
2018-10-11 21:36:19,941 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 26.0 (TID 26) in 6 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:19,941 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 26.0, whose tasks have all completed, from pool 
2018-10-11 21:36:19,941 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 26 (collect at Statement.scala:24) finished in 0.011 s
2018-10-11 21:36:19,942 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 18 finished: collect at Statement.scala:24, took 0.034131 s
2018-10-11 21:36:19,943 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:36:19,973 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:19,973 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:20,017 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:20,018 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:20,019 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:20,034 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:20,035 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 19 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:20,035 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 27 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:20,035 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:20,036 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:20,036 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 27 (MapPartitionsRDD[89] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:20,047 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_40 stored as values in memory (estimated size 131.0 KB, free 1390.0 MB)
2018-10-11 21:36:20,048 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_40_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.0 MB)
2018-10-11 21:36:20,048 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_40_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:20,049 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 40 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:20,049 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[89] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:20,049 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 27.0 with 1 tasks
2018-10-11 21:36:20,050 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 27.0 (TID 27, localhost, executor driver, partition 0, PROCESS_LOCAL, 10844 bytes)
2018-10-11 21:36:20,050 INFO [Executor task launch worker for task 27] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 27.0 (TID 27)
2018-10-11 21:36:20,057 INFO [Executor task launch worker for task 27] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:20,058 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:20,059 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:20,059 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:20,059 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:20,059 INFO [Executor task launch worker for task 27] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:20,060 INFO [Executor task launch worker for task 27] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:20,075 INFO [Executor task launch worker for task 27] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2783
2018-10-11 21:36:20,079 INFO [Executor task launch worker for task 27] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213620_0027_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213620_0027_m_000000
2018-10-11 21:36:20,080 INFO [Executor task launch worker for task 27] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213620_0027_m_000000_0: Committed
2018-10-11 21:36:20,081 INFO [Executor task launch worker for task 27] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 27.0 (TID 27). 1996 bytes result sent to driver
2018-10-11 21:36:20,082 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 27.0 (TID 27) in 32 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:20,082 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 27.0, whose tasks have all completed, from pool 
2018-10-11 21:36:20,090 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 27 (insertInto at SparkDataManager.scala:61) finished in 0.054 s
2018-10-11 21:36:20,091 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 19 finished: insertInto at SparkDataManager.scala:61, took 0.056027 s
2018-10-11 21:36:20,100 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:20,101 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:20,120 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:20,121 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:20,125 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:20,125 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:20,144 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:20,144 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:20,161 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:20,162 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:20,182 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:20,183 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:20,220 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:20,221 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2953831
2018-10-11 21:36:20,273 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:36:20,274 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test1'.
2018-10-11 21:36:20,274 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,323 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:20,323 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:20,358 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:20,358 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:20,370 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,370 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,371 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:20,372 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:20,375 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:20,376 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:20,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:20,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:20,511 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:20,512 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:20,512 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:20,513 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:20,555 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 26.006737 ms
2018-10-11 21:36:20,558 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_41 stored as values in memory (estimated size 230.0 KB, free 1389.8 MB)
2018-10-11 21:36:20,566 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_41_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1389.8 MB)
2018-10-11 21:36:20,567 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_41_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:20,568 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 41 from count at SparkDataManager.scala:47
2018-10-11 21:36:20,570 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 12593101 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:20,586 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:20,587 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 93 (count at SparkDataManager.scala:47)
2018-10-11 21:36:20,587 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 20 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:20,587 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 29 (count at SparkDataManager.scala:47)
2018-10-11 21:36:20,587 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 28)
2018-10-11 21:36:20,587 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 28)
2018-10-11 21:36:20,588 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 28 (MapPartitionsRDD[93] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:20,591 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_42 stored as values in memory (estimated size 26.4 KB, free 1389.7 MB)
2018-10-11 21:36:20,593 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_42_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1389.7 MB)
2018-10-11 21:36:20,593 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_42_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.2 MB)
2018-10-11 21:36:20,594 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 42 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:20,595 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[93] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:20,595 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 28.0 with 1 tasks
2018-10-11 21:36:20,596 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 28.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 8677 bytes)
2018-10-11 21:36:20,596 INFO [Executor task launch worker for task 28] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 28.0 (TID 28)
2018-10-11 21:36:20,601 INFO [Executor task launch worker for task 28] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:20,605 INFO [Executor task launch worker for task 28] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:20,611 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:20,614 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:20,615 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:20,617 WARN [Executor task launch worker for task 28] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,618 WARN [Executor task launch worker for task 28] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,618 WARN [Executor task launch worker for task 28] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,618 INFO [Executor task launch worker for task 28] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:20,621 INFO [Executor task launch worker for task 28] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:20,626 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:20,629 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:20,630 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:20,631 WARN [Executor task launch worker for task 28] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,632 WARN [Executor task launch worker for task 28] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,632 WARN [Executor task launch worker for task 28] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,632 INFO [Executor task launch worker for task 28] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:20,634 INFO [Executor task launch worker for task 28] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:20,641 INFO [Executor task launch worker for task 28] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:20,645 INFO [Executor task launch worker for task 28] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 28.0 (TID 28). 1651 bytes result sent to driver
2018-10-11 21:36:20,646 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 28.0 (TID 28) in 51 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:20,646 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 28.0, whose tasks have all completed, from pool 
2018-10-11 21:36:20,647 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 28 (count at SparkDataManager.scala:47) finished in 0.059 s
2018-10-11 21:36:20,647 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:20,647 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:20,647 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 29)
2018-10-11 21:36:20,647 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:20,647 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 29 (MapPartitionsRDD[96] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:20,648 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_43 stored as values in memory (estimated size 7.4 KB, free 1389.7 MB)
2018-10-11 21:36:20,649 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_43_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1389.7 MB)
2018-10-11 21:36:20,650 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_43_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:20,650 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 43 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:20,651 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[96] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:20,651 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 29.0 with 1 tasks
2018-10-11 21:36:20,652 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 29.0 (TID 29, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:20,653 INFO [Executor task launch worker for task 29] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 29.0 (TID 29)
2018-10-11 21:36:20,656 INFO [Executor task launch worker for task 29] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:20,656 INFO [Executor task launch worker for task 29] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:20,657 INFO [Executor task launch worker for task 29] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 29.0 (TID 29). 1775 bytes result sent to driver
2018-10-11 21:36:20,657 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 29.0 (TID 29) in 6 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:20,657 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 29.0, whose tasks have all completed, from pool 
2018-10-11 21:36:20,658 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 29 (count at SparkDataManager.scala:47) finished in 0.010 s
2018-10-11 21:36:20,658 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 20 finished: count at SparkDataManager.scala:47, took 0.072038 s
2018-10-11 21:36:20,660 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:20,660 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:20,663 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:20,663 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:20,674 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:20,674 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:20,827 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:20,828 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:20,831 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:20,831 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:20,875 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 26.641987 ms
2018-10-11 21:36:20,879 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_44 stored as values in memory (estimated size 230.0 KB, free 1389.5 MB)
2018-10-11 21:36:20,889 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_44_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1389.5 MB)
2018-10-11 21:36:20,890 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_44_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.1 MB)
2018-10-11 21:36:20,891 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 44 from count at SparkDataManager.scala:47
2018-10-11 21:36:20,892 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 12593101 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:20,909 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:20,910 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 99 (count at SparkDataManager.scala:47)
2018-10-11 21:36:20,910 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 21 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:20,910 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 31 (count at SparkDataManager.scala:47)
2018-10-11 21:36:20,911 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 30)
2018-10-11 21:36:20,911 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 30)
2018-10-11 21:36:20,911 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 30 (MapPartitionsRDD[99] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:20,913 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_45 stored as values in memory (estimated size 26.4 KB, free 1389.4 MB)
2018-10-11 21:36:20,915 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_45_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1389.4 MB)
2018-10-11 21:36:20,915 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_45_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.1 MB)
2018-10-11 21:36:20,916 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 45 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:20,916 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[99] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:20,916 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 30.0 with 1 tasks
2018-10-11 21:36:20,917 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 30.0 (TID 30, localhost, executor driver, partition 0, PROCESS_LOCAL, 8677 bytes)
2018-10-11 21:36:20,918 INFO [Executor task launch worker for task 30] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 30.0 (TID 30)
2018-10-11 21:36:20,922 INFO [Executor task launch worker for task 30] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:20,926 INFO [Executor task launch worker for task 30] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:20,933 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:20,937 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:20,939 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 2 ms. row count = 1
2018-10-11 21:36:20,942 WARN [Executor task launch worker for task 30] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,942 WARN [Executor task launch worker for task 30] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,942 WARN [Executor task launch worker for task 30] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,943 INFO [Executor task launch worker for task 30] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:20,946 INFO [Executor task launch worker for task 30] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:20,952 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:20,955 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:20,957 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:20,959 WARN [Executor task launch worker for task 30] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,959 WARN [Executor task launch worker for task 30] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,960 WARN [Executor task launch worker for task 30] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:20,960 INFO [Executor task launch worker for task 30] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:20,962 INFO [Executor task launch worker for task 30] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:20,968 INFO [Executor task launch worker for task 30] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:20,975 INFO [Executor task launch worker for task 30] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 30.0 (TID 30). 1651 bytes result sent to driver
2018-10-11 21:36:20,975 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 30.0 (TID 30) in 58 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:20,975 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 30.0, whose tasks have all completed, from pool 
2018-10-11 21:36:20,976 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 30 (count at SparkDataManager.scala:47) finished in 0.064 s
2018-10-11 21:36:20,976 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:20,976 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:20,977 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 31)
2018-10-11 21:36:20,977 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:20,977 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 31 (MapPartitionsRDD[102] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:20,979 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_46 stored as values in memory (estimated size 7.4 KB, free 1389.4 MB)
2018-10-11 21:36:20,980 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1389.4 MB)
2018-10-11 21:36:20,980 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_46_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:20,981 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 46 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:20,981 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[102] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:20,981 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 31.0 with 1 tasks
2018-10-11 21:36:20,982 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 31.0 (TID 31, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:20,982 INFO [Executor task launch worker for task 31] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 31.0 (TID 31)
2018-10-11 21:36:20,985 INFO [Executor task launch worker for task 31] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:20,985 INFO [Executor task launch worker for task 31] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:20,987 INFO [Executor task launch worker for task 31] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 31.0 (TID 31). 1775 bytes result sent to driver
2018-10-11 21:36:20,987 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 31.0 (TID 31) in 5 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:20,988 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 31.0, whose tasks have all completed, from pool 
2018-10-11 21:36:20,988 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 31 (count at SparkDataManager.scala:47) finished in 0.010 s
2018-10-11 21:36:20,989 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 21 finished: count at SparkDataManager.scala:47, took 0.079447 s
2018-10-11 21:36:21,081 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 39.285469 ms
2018-10-11 21:36:21,117 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,117 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:21,150 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,151 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,151 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,173 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:55
2018-10-11 21:36:21,174 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 22 (insertInto at SparkDataManager.scala:55) with 1 output partitions
2018-10-11 21:36:21,174 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 32 (insertInto at SparkDataManager.scala:55)
2018-10-11 21:36:21,174 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:21,174 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:21,174 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 32 (MapPartitionsRDD[104] at insertInto at SparkDataManager.scala:55), which has no missing parents
2018-10-11 21:36:21,189 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_47 stored as values in memory (estimated size 131.8 KB, free 1389.3 MB)
2018-10-11 21:36:21,192 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_47_piece0 stored as bytes in memory (estimated size 46.7 KB, free 1389.2 MB)
2018-10-11 21:36:21,192 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_47_piece0 in memory on 192.168.0.206:34485 (size: 46.7 KB, free: 1391.1 MB)
2018-10-11 21:36:21,193 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 47 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:21,193 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[104] at insertInto at SparkDataManager.scala:55) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:21,193 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 32.0 with 1 tasks
2018-10-11 21:36:21,194 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 32.0 (TID 32, localhost, executor driver, partition 0, PROCESS_LOCAL, 9018 bytes)
2018-10-11 21:36:21,194 INFO [Executor task launch worker for task 32] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 32.0 (TID 32)
2018-10-11 21:36:21,204 INFO [Executor task launch worker for task 32] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,204 INFO [Executor task launch worker for task 32] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,205 INFO [Executor task launch worker for task 32] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:21,205 INFO [Executor task launch worker for task 32] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:21,206 INFO [Executor task launch worker for task 32] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:21,229 INFO [Executor task launch worker for task 32] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "database",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "executionDateTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "sparkConfig",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "queries",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "id",
          "type" : "short",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "businessQuestion",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "queryClass",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "statements",
          "type" : {
            "type" : "array",
            "elementType" : {
              "type" : "struct",
              "fields" : [ {
                "name" : "id",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              }, {
                "name" : "text",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              } ]
            },
            "containsNull" : true
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

       
2018-10-11 21:36:21,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 916
2018-10-11 21:36:21,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 717
2018-10-11 21:36:21,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 977
2018-10-11 21:36:21,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 718
2018-10-11 21:36:21,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 821
2018-10-11 21:36:21,235 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 745
2018-10-11 21:36:21,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 984
2018-10-11 21:36:21,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 779
2018-10-11 21:36:21,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 850
2018-10-11 21:36:21,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1066
2018-10-11 21:36:21,236 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 701
2018-10-11 21:36:21,237 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_39_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:21,238 INFO [Executor task launch worker for task 32] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 900
2018-10-11 21:36:21,241 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 8
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 731
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 958
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1004
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1044
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 710
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1009
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 985
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 809
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 946
2018-10-11 21:36:21,242 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 919
2018-10-11 21:36:21,243 INFO [Executor task launch worker for task 32] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213621_0032_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs/_temporary/0/task_20181011213621_0032_m_000000
2018-10-11 21:36:21,243 INFO [Executor task launch worker for task 32] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213621_0032_m_000000_0: Committed
2018-10-11 21:36:21,243 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 7
2018-10-11 21:36:21,244 INFO [Executor task launch worker for task 32] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 32.0 (TID 32). 2039 bytes result sent to driver
2018-10-11 21:36:21,244 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_42_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.1 MB)
2018-10-11 21:36:21,245 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 32.0 (TID 32) in 51 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:21,245 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 32.0, whose tasks have all completed, from pool 
2018-10-11 21:36:21,245 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 729
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 922
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 769
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 813
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 741
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 866
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 989
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 969
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 705
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 881
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1035
2018-10-11 21:36:21,246 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 926
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1036
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1022
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 950
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 771
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 878
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1005
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 719
2018-10-11 21:36:21,246 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 32 (insertInto at SparkDataManager.scala:55) finished in 0.071 s
2018-10-11 21:36:21,247 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 943
2018-10-11 21:36:21,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1008
2018-10-11 21:36:21,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 781
2018-10-11 21:36:21,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 828
2018-10-11 21:36:21,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 917
2018-10-11 21:36:21,248 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 22 finished: insertInto at SparkDataManager.scala:55, took 0.075179 s
2018-10-11 21:36:21,248 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 819
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 955
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 990
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 734
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 834
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 820
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 816
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1030
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1068
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 918
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 962
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 903
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 963
2018-10-11 21:36:21,249 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 885
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 923
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 932
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 761
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1054
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1050
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 973
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 976
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 706
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 722
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 785
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 894
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 814
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 910
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 763
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 793
2018-10-11 21:36:21,250 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1003
2018-10-11 21:36:21,251 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 970
2018-10-11 21:36:21,251 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1040
2018-10-11 21:36:21,251 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 902
2018-10-11 21:36:21,251 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_45_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.1 MB)
2018-10-11 21:36:21,252 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_33_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:21,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 702
2018-10-11 21:36:21,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 959
2018-10-11 21:36:21,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 870
2018-10-11 21:36:21,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 812
2018-10-11 21:36:21,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1057
2018-10-11 21:36:21,253 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 889
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1016
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 953
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 971
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 956
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 853
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 802
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 890
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 806
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 915
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 869
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 884
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 858
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 720
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 750
2018-10-11 21:36:21,254 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1055
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 827
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 852
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 807
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 801
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 837
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1038
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 936
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 772
2018-10-11 21:36:21,255 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 815
2018-10-11 21:36:21,256 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_43_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 974
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1018
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 826
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1064
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 733
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1039
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 804
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 742
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 920
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 948
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 982
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 927
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 700
2018-10-11 21:36:21,257 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 6
2018-10-11 21:36:21,258 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 979
2018-10-11 21:36:21,258 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_44_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1067
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 709
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 992
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1047
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 874
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1062
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 951
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 747
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 759
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1043
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 810
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 934
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 942
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 964
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 738
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 899
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 704
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 965
2018-10-11 21:36:21,259 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 840
2018-10-11 21:36:21,260 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_37_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.2 MB)
2018-10-11 21:36:21,260 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 774
2018-10-11 21:36:21,261 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 960
2018-10-11 21:36:21,261 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:21,261 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 724
2018-10-11 21:36:21,261 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1019
2018-10-11 21:36:21,261 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 744
2018-10-11 21:36:21,261 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:21,261 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_40_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 845
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 712
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 752
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1058
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1060
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 865
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 886
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 993
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 877
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 818
2018-10-11 21:36:21,262 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 740
2018-10-11 21:36:21,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 921
2018-10-11 21:36:21,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 794
2018-10-11 21:36:21,263 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:21,263 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 945
2018-10-11 21:36:21,263 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:21,264 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_46_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 999
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1002
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 790
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 721
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 891
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 871
2018-10-11 21:36:21,264 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 773
2018-10-11 21:36:21,265 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1006
2018-10-11 21:36:21,265 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_34_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 912
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1029
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 776
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 972
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 757
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 952
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 803
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 996
2018-10-11 21:36:21,266 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 836
2018-10-11 21:36:21,266 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,267 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_36_piece0 on 192.168.0.206:34485 in memory (size: 201.0 B, free: 1391.3 MB)
2018-10-11 21:36:21,267 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:21,272 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 933
2018-10-11 21:36:21,272 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 791
2018-10-11 21:36:21,272 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 796
2018-10-11 21:36:21,272 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 841
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 876
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 703
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 944
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 906
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 799
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 935
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 760
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 817
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 875
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 737
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 749
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 940
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 833
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 925
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 851
2018-10-11 21:36:21,273 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 930
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 883
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 838
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 787
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 780
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 986
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 798
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 730
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 966
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1026
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1061
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 861
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 788
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 822
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1011
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1024
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 823
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 797
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 713
2018-10-11 21:36:21,274 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 998
2018-10-11 21:36:21,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 961
2018-10-11 21:36:21,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 764
2018-10-11 21:36:21,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 864
2018-10-11 21:36:21,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 739
2018-10-11 21:36:21,275 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 854
2018-10-11 21:36:21,276 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_35_piece0 on 192.168.0.206:34485 in memory (size: 3.9 KB, free: 1391.3 MB)
2018-10-11 21:36:21,276 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1051
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 929
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 782
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 767
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 811
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 857
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 872
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1033
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 904
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 831
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 726
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 849
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 909
2018-10-11 21:36:21,277 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1007
2018-10-11 21:36:21,278 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 795
2018-10-11 21:36:21,278 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_30_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:21,279 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 928
2018-10-11 21:36:21,279 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 707
2018-10-11 21:36:21,279 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1041
2018-10-11 21:36:21,279 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 800
2018-10-11 21:36:21,279 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 862
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 753
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 825
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 957
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 846
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 892
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 863
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 981
2018-10-11 21:36:21,280 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 715
2018-10-11 21:36:21,281 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_32_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:21,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1037
2018-10-11 21:36:21,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 954
2018-10-11 21:36:21,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1001
2018-10-11 21:36:21,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 792
2018-10-11 21:36:21,281 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 860
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1020
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 824
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 893
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 914
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 895
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 995
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1025
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1010
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 842
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 847
2018-10-11 21:36:21,282 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,282 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 880
2018-10-11 21:36:21,283 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 905
2018-10-11 21:36:21,283 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:21,283 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 848
2018-10-11 21:36:21,283 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 832
2018-10-11 21:36:21,283 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1031
2018-10-11 21:36:21,284 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_38_piece0 on 192.168.0.206:34485 in memory (size: 8.1 KB, free: 1391.3 MB)
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 924
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1028
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1048
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1046
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1032
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 751
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1053
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 900
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1015
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 983
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 938
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 994
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 755
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 975
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 789
2018-10-11 21:36:21,285 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 887
2018-10-11 21:36:21,286 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 756
2018-10-11 21:36:21,286 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1052
2018-10-11 21:36:21,286 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 978
2018-10-11 21:36:21,286 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 783
2018-10-11 21:36:21,286 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 941
2018-10-11 21:36:21,286 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 980
2018-10-11 21:36:21,287 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_31_piece0 on 192.168.0.206:34485 in memory (size: 5.4 KB, free: 1391.3 MB)
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 844
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 896
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 968
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 867
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 939
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 727
2018-10-11 21:36:21,288 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 967
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 830
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 907
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 778
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 888
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 931
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 868
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 829
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 898
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 711
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 758
2018-10-11 21:36:21,289 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1034
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1063
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 808
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 908
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1017
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 911
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 714
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 777
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 879
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 997
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1027
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 937
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 725
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 991
2018-10-11 21:36:21,290 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1021
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 746
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 768
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1013
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 897
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 775
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 708
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 766
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 843
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 754
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 859
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 784
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1056
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1059
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 988
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1012
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 949
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 748
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 716
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1069
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 882
2018-10-11 21:36:21,291 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1049
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1014
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1045
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 805
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 762
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 855
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 723
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 786
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 743
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1065
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 735
2018-10-11 21:36:21,292 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 987
2018-10-11 21:36:21,293 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_41_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:21,293 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 856
2018-10-11 21:36:21,293 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 947
2018-10-11 21:36:21,293 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 835
2018-10-11 21:36:21,293 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1000
2018-10-11 21:36:21,293 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 736
2018-10-11 21:36:21,293 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,293 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 728
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1023
2018-10-11 21:36:21,294 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 873
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 839
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 770
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 901
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 732
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 913
2018-10-11 21:36:21,294 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 765
2018-10-11 21:36:21,295 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1042
2018-10-11 21:36:21,295 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 9
2018-10-11 21:36:21,309 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs
2018-10-11 21:36:21,309 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs	
2018-10-11 21:36:21,333 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_runs
2018-10-11 21:36:21,334 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_runs to 14767
2018-10-11 21:36:21,375 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test2' ...
2018-10-11 21:36:21,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:21,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:21,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 21:36:21,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:21,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:21,391 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test2
2018-10-11 21:36:21,392 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test2	
2018-10-11 21:36:21,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:21,426 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:21,426 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:21,426 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:21,439 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_48 stored as values in memory (estimated size 226.1 KB, free 1390.8 MB)
2018-10-11 21:36:21,446 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_48_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.7 MB)
2018-10-11 21:36:21,447 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_48_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:21,447 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 48 from collect at Statement.scala:24
2018-10-11 21:36:21,448 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4234675 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:21,464 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:21,465 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 108 (collect at Statement.scala:24)
2018-10-11 21:36:21,465 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 23 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:21,465 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 34 (collect at Statement.scala:24)
2018-10-11 21:36:21,465 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 33)
2018-10-11 21:36:21,465 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 33)
2018-10-11 21:36:21,466 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 33 (MapPartitionsRDD[108] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:21,467 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_49 stored as values in memory (estimated size 11.1 KB, free 1390.7 MB)
2018-10-11 21:36:21,468 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_49_piece0 stored as bytes in memory (estimated size 5.4 KB, free 1390.7 MB)
2018-10-11 21:36:21,468 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_49_piece0 in memory on 192.168.0.206:34485 (size: 5.4 KB, free: 1391.3 MB)
2018-10-11 21:36:21,469 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 49 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:21,469 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[108] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:21,469 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 33.0 with 1 tasks
2018-10-11 21:36:21,470 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 33.0 (TID 33, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:21,470 INFO [Executor task launch worker for task 33] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 33.0 (TID 33)
2018-10-11 21:36:21,472 INFO [Executor task launch worker for task 33] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test2/part-00000-85cea767-fab7-43f9-b942-a5e632825c4d-c000.snappy.parquet, range: 0-40371, partition values: [empty row]
2018-10-11 21:36:21,476 INFO [Executor task launch worker for task 33] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 33.0 (TID 33). 1680 bytes result sent to driver
2018-10-11 21:36:21,477 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 33.0 (TID 33) in 7 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:21,477 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 33.0, whose tasks have all completed, from pool 
2018-10-11 21:36:21,478 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 33 (collect at Statement.scala:24) finished in 0.012 s
2018-10-11 21:36:21,478 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:21,478 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:21,478 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 34)
2018-10-11 21:36:21,478 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:21,479 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 34 (MapPartitionsRDD[111] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:21,480 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_50 stored as values in memory (estimated size 7.4 KB, free 1390.7 MB)
2018-10-11 21:36:21,481 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_50_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.7 MB)
2018-10-11 21:36:21,482 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_50_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:21,482 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 50 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:21,483 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[111] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:21,483 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 34.0 with 1 tasks
2018-10-11 21:36:21,484 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 34.0 (TID 34, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:21,484 INFO [Executor task launch worker for task 34] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 34.0 (TID 34)
2018-10-11 21:36:21,486 INFO [Executor task launch worker for task 34] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:21,487 INFO [Executor task launch worker for task 34] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:21,489 INFO [Executor task launch worker for task 34] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 34.0 (TID 34). 1782 bytes result sent to driver
2018-10-11 21:36:21,489 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 34.0 (TID 34) in 5 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:21,489 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 34.0, whose tasks have all completed, from pool 
2018-10-11 21:36:21,490 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 34 (collect at Statement.scala:24) finished in 0.011 s
2018-10-11 21:36:21,490 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 23 finished: collect at Statement.scala:24, took 0.025650 s
2018-10-11 21:36:21,491 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 21:36:21,520 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:21,520 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:21,567 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,569 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,569 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,594 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:21,595 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 24 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:21,595 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 35 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:21,595 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:21,596 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:21,596 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 35 (MapPartitionsRDD[113] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:21,613 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_51 stored as values in memory (estimated size 131.0 KB, free 1390.6 MB)
2018-10-11 21:36:21,615 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_51_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.5 MB)
2018-10-11 21:36:21,616 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_51_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.3 MB)
2018-10-11 21:36:21,617 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 51 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:21,618 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[113] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:21,618 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 35.0 with 1 tasks
2018-10-11 21:36:21,619 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 35.0 (TID 35, localhost, executor driver, partition 0, PROCESS_LOCAL, 10313 bytes)
2018-10-11 21:36:21,619 INFO [Executor task launch worker for task 35] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 35.0 (TID 35)
2018-10-11 21:36:21,631 INFO [Executor task launch worker for task 35] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,631 INFO [Executor task launch worker for task 35] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:21,631 INFO [Executor task launch worker for task 35] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:21,632 INFO [Executor task launch worker for task 35] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:21,632 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:21,632 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:21,632 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:21,632 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:21,632 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:21,633 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:21,633 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:21,633 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:21,633 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:21,633 INFO [Executor task launch worker for task 35] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:21,635 INFO [Executor task launch worker for task 35] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:21,646 INFO [Executor task launch worker for task 35] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2245
2018-10-11 21:36:21,650 INFO [Executor task launch worker for task 35] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213621_0035_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213621_0035_m_000000
2018-10-11 21:36:21,651 INFO [Executor task launch worker for task 35] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213621_0035_m_000000_0: Committed
2018-10-11 21:36:21,652 INFO [Executor task launch worker for task 35] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 35.0 (TID 35). 1996 bytes result sent to driver
2018-10-11 21:36:21,657 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 35.0 (TID 35) in 38 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:21,657 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 35.0, whose tasks have all completed, from pool 
2018-10-11 21:36:21,658 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 35 (insertInto at SparkDataManager.scala:61) finished in 0.060 s
2018-10-11 21:36:21,658 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 24 finished: insertInto at SparkDataManager.scala:61, took 0.063534 s
2018-10-11 21:36:21,665 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:21,666 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:21,683 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:21,683 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:21,686 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:21,687 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:21,696 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:21,697 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:21,708 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:21,709 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:21,720 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:21,720 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:21,750 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:21,751 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2965194
2018-10-11 21:36:21,851 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 21:36:21,852 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test2'.
2018-10-11 21:36:21,852 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:21,882 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,882 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:21,912 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:21,912 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:21,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:21,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:21,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:21,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:21,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:22,066 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:22,066 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:22,067 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:22,067 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:22,076 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 4.035293 ms
2018-10-11 21:36:22,106 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 20.267714 ms
2018-10-11 21:36:22,108 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_52 stored as values in memory (estimated size 230.0 KB, free 1390.3 MB)
2018-10-11 21:36:22,114 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_52_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.3 MB)
2018-10-11 21:36:22,115 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_52_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:22,116 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 52 from collect at SparkDataManager.scala:88
2018-10-11 21:36:22,116 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 16791983 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:22,125 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:88
2018-10-11 21:36:22,126 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 25 (collect at SparkDataManager.scala:88) with 1 output partitions
2018-10-11 21:36:22,126 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 36 (collect at SparkDataManager.scala:88)
2018-10-11 21:36:22,126 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:22,126 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:22,127 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 36 (MapPartitionsRDD[117] at collect at SparkDataManager.scala:88), which has no missing parents
2018-10-11 21:36:22,128 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_53 stored as values in memory (estimated size 30.5 KB, free 1390.3 MB)
2018-10-11 21:36:22,128 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_53_piece0 stored as bytes in memory (estimated size 9.5 KB, free 1390.3 MB)
2018-10-11 21:36:22,129 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_53_piece0 in memory on 192.168.0.206:34485 (size: 9.5 KB, free: 1391.2 MB)
2018-10-11 21:36:22,129 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 53 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:22,129 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[117] at collect at SparkDataManager.scala:88) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:22,129 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 36.0 with 1 tasks
2018-10-11 21:36:22,130 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 36.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 8839 bytes)
2018-10-11 21:36:22,130 INFO [Executor task launch worker for task 36] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 36.0 (TID 36)
2018-10-11 21:36:22,134 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:22,137 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,142 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,144 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,145 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,148 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,148 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,148 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:22,151 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,156 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,159 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,160 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,162 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,162 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,162 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,162 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:22,165 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,170 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,174 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,175 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,177 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,177 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,177 WARN [Executor task launch worker for task 36] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,177 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:22,179 INFO [Executor task launch worker for task 36] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,183 INFO [Executor task launch worker for task 36] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:22,186 INFO [Executor task launch worker for task 36] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 36.0 (TID 36). 1402 bytes result sent to driver
2018-10-11 21:36:22,187 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 36.0 (TID 36) in 57 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:22,187 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 36.0, whose tasks have all completed, from pool 
2018-10-11 21:36:22,187 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 36 (collect at SparkDataManager.scala:88) finished in 0.060 s
2018-10-11 21:36:22,188 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 25 finished: collect at SparkDataManager.scala:88, took 0.061777 s
2018-10-11 21:36:22,190 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:22,190 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:22,192 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:22,192 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:22,199 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:22,199 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:22,293 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:22,293 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:22,293 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:22,294 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:22,316 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 16.277429 ms
2018-10-11 21:36:22,359 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 32.255399 ms
2018-10-11 21:36:22,371 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_54 stored as values in memory (estimated size 230.0 KB, free 1390.0 MB)
2018-10-11 21:36:22,382 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_54_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.0 MB)
2018-10-11 21:36:22,383 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_54_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:22,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 54 from collect at SparkDataManager.scala:66
2018-10-11 21:36:22,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 16791983 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:22,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:66
2018-10-11 21:36:22,398 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 26 (collect at SparkDataManager.scala:66) with 1 output partitions
2018-10-11 21:36:22,398 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 37 (collect at SparkDataManager.scala:66)
2018-10-11 21:36:22,399 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:22,399 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:22,399 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 37 (MapPartitionsRDD[120] at collect at SparkDataManager.scala:66), which has no missing parents
2018-10-11 21:36:22,401 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_55 stored as values in memory (estimated size 31.8 KB, free 1390.0 MB)
2018-10-11 21:36:22,402 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_55_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1390.0 MB)
2018-10-11 21:36:22,403 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_55_piece0 in memory on 192.168.0.206:34485 (size: 10.4 KB, free: 1391.2 MB)
2018-10-11 21:36:22,404 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 55 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:22,406 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[120] at collect at SparkDataManager.scala:66) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:22,406 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 37.0 with 1 tasks
2018-10-11 21:36:22,407 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 37.0 (TID 37, localhost, executor driver, partition 0, PROCESS_LOCAL, 8839 bytes)
2018-10-11 21:36:22,408 INFO [Executor task launch worker for task 37] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 37.0 (TID 37)
2018-10-11 21:36:22,413 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:22,417 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,423 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,427 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,428 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,430 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,431 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,431 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:22,434 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,441 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,444 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,445 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,448 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,448 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,448 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,449 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:22,452 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,459 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,462 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,463 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,466 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,466 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,466 WARN [Executor task launch worker for task 37] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,467 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:22,469 INFO [Executor task launch worker for task 37] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:22,475 INFO [Executor task launch worker for task 37] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:22,479 INFO [Executor task launch worker for task 37] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 37.0 (TID 37). 2167 bytes result sent to driver
2018-10-11 21:36:22,480 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 37.0 (TID 37) in 73 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:22,480 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 37.0, whose tasks have all completed, from pool 
2018-10-11 21:36:22,480 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 37 (collect at SparkDataManager.scala:66) finished in 0.080 s
2018-10-11 21:36:22,481 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 26 finished: collect at SparkDataManager.scala:66, took 0.082950 s
2018-10-11 21:36:22,482 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,483 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,487 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:22,488 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:22,490 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:22,491 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:22,500 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:22,500 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:22,565 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:22,566 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:22,566 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<statement: struct<id: string, text: string>, startTime: bigint, endTime: bigint, description: string, details: string ... 1 more field>
2018-10-11 21:36:22,566 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:22,581 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 8.527085 ms
2018-10-11 21:36:22,601 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 12.887696 ms
2018-10-11 21:36:22,604 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_56 stored as values in memory (estimated size 228.2 KB, free 1389.7 MB)
2018-10-11 21:36:22,610 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_56_piece0 stored as bytes in memory (estimated size 21.7 KB, free 1389.7 MB)
2018-10-11 21:36:22,611 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_56_piece0 in memory on 192.168.0.206:34485 (size: 21.7 KB, free: 1391.2 MB)
2018-10-11 21:36:22,612 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 56 from collect at SparkDataManager.scala:77
2018-10-11 21:36:22,612 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:22,629 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:77
2018-10-11 21:36:22,630 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 27 (collect at SparkDataManager.scala:77) with 8 output partitions
2018-10-11 21:36:22,630 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 38 (collect at SparkDataManager.scala:77)
2018-10-11 21:36:22,630 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:22,630 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:22,631 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 38 (MapPartitionsRDD[123] at collect at SparkDataManager.scala:77), which has no missing parents
2018-10-11 21:36:22,632 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_57 stored as values in memory (estimated size 16.2 KB, free 1389.7 MB)
2018-10-11 21:36:22,633 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_57_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1389.7 MB)
2018-10-11 21:36:22,634 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_57_piece0 in memory on 192.168.0.206:34485 (size: 6.7 KB, free: 1391.2 MB)
2018-10-11 21:36:22,634 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 57 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:22,635 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 8 missing tasks from ResultStage 38 (MapPartitionsRDD[123] at collect at SparkDataManager.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-10-11 21:36:22,635 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 38.0 with 8 tasks
2018-10-11 21:36:22,636 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 38.0 (TID 38, localhost, executor driver, partition 0, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:22,636 INFO [Executor task launch worker for task 38] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 38.0 (TID 38)
2018-10-11 21:36:22,640 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e31abc9-a228-4133-b02e-0f6cd7638510-c000.snappy.parquet, range: 0-13723, partition values: [empty row]
2018-10-11 21:36:22,642 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,653 INFO [Executor task launch worker for task 38] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 8.079888 ms
2018-10-11 21:36:22,655 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,657 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,658 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,659 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,660 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b1982672-2d41-4bff-8f45-9e6f8c921d5d-c000.snappy.parquet, range: 0-13722, partition values: [empty row]
2018-10-11 21:36:22,662 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,665 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,667 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,668 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,669 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,669 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e5316a0-c580-49fd-91d0-550955cd9bbc-c000.snappy.parquet, range: 0-13721, partition values: [empty row]
2018-10-11 21:36:22,672 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,675 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,677 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,677 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,678 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,679 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e15f7475-849c-45f2-9dcf-5bb2d5b69bd4-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:22,681 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,684 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,686 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,687 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,689 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,689 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a46c4b1e-6e4f-4382-a659-662347269774-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:22,693 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,695 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,697 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,698 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,699 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,699 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c7514033-0170-45b0-ad15-cb3097418dfb-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:22,702 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,704 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,706 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,707 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,708 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,708 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3723b14f-e12a-4f5a-b016-809b54de139c-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:22,711 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,713 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,715 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,716 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,718 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,718 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-07c85578-afbb-490a-bd9d-3837ab2a7baa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:22,720 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,723 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,726 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,727 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,728 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,728 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-603f4016-bd22-4ed6-b2c7-be69003d48fa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:22,731 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,734 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,736 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,737 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,739 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,739 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a6043ae9-7f4d-4793-ad20-a149ce2339bb-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:22,742 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,745 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,747 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,748 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,749 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,750 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8d65fb3-afa1-45c2-9354-709d30d7ae22-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:22,752 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,755 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,757 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,758 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,759 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,760 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0e52f33e-2792-451e-bfca-4fb2393d1315-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:22,767 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,771 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,773 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,774 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,775 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,776 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e001f918-e226-4b11-a792-e057310c71dc-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:22,778 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,782 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,783 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,784 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,785 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,786 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a17d8d1-3cec-4462-90f4-c0c77d76c5ce-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:22,788 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,790 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,792 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,793 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,794 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,795 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7992780f-0fa1-4c14-beca-90350009f018-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:22,798 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,801 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,803 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,804 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,808 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,809 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d57f919b-2b39-4c28-b8f3-61f31041cb83-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:22,811 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,813 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,815 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,815 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,816 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,816 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-735e4a0c-71b2-4dfe-9652-eddb95823bd0-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:22,818 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,821 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,822 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,823 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,824 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,824 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e132893-66ca-44f4-810d-b31bd307f3bf-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:22,826 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,829 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,831 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,832 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,833 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,834 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e32d4e0b-152b-41f5-9d5d-6319207c046a-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:22,836 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,839 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,841 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,841 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,842 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,843 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d24eaa68-02d1-43c1-b2da-ef94148610ed-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:22,845 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,848 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,850 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,850 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,851 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,852 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad3d0b86-19cf-474c-b3d4-796d7afaa4ae-c000.snappy.parquet, range: 0-13699, partition values: [empty row]
2018-10-11 21:36:22,855 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,858 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,860 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,860 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,861 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,862 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0247bc41-a2af-4b30-a5e0-5bc327f47cc2-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:22,864 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,866 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,868 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,869 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,870 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,871 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-26318166-52ec-4313-a4e5-3dc130f82d18-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:22,873 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,876 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,877 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,878 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,879 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,879 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2227df14-fd69-4e52-88fc-ffa906e8e96a-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:22,881 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,884 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,886 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,886 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,888 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,888 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-16e0e1c7-4724-40d4-8fe9-64c573377aac-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:22,890 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,893 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,895 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,896 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,897 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,897 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-12cc2ef7-5c98-45c7-bfca-9f6852b4f9cb-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:22,901 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,906 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,908 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,908 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,910 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,910 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5704978-3346-4cf0-a83a-fd67411e60c0-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:22,912 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,915 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,917 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,918 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,919 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,919 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-611aa927-9eff-4acc-a956-1419efe00229-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:22,921 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,924 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,926 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,927 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,927 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,928 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b6173355-0a8b-414c-9ed0-27985f88914d-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:22,930 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,933 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,935 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,936 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:22,937 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,937 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb54fd41-c4a7-41e8-a051-2493b4fc4fed-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:22,939 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,942 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,943 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,944 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,947 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,947 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9af47b28-32d6-4d91-b198-acee5bfe01dd-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:22,952 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,957 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,959 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,959 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,961 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,962 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-db7aecb7-d25c-4a59-ad0e-2bacf5772cb8-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:22,964 INFO [Executor task launch worker for task 38] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:22,967 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:22,969 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:22,969 INFO [Executor task launch worker for task 38] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:22,970 WARN [Executor task launch worker for task 38] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:22,972 INFO [Executor task launch worker for task 38] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 38.0 (TID 38). 1423 bytes result sent to driver
2018-10-11 21:36:22,973 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 1.0 in stage 38.0 (TID 39, localhost, executor driver, partition 1, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:22,973 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 38.0 (TID 38) in 337 ms on localhost (executor driver) (1/8)
2018-10-11 21:36:22,993 INFO [Executor task launch worker for task 39] o.a.s.e.Executor [Logging.scala:54] Running task 1.0 in stage 38.0 (TID 39)
2018-10-11 21:36:22,996 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ae6870f1-1f08-443f-8701-2a8a8fc509da-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,003 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,015 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,018 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,022 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 4 ms. row count = 1
2018-10-11 21:36:23,023 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,023 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d164cc8-eee4-4f35-920d-9fa0984dbb40-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,028 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,031 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,037 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,038 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,039 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,039 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b48d131c-6bd6-478f-9abc-96bd891b0584-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,043 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,045 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,047 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,053 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 6 ms. row count = 1
2018-10-11 21:36:23,055 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,055 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa252a4c-1080-4e94-9dc5-d3e35b8e70ea-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,058 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,060 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,062 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,063 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,068 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,069 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-586baf62-c0dd-443b-a8db-5822599f893a-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,071 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,074 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,076 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,076 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,077 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,078 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5ae7e692-d21c-4b25-9cdf-83f59ef40407-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,081 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,084 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,085 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,086 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,087 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,088 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5d04bbb7-c918-4d38-9348-7833d532fb6c-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,090 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,093 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,094 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,095 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,096 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,096 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-178dd25a-2181-4b65-970b-0683491eca83-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,098 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,101 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,103 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,104 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,105 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,105 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-90d7046c-97fc-4066-9720-fc9230ba0c54-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:23,107 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,110 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,111 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,112 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,113 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,113 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2bc1210e-eaa6-47cf-ba41-7e0b223d190f-c000.snappy.parquet, range: 0-13695, partition values: [empty row]
2018-10-11 21:36:23,116 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,119 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,121 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,122 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,123 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,123 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e7cb913a-50de-4609-aada-ae4eafd1de31-c000.snappy.parquet, range: 0-13694, partition values: [empty row]
2018-10-11 21:36:23,126 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,130 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,132 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,133 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,134 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,135 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f1d46819-9d92-46ba-9025-2adb0818719c-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:23,138 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,142 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,145 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,146 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,148 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,148 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b85e87fe-5382-4ea3-895f-13f9f032ef4b-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:23,150 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,154 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,155 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,157 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,159 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,159 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3831987b-b4b7-4617-a261-5bf721ece358-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:23,161 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,165 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,167 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,168 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,169 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,169 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-21e4180a-5c27-419b-ab1c-78aa6b2cb324-c000.snappy.parquet, range: 0-13691, partition values: [empty row]
2018-10-11 21:36:23,172 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,176 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,178 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,179 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,181 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,181 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c33bde23-fc3e-4548-a230-3eb78aac57fb-c000.snappy.parquet, range: 0-13690, partition values: [empty row]
2018-10-11 21:36:23,184 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,187 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,189 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,190 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,191 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,192 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0232792-2703-4cd0-a57b-e87871b66a88-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,194 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,198 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,201 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,202 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,203 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,203 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6f55e7b2-ed9f-4a3f-9a54-06887558e22d-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,206 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,211 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,212 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,213 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,214 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,215 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4384b7b2-8b67-4077-a182-89d2e37a4833-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,216 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,220 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,222 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,222 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,224 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,224 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19b79ab5-3105-4028-bbd0-c35b902de4a4-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,227 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,230 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,232 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,233 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,234 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,234 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6d920ff1-e3fc-40ac-a84e-386f13c16e67-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,237 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,239 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,241 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,242 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,244 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,244 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d63edb8d-fac0-4840-b019-b0b6e7a639d6-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,246 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,249 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,251 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,252 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,254 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,255 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09613236-c7ea-4e03-a2f6-c2dd85f36bed-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,257 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,261 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,263 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,264 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,264 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,265 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aafd0534-3203-4405-a282-194c9a55812b-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,267 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,270 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,271 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,272 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,273 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,273 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-018f2c55-3be6-41b0-b9cd-c3e9619e0cda-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,275 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,277 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,278 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,279 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,280 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,280 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b9578903-dcf4-4d51-9126-5a90dd27a1ef-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:23,281 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,284 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,285 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,286 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,287 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,288 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9f7a9a62-4bad-4d7b-8940-549d39238770-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:23,291 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,292 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,294 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,295 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,295 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,296 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8b0e504c-c3cf-496a-8730-d46b0b7df1b9-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:23,297 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,300 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,301 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,302 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,303 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,303 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-79b8fe3f-ef98-4c64-bedc-86c3fb9283fd-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:23,306 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,309 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,311 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,311 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,312 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,312 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57dddecf-aa33-4874-99e6-9748f780fe78-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:23,314 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,316 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,317 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,318 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,319 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,320 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7bfc2fc2-2b64-4ee4-83ae-ad0b645e1322-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:23,322 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,325 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,327 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,328 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,329 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,329 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a928c40-d93a-4600-a43e-d6259a8a7c26-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:23,332 INFO [Executor task launch worker for task 39] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,335 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,337 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,338 INFO [Executor task launch worker for task 39] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,339 WARN [Executor task launch worker for task 39] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,341 INFO [Executor task launch worker for task 39] o.a.s.e.Executor [Logging.scala:54] Finished task 1.0 in stage 38.0 (TID 39). 1423 bytes result sent to driver
2018-10-11 21:36:23,342 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 2.0 in stage 38.0 (TID 40, localhost, executor driver, partition 2, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:23,342 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 1.0 in stage 38.0 (TID 39) in 370 ms on localhost (executor driver) (2/8)
2018-10-11 21:36:23,342 INFO [Executor task launch worker for task 40] o.a.s.e.Executor [Logging.scala:54] Running task 2.0 in stage 38.0 (TID 40)
2018-10-11 21:36:23,344 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-62170ceb-3786-48a8-93d4-b567247e8037-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:23,346 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,348 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,349 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,349 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,350 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,351 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8643ee33-f4b3-4b9d-a32e-39e43cfe4ba7-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:23,353 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,355 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,357 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,358 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,359 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,359 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0680d8e3-d6a3-46d1-b2d8-76df28d3b02a-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:23,361 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,364 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,366 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,367 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,368 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,368 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aae2da02-ce8a-4dad-87c4-6a12978949be-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:23,394 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,394 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1097
2018-10-11 21:36:23,394 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1241
2018-10-11 21:36:23,394 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1194
2018-10-11 21:36:23,394 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1195
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1213
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1089
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1208
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1223
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1099
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1176
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1207
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1127
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1152
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1212
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1169
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1253
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1108
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1167
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1125
2018-10-11 21:36:23,395 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1173
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1214
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1117
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1216
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1161
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1189
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1104
2018-10-11 21:36:23,396 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1081
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1170
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1201
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1220
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1235
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1232
2018-10-11 21:36:23,396 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1100
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1155
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1199
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1237
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1184
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1163
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1148
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1118
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1095
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1231
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1087
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1226
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1172
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1103
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1119
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1248
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1146
2018-10-11 21:36:23,397 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1121
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1128
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1247
2018-10-11 21:36:23,398 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1166
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1229
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1078
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1233
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1110
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1206
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1177
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1183
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1257
2018-10-11 21:36:23,398 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1112
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1191
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1217
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1140
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1132
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1182
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1168
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1246
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1236
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1242
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1165
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1116
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1109
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1106
2018-10-11 21:36:23,399 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1224
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1162
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1113
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1105
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1154
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1218
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1151
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1114
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1186
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1164
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1083
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1073
2018-10-11 21:36:23,400 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1094
2018-10-11 21:36:23,401 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,402 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_52_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:23,403 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1221
2018-10-11 21:36:23,403 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1082
2018-10-11 21:36:23,403 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1133
2018-10-11 21:36:23,403 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1135
2018-10-11 21:36:23,403 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1178
2018-10-11 21:36:23,403 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1141
2018-10-11 21:36:23,404 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,405 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8eae07df-593a-43cc-ac4a-2be464a53d3f-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:23,405 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_50_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:23,406 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1230
2018-10-11 21:36:23,406 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1211
2018-10-11 21:36:23,407 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,408 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_51_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:23,409 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1090
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1197
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1157
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1179
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1111
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1243
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1070
2018-10-11 21:36:23,410 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1091
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1075
2018-10-11 21:36:23,411 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1187
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1144
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1079
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1234
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1227
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1143
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1196
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1145
2018-10-11 21:36:23,411 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1244
2018-10-11 21:36:23,412 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1180
2018-10-11 21:36:23,412 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1215
2018-10-11 21:36:23,412 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1171
2018-10-11 21:36:23,413 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,413 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_53_piece0 on 192.168.0.206:34485 in memory (size: 9.5 KB, free: 1391.2 MB)
2018-10-11 21:36:23,413 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,415 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1225
2018-10-11 21:36:23,415 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a663e89f-b19d-4f56-bfd7-d844367b5dc2-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1258
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1085
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1156
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1098
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1192
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1190
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1159
2018-10-11 21:36:23,415 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1175
2018-10-11 21:36:23,416 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1219
2018-10-11 21:36:23,416 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_54_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:23,417 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,417 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1174
2018-10-11 21:36:23,419 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_55_piece0 on 192.168.0.206:34485 in memory (size: 10.4 KB, free: 1391.3 MB)
2018-10-11 21:36:23,420 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,420 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1188
2018-10-11 21:36:23,420 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1101
2018-10-11 21:36:23,420 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1138
2018-10-11 21:36:23,420 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1158
2018-10-11 21:36:23,420 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1254
2018-10-11 21:36:23,421 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1071
2018-10-11 21:36:23,421 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1255
2018-10-11 21:36:23,421 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,422 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_47_piece0 on 192.168.0.206:34485 in memory (size: 46.7 KB, free: 1391.3 MB)
2018-10-11 21:36:23,422 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,423 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1139
2018-10-11 21:36:23,423 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,423 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1210
2018-10-11 21:36:23,423 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1142
2018-10-11 21:36:23,423 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2318bf98-30f1-4cb7-89ff-f54e8f26ee77-c000.snappy.parquet, range: 0-13670, partition values: [empty row]
2018-10-11 21:36:23,423 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1185
2018-10-11 21:36:23,423 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1124
2018-10-11 21:36:23,424 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1077
2018-10-11 21:36:23,424 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1122
2018-10-11 21:36:23,424 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1080
2018-10-11 21:36:23,425 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_49_piece0 on 192.168.0.206:34485 in memory (size: 5.4 KB, free: 1391.3 MB)
2018-10-11 21:36:23,425 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,426 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1204
2018-10-11 21:36:23,427 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1131
2018-10-11 21:36:23,427 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1134
2018-10-11 21:36:23,427 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1252
2018-10-11 21:36:23,427 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1072
2018-10-11 21:36:23,427 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1086
2018-10-11 21:36:23,427 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1123
2018-10-11 21:36:23,428 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 10
2018-10-11 21:36:23,428 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1130
2018-10-11 21:36:23,428 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1126
2018-10-11 21:36:23,428 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1092
2018-10-11 21:36:23,428 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,428 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1147
2018-10-11 21:36:23,429 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1251
2018-10-11 21:36:23,429 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1239
2018-10-11 21:36:23,429 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1228
2018-10-11 21:36:23,429 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1256
2018-10-11 21:36:23,429 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1198
2018-10-11 21:36:23,430 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,430 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_48_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.4 MB)
2018-10-11 21:36:23,431 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1193
2018-10-11 21:36:23,431 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1153
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1084
2018-10-11 21:36:23,432 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 2 ms. row count = 1
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1238
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1209
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1129
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1115
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1149
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1102
2018-10-11 21:36:23,432 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1202
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1245
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1074
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1181
2018-10-11 21:36:23,433 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1249
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1120
2018-10-11 21:36:23,433 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-854fe582-8eec-43ce-b79a-f5a9b836d2fd-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1107
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1205
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1200
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1240
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1203
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1096
2018-10-11 21:36:23,433 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1222
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1137
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1250
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1093
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1136
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1076
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1160
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1150
2018-10-11 21:36:23,434 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1088
2018-10-11 21:36:23,435 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,437 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,439 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,440 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,442 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,442 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5793a580-0e06-4393-afec-1a4eefbbd626-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,444 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,446 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,448 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,449 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,450 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,450 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e7b564a-5bd2-4701-bdea-b107946c9b0c-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,452 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,454 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,456 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,456 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,457 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,457 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d10c8980-7d22-4a6d-99ec-053805bdf9ec-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,459 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,462 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,464 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,465 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,466 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,466 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-59858d75-788e-4333-9577-bd4947258eff-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,468 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,471 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,473 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,473 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,474 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,475 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-274749d2-43f1-4c5d-81fe-d888dc9dff47-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,477 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,480 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,481 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,482 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,482 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,483 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ddde392-a677-4c60-92fc-49cd2be2c0f1-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:23,486 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,489 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,492 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,493 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,494 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,494 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b273f389-8ffb-4cbf-b108-6506095ae632-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:23,498 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,501 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,503 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,503 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,504 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,504 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7233e2d1-9862-45dc-9824-4e41f2c2867e-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:23,507 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,509 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,511 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,511 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,512 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,512 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-86a299de-f8fc-46fe-86a9-60a4b26a9b38-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:23,514 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,516 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,517 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,518 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,518 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,519 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-defaedb6-dcf3-4709-916c-a3e955107f58-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:23,520 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,522 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,523 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,523 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,524 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,524 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a837d014-9d5d-454e-a7f6-8f2fb06c37d0-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:23,525 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,527 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,528 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,529 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,530 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,530 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-45a99307-69fd-4a6c-8a0c-91d96b83817a-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:23,532 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,533 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,535 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,535 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,535 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,536 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-29b6c682-0e21-4836-904c-33bbd018067c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:23,537 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,540 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,541 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,541 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,542 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,542 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bbb741a3-045d-451c-a803-96b4d0493b04-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:23,543 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,545 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,547 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,547 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,548 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,548 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e33d9f4-1cd5-43f7-b792-6d744d0a7505-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:23,549 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,551 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,553 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,553 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,554 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,554 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-71b8e55a-40c5-4b5c-9dae-af4d5551ef2c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:23,556 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,558 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,559 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,559 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,560 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,560 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e2d6c34-9f3a-4d0c-9b3f-d0afa5e7e097-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,562 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,564 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,566 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,566 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,567 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,568 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aadda0f-0463-4bfe-9f32-62f122ef3ff4-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,569 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,572 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,573 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,574 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,574 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,575 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bea86d33-775a-4e73-97cd-927d76b9352f-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,577 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,579 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,581 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,582 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,583 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,583 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e0852e5-e63c-45eb-912a-d3964270da43-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,587 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,590 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,591 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,592 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,593 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,593 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4cc621ee-7546-4d9f-8ba0-c4c66f0adeec-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,595 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,598 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,600 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,601 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,602 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,602 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58131256-2685-47d1-abb5-a4357c095a67-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,604 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,607 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,609 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,610 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,611 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,611 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a5a52bbe-0c0c-41d6-a8be-9839d1629b37-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:23,613 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,615 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,617 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,618 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,618 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,619 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-869a108e-1223-444d-9a58-a852afc71fdc-c000.snappy.parquet, range: 0-13658, partition values: [empty row]
2018-10-11 21:36:23,621 INFO [Executor task launch worker for task 40] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,623 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,625 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,626 INFO [Executor task launch worker for task 40] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,626 WARN [Executor task launch worker for task 40] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,627 INFO [Executor task launch worker for task 40] o.a.s.e.Executor [Logging.scala:54] Finished task 2.0 in stage 38.0 (TID 40). 1466 bytes result sent to driver
2018-10-11 21:36:23,630 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 2.0 in stage 38.0 (TID 40) in 288 ms on localhost (executor driver) (3/8)
2018-10-11 21:36:23,633 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 3.0 in stage 38.0 (TID 41, localhost, executor driver, partition 3, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:23,634 INFO [Executor task launch worker for task 41] o.a.s.e.Executor [Logging.scala:54] Running task 3.0 in stage 38.0 (TID 41)
2018-10-11 21:36:23,636 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-666a2e39-edd2-4992-85ee-cd58cc019c85-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:23,638 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,640 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,642 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,643 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,644 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,644 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca11e9c0-3060-4a86-91dd-db9ce2ca7e5f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:23,647 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,650 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,651 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,653 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,654 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,655 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4a6558fd-1e62-4e07-9df1-400871fc5cec-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:23,660 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,662 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,664 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,664 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,665 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,666 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19122b21-1ceb-4cf4-8593-9fa7c8624d2d-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:23,667 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,670 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,671 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,672 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,673 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,673 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d7ef5b02-cddb-495a-aad3-f3679ca76b8f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:23,675 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,677 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,678 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,679 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,680 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,680 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-694f93cb-34fe-4247-821f-f5ca506e1459-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:23,681 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,684 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,686 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,687 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,688 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,688 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-701ae0c0-29df-4b97-839c-9c83df4e1b68-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:23,690 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,693 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,694 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,695 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,696 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,696 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5b6171fb-919a-4c7b-98aa-5dfe9d454475-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:23,698 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,701 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,702 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,703 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,703 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,704 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d21bcc-601f-4e42-b182-081c470c659d-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:23,705 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,707 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,709 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,710 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,710 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,710 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-600d8f03-7aac-4957-baf4-c554ff43494a-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:23,712 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,714 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,716 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,717 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,717 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,718 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e02ee93e-e796-42b0-bf02-5f75dfdc1c6c-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:23,719 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,721 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,723 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,724 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,724 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,725 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a90b0f72-7d9a-4a49-a68b-f290f02aa529-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:23,726 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,728 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,730 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,730 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,731 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,731 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f41d2183-74d9-4df5-be81-a2fb23bf9de7-c000.snappy.parquet, range: 0-11392, partition values: [empty row]
2018-10-11 21:36:23,733 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,735 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,737 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,738 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,739 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,740 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3fa69d1c-3edf-45d2-b0ac-1272168f52c9-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:23,741 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,744 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,745 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,746 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,746 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,746 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a0b202fc-2bf1-495f-9764-acfb0429a5c8-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:23,748 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,750 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,751 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,752 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,752 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,753 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-936820e3-b496-4cc2-8571-9635f4fdbfa3-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:23,754 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,756 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,757 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,758 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,758 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,759 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-eb4c07ef-088b-4a15-be2f-b273fd27d040-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:23,760 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,762 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,764 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,764 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,765 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,765 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4d67cb5-e9dc-4b19-acea-905574bf8522-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:23,766 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,769 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,770 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,771 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,771 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,772 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-224b809f-76f3-4919-a9b8-43b820662112-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:23,773 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,775 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,777 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,777 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,777 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,778 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7501282c-59f0-4b1a-88f5-82366eca768c-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:23,779 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,781 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,783 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,783 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,783 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,784 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3cad3b64-e9c9-41b4-a422-dc665b78c2c1-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:23,785 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,787 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,788 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,788 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,789 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,789 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-390c15b5-d721-4535-be3f-12b05cfbe30a-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:23,790 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,792 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,793 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,794 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,794 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,795 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d28ee0-4831-43b1-a27f-dab415ede7a0-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:23,796 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,798 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,799 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,800 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,800 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,800 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-56056fe1-8392-42d6-b2c1-4463f7381740-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:23,802 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,805 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,807 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,808 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,809 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,810 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dd3999fe-68a8-43bd-b968-d1abe12b7d68-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:23,812 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,815 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,816 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,817 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,818 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,818 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-40f0422a-c31f-4b28-870e-1da30f3d4173-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:23,821 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,824 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,826 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,826 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,827 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,827 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-81102eef-ee91-4905-a219-3a7d6ad68001-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:23,829 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,832 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,834 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,834 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,835 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,835 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a7708ac5-ff0d-4d9b-b829-432325d537bd-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:23,837 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,839 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,840 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,841 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,842 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,842 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-de923178-e27e-43df-9376-a8765b42b212-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:23,844 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,847 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,849 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,849 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,850 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,850 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2366af80-dffe-477b-8217-7f704825188b-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:23,855 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,858 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,859 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,859 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,860 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,860 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad60e729-defb-4a4e-99d9-9ca74fcf6a2d-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:23,861 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,863 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,864 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,865 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,865 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,866 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0cf40412-95c9-405c-9299-90b8afa2a414-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,868 INFO [Executor task launch worker for task 41] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,870 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,871 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,872 INFO [Executor task launch worker for task 41] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,872 WARN [Executor task launch worker for task 41] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,873 INFO [Executor task launch worker for task 41] o.a.s.e.Executor [Logging.scala:54] Finished task 3.0 in stage 38.0 (TID 41). 1423 bytes result sent to driver
2018-10-11 21:36:23,873 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 4.0 in stage 38.0 (TID 42, localhost, executor driver, partition 4, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:23,874 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 3.0 in stage 38.0 (TID 41) in 241 ms on localhost (executor driver) (4/8)
2018-10-11 21:36:23,876 INFO [Executor task launch worker for task 42] o.a.s.e.Executor [Logging.scala:54] Running task 4.0 in stage 38.0 (TID 42)
2018-10-11 21:36:23,878 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8f84cc33-f1d5-478f-87ce-0663d825408c-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,880 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,882 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,884 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,884 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,885 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,885 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64602a48-9868-4fdb-bbce-e23fdb1ff4ad-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,888 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,891 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,892 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,893 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,893 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,894 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-22885ce6-f042-455f-87e9-76587998ab27-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,895 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,896 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,897 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,898 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,899 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,899 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4f0d15a7-be71-41c6-9bcc-c559eb6c8a2a-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,900 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,906 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,908 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,908 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,909 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,909 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3bf97aa1-dd22-42f0-8f17-b66b1cd270a5-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,911 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,913 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,915 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,915 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,916 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,916 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57c3815d-7209-4ed6-90cc-5597db2fc409-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:23,918 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,919 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,920 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,921 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,921 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,922 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fff88882-0587-48ef-95f1-5dd0508833d6-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,923 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,925 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,926 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,927 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,927 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,928 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0df7b64b-c870-4b00-b1dc-59fec5d6c208-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,929 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,930 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,931 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,931 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,932 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,932 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-384bf5ec-e9b8-429f-8281-82432437fb02-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,933 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,936 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,937 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,938 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,938 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,939 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f08e80af-add7-42b2-b4ad-2a22d1ce2a56-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,940 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,942 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,943 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,943 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,943 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,944 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-615e7057-357a-4616-a738-affe25bf8b3a-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,945 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,946 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,947 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,947 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,948 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,948 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-73bd7a91-071c-42d6-a80a-c13db79d72d2-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,949 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,950 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,951 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,951 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,952 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,952 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dbf5cfde-c2ae-4ddf-bdee-0e703dcab699-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,953 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,954 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,955 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,956 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,956 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,956 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-70719782-4616-4b3d-82dd-7bc33b67022f-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,958 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,959 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,960 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,961 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,961 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,961 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1e620604-bee6-486e-b171-860be8b8b75e-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,963 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,964 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,965 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,965 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,966 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,966 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05b626a4-35ba-4e5f-9c93-d7f62d4d3027-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,967 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,970 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,972 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,972 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,973 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,973 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0ac09a5-d2c0-4844-95ed-3706f6eb6930-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,975 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,976 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,977 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,978 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,978 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,978 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e6fcf0f4-d681-401e-b5b6-7075100f24ee-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,979 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,981 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,982 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,982 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:23,982 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,983 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-00ef3320-f515-47a4-b26f-5b5caf94f65d-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,984 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,985 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,986 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,987 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,987 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,987 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ff4df6d7-b381-401d-987b-233a02e4d909-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,988 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,990 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,991 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,992 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,992 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,992 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f5fe52bb-22d0-4408-b9d4-9b3a37c89088-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,994 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:23,995 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:23,996 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:23,997 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:23,997 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:23,997 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a3db9e49-1db1-4e94-a28e-f73df6d27fa8-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:23,998 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,000 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,001 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,001 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,002 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,002 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-66b4c136-f4b2-4428-9331-2adf6a097511-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:24,003 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,005 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,006 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,006 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,007 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,007 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e3141f79-7a0f-4730-8468-e54a6c54c533-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:24,008 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,009 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,010 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,011 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,011 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,011 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0b0c8187-cf2f-44d8-88fc-49ce4dd62874-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:24,012 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,014 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,015 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,015 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,015 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,015 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-08d62d99-4b8f-4e06-8620-bba67557d59d-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:24,017 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,018 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,019 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,020 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,020 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,021 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64403a21-b2b7-43f7-a22d-5f7706bad082-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:24,023 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,025 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,027 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,027 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,028 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,028 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c1e3a260-62f4-4aeb-8a26-82616f973030-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:24,030 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,032 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,033 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,034 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,034 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,034 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9bff7912-93ac-4fc7-a1db-f8e566a70655-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:24,036 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,038 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,040 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,041 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,042 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,042 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-47728647-7a80-4592-ba7c-c3ebcb1e5224-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:24,044 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,046 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,047 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,048 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,049 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,049 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9b3bccf5-ad25-404d-9310-88a915f9b7a2-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:24,051 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,053 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,055 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,056 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,057 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,057 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-510a5951-088e-4dab-8dd0-c02c47869d79-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:24,059 INFO [Executor task launch worker for task 42] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,061 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,062 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,062 INFO [Executor task launch worker for task 42] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,063 WARN [Executor task launch worker for task 42] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,064 INFO [Executor task launch worker for task 42] o.a.s.e.Executor [Logging.scala:54] Finished task 4.0 in stage 38.0 (TID 42). 1423 bytes result sent to driver
2018-10-11 21:36:24,064 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 5.0 in stage 38.0 (TID 43, localhost, executor driver, partition 5, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:24,065 INFO [Executor task launch worker for task 43] o.a.s.e.Executor [Logging.scala:54] Running task 5.0 in stage 38.0 (TID 43)
2018-10-11 21:36:24,065 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 4.0 in stage 38.0 (TID 42) in 192 ms on localhost (executor driver) (5/8)
2018-10-11 21:36:24,067 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6650061f-a0ab-42f3-be0d-e29bba951689-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:24,068 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,070 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,071 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,071 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,072 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,072 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a73de2ae-2a91-46fe-a61e-d06104068675-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:24,074 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,076 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,078 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,078 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,079 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,079 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-60348a41-e7a8-4c02-9dbf-a4848152bf0a-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:24,080 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,082 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,084 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,084 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,085 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,085 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca9d3388-a32e-42aa-9b9a-f2ba826d1b2e-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:24,087 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,090 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,092 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,092 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,092 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,093 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46ee92fd-a03b-4ad7-a847-fac273921f57-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:24,094 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,096 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,098 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,098 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,099 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,099 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4317e6e3-7c07-49f4-9bea-564768382089-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:24,101 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,103 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,104 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,105 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,105 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,105 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dc5fe75d-ae31-4bd0-a866-d3274b2d629b-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:24,108 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,110 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,111 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,112 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,113 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,113 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-49c1ab9d-a225-4ba8-911f-7c8138662881-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:24,114 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,116 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,117 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,118 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,118 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,119 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a910a7f7-d2c2-4fb9-9907-6f0a12ef5816-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,120 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,122 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,123 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,124 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,125 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,125 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af526506-da3a-461a-b7d6-7576a7ca2dc6-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,126 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,128 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,129 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,129 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,130 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,130 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f7492e12-3b8e-43e6-b84c-e42d363116f9-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,132 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,133 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,134 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,135 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,135 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,136 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-272bed24-7d51-4fc6-9a8a-aba3fb69c4b8-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,137 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,140 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,141 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,142 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,143 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,143 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ecac671-fc2e-4df4-9315-bf4c516db607-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,145 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,147 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,148 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,148 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,149 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,149 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f2c3b673-16d6-4469-a5ed-13de2cc92f10-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,151 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,153 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,154 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,155 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,156 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,156 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a6947b4-4ad6-4ab6-8907-a8320e284bcb-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:24,157 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,159 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,160 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,160 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,161 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,161 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3a0fc732-ce1b-4da0-9cd5-162ab4573e6e-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,163 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,165 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,166 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,166 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,167 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,167 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a087b5c1-75ef-4738-bcac-2ee86aa294bd-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,169 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,170 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,172 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,172 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,173 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,173 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e0eae261-dec5-4554-b126-6e1829f78050-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,175 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,177 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,178 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,179 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,179 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,179 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-987c95c1-23dd-407a-98e6-0e238fe1473b-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,181 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,183 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,185 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,185 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,186 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,186 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb60375b-4e46-4d45-99c6-540adc7208c3-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,187 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,188 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,190 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,190 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,190 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,191 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-319157ef-58f7-4406-b9b1-6baf5b53b0a8-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,192 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,193 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,194 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,194 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,195 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,195 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8022bee4-ba1f-4f2d-87cc-7ae422111479-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,196 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,197 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,198 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,198 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,199 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,199 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-861da466-0eee-47d7-9ff5-971c6d7f5a93-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,200 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,202 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,203 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,203 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,204 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,204 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8a3516de-8b82-44d1-82a8-69aa660cbbdf-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,205 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,207 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,208 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,208 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,209 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,209 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f419bf0e-cdd8-4f6c-8beb-f4dedb61a260-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,210 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,212 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,212 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,213 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,213 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,213 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e766d71-e245-4297-857b-bdd21b58b6f4-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,214 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,216 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,216 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,217 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,217 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,217 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4c33fd2a-9846-4f6b-a1f0-dccf06433485-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,218 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,220 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,222 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,222 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,223 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,223 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8fc6b69-477b-42e7-9d1a-3c499d4d10b2-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:24,224 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,225 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,226 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,226 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,227 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,227 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9039fff7-2957-4b0e-992e-e87565413ec8-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,228 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,229 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,230 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,230 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,231 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,231 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ef4e6a53-30c6-43c0-8eff-8a34e34edf6b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,232 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,233 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,234 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,235 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,235 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,235 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a398596-1df5-4ca7-911f-e74abd3762dd-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,237 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,239 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,240 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,241 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,241 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,241 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58f4dff5-7214-4777-ba17-784e2cef12f4-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,243 INFO [Executor task launch worker for task 43] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,245 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,247 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,247 INFO [Executor task launch worker for task 43] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,248 WARN [Executor task launch worker for task 43] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,249 INFO [Executor task launch worker for task 43] o.a.s.e.Executor [Logging.scala:54] Finished task 5.0 in stage 38.0 (TID 43). 2746 bytes result sent to driver
2018-10-11 21:36:24,249 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 6.0 in stage 38.0 (TID 44, localhost, executor driver, partition 6, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:24,250 INFO [Executor task launch worker for task 44] o.a.s.e.Executor [Logging.scala:54] Running task 6.0 in stage 38.0 (TID 44)
2018-10-11 21:36:24,251 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 5.0 in stage 38.0 (TID 43) in 187 ms on localhost (executor driver) (6/8)
2018-10-11 21:36:24,252 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2422c402-71d9-41d7-9987-1d4ab7fc19fb-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,254 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,257 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,259 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,260 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,261 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,261 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0ff4b192-b8e8-4b6c-9828-f5d9ec00cbce-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,264 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,267 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,268 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,269 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,270 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,270 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a2d93e67-4023-47f3-9d4a-f50a9c8541fe-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,272 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,274 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,276 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,277 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,277 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,277 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a29231b4-e038-4ee0-88ef-1d68cfe9d2be-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,279 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,282 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,284 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,284 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,285 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,285 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-55b99cce-2e82-46d0-8e53-bee4918b429e-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,287 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,290 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,292 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,292 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,293 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,293 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9dc2a350-c1b5-4c5c-8f81-b54fcb2ffa3a-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,295 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,298 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,299 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,300 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,300 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,301 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1be87a1d-cfca-46a7-b3c5-5377826665a1-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,302 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,305 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,306 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,307 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,307 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,308 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ac9002e-c497-481d-b337-445644058c65-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,309 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,311 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,313 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,314 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,314 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,315 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7453f0e9-fcdc-45d9-aa4e-04e2742b8b2b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,317 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,319 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,321 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,321 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,322 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,322 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3ce2c6c2-8244-4825-9d79-b2816de2acda-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,323 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,326 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,327 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,328 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,328 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,329 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e32c7dc-dcea-456e-a878-c9c835248ea9-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,330 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,332 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,334 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,334 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,334 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,335 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b2dcecd1-372e-4f0b-9562-f96ebf3af8f6-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,336 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,339 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,341 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,341 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,342 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,342 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b266ce1f-aa2d-4c98-8bd7-38ee65369d89-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,343 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,345 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,347 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,347 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,348 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,348 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6c58af95-e7c8-4556-989d-8a8fc881fc54-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:24,350 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,352 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,354 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,354 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,355 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,355 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-da19fb6b-0851-4b26-8454-89e7d9d29409-c000.snappy.parquet, range: 0-11359, partition values: [empty row]
2018-10-11 21:36:24,357 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,359 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,361 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,361 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,362 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,362 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2646c07b-af07-4e07-8e28-e3b57bbb514c-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,364 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,366 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,367 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,368 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,368 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,369 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7fed2b39-6f8a-4f9e-8921-caa7441424ec-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,371 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,373 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,375 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,375 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,376 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,376 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f4b5fa6b-5fa7-4ccf-b1ac-7fe087ca4972-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,377 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,379 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,381 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,381 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,381 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,382 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e8617665-882c-4cd9-af6c-d70cf9357935-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,383 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,385 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,386 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,387 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,387 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,388 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9c42e760-f3d5-4337-93cc-2faeb5608d5f-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,389 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,391 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,392 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,393 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,393 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,393 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-992bc073-6936-47bb-9ce0-4bd6ff5312f3-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,394 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,396 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,398 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,398 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,399 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,399 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bdc96751-af2e-4c0d-aad9-9b6309410e26-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,400 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,402 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,404 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,404 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,405 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,405 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2a8062ca-53c1-4efb-887d-a5f15b2e2675-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:24,407 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,409 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,410 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,410 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,411 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,411 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-461919ce-4e71-49bc-ab59-c7e999f082df-c000.snappy.parquet, range: 0-11357, partition values: [empty row]
2018-10-11 21:36:24,412 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,414 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,415 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,415 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,416 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,416 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d184ff46-c4c0-4adb-993f-d1625bf18c93-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:24,418 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,419 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,420 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,421 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,421 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,422 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a542cfa-fb0f-420c-808c-c23861bc5e49-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:24,424 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,425 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,426 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,426 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,427 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,427 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3744292d-429a-4b99-8f87-0174a77a5f02-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:24,428 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,429 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,430 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,430 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,431 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,431 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ce13747d-84bc-4b83-bcbf-ed87cb408c63-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:24,432 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,434 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,436 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,436 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,436 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,436 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa3ef260-acd7-4ea3-b2bb-08ce39dc72a4-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:24,437 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,439 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,440 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,440 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,441 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,441 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09afdad3-a22e-46d0-8913-a98493735de8-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:24,442 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,444 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,445 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,445 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,446 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,446 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4de0605-1810-4659-8565-775942a08750-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:24,447 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,448 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,449 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,450 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,450 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,450 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c9cfad0a-8f5c-4599-8a7c-0d13b6068b66-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:24,451 INFO [Executor task launch worker for task 44] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,453 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,454 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,455 INFO [Executor task launch worker for task 44] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,455 WARN [Executor task launch worker for task 44] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,456 INFO [Executor task launch worker for task 44] o.a.s.e.Executor [Logging.scala:54] Finished task 6.0 in stage 38.0 (TID 44). 3033 bytes result sent to driver
2018-10-11 21:36:24,457 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 7.0 in stage 38.0 (TID 45, localhost, executor driver, partition 7, PROCESS_LOCAL, 10904 bytes)
2018-10-11 21:36:24,457 INFO [Executor task launch worker for task 45] o.a.s.e.Executor [Logging.scala:54] Running task 7.0 in stage 38.0 (TID 45)
2018-10-11 21:36:24,457 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 6.0 in stage 38.0 (TID 44) in 208 ms on localhost (executor driver) (7/8)
2018-10-11 21:36:24,459 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aa132c9-1c2c-4bed-8054-4f01f905b173-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:24,461 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,463 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,464 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,465 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,465 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,466 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1f52f9b8-ea21-4d2b-a7d2-11744187f778-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:24,467 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,469 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,471 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,472 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,472 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,473 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-51f29362-71b4-40bb-9ac9-c8fd50e8b54c-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,475 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,477 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,479 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,480 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,480 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,481 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca8ad858-bbe8-4c3b-a0f5-ea89627075a3-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,483 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,485 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,488 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,488 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,489 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,489 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-000c5114-25aa-43b6-ae19-ab0069bbeadf-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,491 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,493 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,495 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,495 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,496 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,496 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af314aee-5298-45df-899e-f5cf50b26e6a-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,497 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,499 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,501 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,501 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,502 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,503 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-06648b17-d906-45c9-b987-0c863fb63c7b-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,504 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,507 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,508 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,509 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,509 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,509 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-deaed750-dc34-4faa-bfea-c7b9e6d08139-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,510 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,512 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,514 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,514 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,515 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,515 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-edc1f6e1-52af-4b5a-9623-413fdd0995f1-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:24,516 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,518 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,519 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,519 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,520 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,520 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46686477-1f45-40b8-b264-443e98dbf08f-c000.snappy.parquet, range: 0-11350, partition values: [empty row]
2018-10-11 21:36:24,521 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,523 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,524 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,525 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,525 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,525 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d8e5bc47-ef5e-484a-98b7-34e73c59faf3-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:24,527 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,529 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,531 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,531 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,532 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,532 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-217b388a-5636-42f5-a2f7-f7e650de6c64-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:24,534 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,535 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,536 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,536 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,537 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,537 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-824fe1c8-dff2-491e-86b8-2e2542684692-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:24,540 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,541 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,542 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,543 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,543 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,543 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f812e08f-6f88-4c2d-96ec-83fdeb1e434a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:24,545 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,547 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,548 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,548 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,549 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,549 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9020ecd0-b01d-408c-baa5-a3b077d92f4a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:24,550 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,552 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,553 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,554 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,554 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,554 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5dccade-9231-4d83-bfc0-eeddbd723979-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:24,555 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,557 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,557 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,558 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,558 WARN [Executor task launch worker for task 45] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,558 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ce47add-8d58-4bb1-b893-375d2709dd55-c000.snappy.parquet, range: 0-882, partition values: [empty row]
2018-10-11 21:36:24,559 INFO [Executor task launch worker for task 45] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,561 INFO [Executor task launch worker for task 45] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:24,563 INFO [Executor task launch worker for task 45] o.a.s.e.Executor [Logging.scala:54] Finished task 7.0 in stage 38.0 (TID 45). 1423 bytes result sent to driver
2018-10-11 21:36:24,564 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 7.0 in stage 38.0 (TID 45) in 107 ms on localhost (executor driver) (8/8)
2018-10-11 21:36:24,564 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 38.0, whose tasks have all completed, from pool 
2018-10-11 21:36:24,564 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 38 (collect at SparkDataManager.scala:77) finished in 1.932 s
2018-10-11 21:36:24,565 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 27 finished: collect at SparkDataManager.scala:77, took 1.935756 s
2018-10-11 21:36:24,567 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,567 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,568 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,568 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,568 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,568 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,568 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,569 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,569 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,569 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,569 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,570 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,570 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,570 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,571 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,571 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,571 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,571 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,571 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,572 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,576 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 21:36:24,576 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:24,577 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:24,579 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:24,580 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:24,592 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:24,593 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:24,691 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:24,692 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:24,692 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:24,692 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:24,714 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 15.591761 ms
2018-10-11 21:36:24,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 30.81057 ms
2018-10-11 21:36:24,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_58 stored as values in memory (estimated size 230.0 KB, free 1390.7 MB)
2018-10-11 21:36:24,767 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_58_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.6 MB)
2018-10-11 21:36:24,767 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_58_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:24,768 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 58 from collect at SparkDataManager.scala:66
2018-10-11 21:36:24,770 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 16791983 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:24,782 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:66
2018-10-11 21:36:24,783 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 28 (collect at SparkDataManager.scala:66) with 1 output partitions
2018-10-11 21:36:24,783 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 39 (collect at SparkDataManager.scala:66)
2018-10-11 21:36:24,783 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:24,783 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:24,784 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 39 (MapPartitionsRDD[126] at collect at SparkDataManager.scala:66), which has no missing parents
2018-10-11 21:36:24,785 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_59 stored as values in memory (estimated size 31.8 KB, free 1390.6 MB)
2018-10-11 21:36:24,786 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_59_piece0 stored as bytes in memory (estimated size 10.4 KB, free 1390.6 MB)
2018-10-11 21:36:24,786 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_59_piece0 in memory on 192.168.0.206:34485 (size: 10.4 KB, free: 1391.3 MB)
2018-10-11 21:36:24,787 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 59 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:24,787 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[126] at collect at SparkDataManager.scala:66) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:24,787 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 39.0 with 1 tasks
2018-10-11 21:36:24,788 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 39.0 (TID 46, localhost, executor driver, partition 0, PROCESS_LOCAL, 8839 bytes)
2018-10-11 21:36:24,788 INFO [Executor task launch worker for task 46] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 39.0 (TID 46)
2018-10-11 21:36:24,793 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:24,795 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:24,799 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,802 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,802 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,805 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,806 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,806 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:24,808 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:24,814 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,817 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,817 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,818 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,819 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,819 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,819 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:24,821 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:24,826 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,829 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,830 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,831 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,831 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,831 WARN [Executor task launch worker for task 46] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,831 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:24,833 INFO [Executor task launch worker for task 46] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:24,838 INFO [Executor task launch worker for task 46] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:24,842 INFO [Executor task launch worker for task 46] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 39.0 (TID 46). 2177 bytes result sent to driver
2018-10-11 21:36:24,843 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 39.0 (TID 46) in 55 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:24,843 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 39.0, whose tasks have all completed, from pool 
2018-10-11 21:36:24,844 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 39 (collect at SparkDataManager.scala:66) finished in 0.059 s
2018-10-11 21:36:24,844 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 28 finished: collect at SparkDataManager.scala:66, took 0.061542 s
2018-10-11 21:36:24,845 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,846 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,846 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,847 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:24,847 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:24,849 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:24,850 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:24,859 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:24,860 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:24,913 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:24,914 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_56_piece0 on 192.168.0.206:34485 in memory (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:24,914 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:24,914 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<statement: struct<id: string, text: string>, startTime: bigint, endTime: bigint, description: string, details: string ... 1 more field>
2018-10-11 21:36:24,915 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1274
2018-10-11 21:36:24,915 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:24,915 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1270
2018-10-11 21:36:24,916 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1317
2018-10-11 21:36:24,916 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1297
2018-10-11 21:36:24,916 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1300
2018-10-11 21:36:24,916 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_58_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.4 MB)
2018-10-11 21:36:24,917 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1310
2018-10-11 21:36:24,917 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1276
2018-10-11 21:36:24,917 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1264
2018-10-11 21:36:24,917 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1283
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1306
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1296
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1261
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1318
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1316
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1265
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1278
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1277
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1309
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1284
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1311
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1320
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1299
2018-10-11 21:36:24,918 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1303
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1266
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1268
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1305
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1314
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1269
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1273
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1298
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1302
2018-10-11 21:36:24,919 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1292
2018-10-11 21:36:24,920 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_57_piece0 on 192.168.0.206:34485 in memory (size: 6.7 KB, free: 1391.4 MB)
2018-10-11 21:36:24,921 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1295
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1275
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1301
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1313
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1271
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1312
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1262
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1285
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1293
2018-10-11 21:36:24,922 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1287
2018-10-11 21:36:24,923 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_59_piece0 on 192.168.0.206:34485 in memory (size: 10.4 KB, free: 1391.4 MB)
2018-10-11 21:36:24,924 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1263
2018-10-11 21:36:24,924 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1272
2018-10-11 21:36:24,924 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1260
2018-10-11 21:36:24,924 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1282
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1281
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1291
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1308
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1267
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1294
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1290
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1307
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1304
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1279
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1319
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1289
2018-10-11 21:36:24,925 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1280
2018-10-11 21:36:24,926 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1315
2018-10-11 21:36:24,926 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1288
2018-10-11 21:36:24,926 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1286
2018-10-11 21:36:24,926 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1259
2018-10-11 21:36:24,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_60 stored as values in memory (estimated size 228.2 KB, free 1390.9 MB)
2018-10-11 21:36:24,936 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_60_piece0 stored as bytes in memory (estimated size 21.7 KB, free 1390.9 MB)
2018-10-11 21:36:24,937 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_60_piece0 in memory on 192.168.0.206:34485 (size: 21.7 KB, free: 1391.4 MB)
2018-10-11 21:36:24,938 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 60 from collect at SparkDataManager.scala:77
2018-10-11 21:36:24,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:24,956 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:77
2018-10-11 21:36:24,957 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 29 (collect at SparkDataManager.scala:77) with 8 output partitions
2018-10-11 21:36:24,957 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 40 (collect at SparkDataManager.scala:77)
2018-10-11 21:36:24,957 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:24,958 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:24,958 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 40 (MapPartitionsRDD[129] at collect at SparkDataManager.scala:77), which has no missing parents
2018-10-11 21:36:24,960 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_61 stored as values in memory (estimated size 16.2 KB, free 1390.9 MB)
2018-10-11 21:36:24,960 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_61_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1390.9 MB)
2018-10-11 21:36:24,961 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_61_piece0 in memory on 192.168.0.206:34485 (size: 6.7 KB, free: 1391.4 MB)
2018-10-11 21:36:24,962 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 61 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:24,962 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 8 missing tasks from ResultStage 40 (MapPartitionsRDD[129] at collect at SparkDataManager.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-10-11 21:36:24,962 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 40.0 with 8 tasks
2018-10-11 21:36:24,963 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 40.0 (TID 47, localhost, executor driver, partition 0, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:24,964 INFO [Executor task launch worker for task 47] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 40.0 (TID 47)
2018-10-11 21:36:24,966 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e31abc9-a228-4133-b02e-0f6cd7638510-c000.snappy.parquet, range: 0-13723, partition values: [empty row]
2018-10-11 21:36:24,968 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,970 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,971 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,972 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,972 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,973 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b1982672-2d41-4bff-8f45-9e6f8c921d5d-c000.snappy.parquet, range: 0-13722, partition values: [empty row]
2018-10-11 21:36:24,975 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,976 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,977 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,978 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:24,978 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,979 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e5316a0-c580-49fd-91d0-550955cd9bbc-c000.snappy.parquet, range: 0-13721, partition values: [empty row]
2018-10-11 21:36:24,980 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,983 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,984 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,985 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,986 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,986 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e15f7475-849c-45f2-9dcf-5bb2d5b69bd4-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:24,987 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,988 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,989 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,989 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,990 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,990 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a46c4b1e-6e4f-4382-a659-662347269774-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:24,992 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:24,994 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:24,996 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:24,996 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:24,998 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:24,999 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c7514033-0170-45b0-ad15-cb3097418dfb-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:25,000 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,002 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,004 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,004 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,005 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,005 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3723b14f-e12a-4f5a-b016-809b54de139c-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:25,007 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,009 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,010 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,010 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,010 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,010 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-07c85578-afbb-490a-bd9d-3837ab2a7baa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:25,011 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,013 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,014 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,014 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,015 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,015 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-603f4016-bd22-4ed6-b2c7-be69003d48fa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:25,016 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,018 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,019 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,019 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,020 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,020 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a6043ae9-7f4d-4793-ad20-a149ce2339bb-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:25,021 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,023 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,025 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,025 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,025 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,025 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8d65fb3-afa1-45c2-9354-709d30d7ae22-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:25,027 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,028 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,029 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,029 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,030 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,030 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0e52f33e-2792-451e-bfca-4fb2393d1315-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:25,031 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,033 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,034 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,034 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,035 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,035 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e001f918-e226-4b11-a792-e057310c71dc-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:25,036 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,037 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,038 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,039 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,039 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,039 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a17d8d1-3cec-4462-90f4-c0c77d76c5ce-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:25,040 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,042 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,043 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,043 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,044 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,044 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7992780f-0fa1-4c14-beca-90350009f018-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:25,045 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,047 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,048 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,048 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,049 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,049 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d57f919b-2b39-4c28-b8f3-61f31041cb83-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:25,050 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,052 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,053 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,053 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,054 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,055 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-735e4a0c-71b2-4dfe-9652-eddb95823bd0-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:25,056 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,058 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,059 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,059 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,060 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,060 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e132893-66ca-44f4-810d-b31bd307f3bf-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:25,061 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,063 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,064 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,064 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,064 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,064 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e32d4e0b-152b-41f5-9d5d-6319207c046a-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:25,065 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,067 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,068 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,068 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,068 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,068 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d24eaa68-02d1-43c1-b2da-ef94148610ed-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:25,069 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,071 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,072 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,072 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,072 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,072 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad3d0b86-19cf-474c-b3d4-796d7afaa4ae-c000.snappy.parquet, range: 0-13699, partition values: [empty row]
2018-10-11 21:36:25,074 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,075 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,076 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,076 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,077 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,077 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0247bc41-a2af-4b30-a5e0-5bc327f47cc2-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:25,078 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,079 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,080 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,080 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,081 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,081 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-26318166-52ec-4313-a4e5-3dc130f82d18-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:25,082 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,084 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,085 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,085 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,086 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,086 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2227df14-fd69-4e52-88fc-ffa906e8e96a-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:25,088 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,090 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,091 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,092 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,092 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,093 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-16e0e1c7-4724-40d4-8fe9-64c573377aac-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:25,094 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,095 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,096 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,096 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,097 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,097 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-12cc2ef7-5c98-45c7-bfca-9f6852b4f9cb-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:25,098 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,100 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,101 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,102 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,102 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,103 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5704978-3346-4cf0-a83a-fd67411e60c0-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:25,104 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,106 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,107 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,107 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,108 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,108 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-611aa927-9eff-4acc-a956-1419efe00229-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:25,109 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,111 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,112 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,112 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,113 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,113 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b6173355-0a8b-414c-9ed0-27985f88914d-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:25,114 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,116 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,117 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,117 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,118 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,118 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb54fd41-c4a7-41e8-a051-2493b4fc4fed-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:25,119 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,121 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,122 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,122 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,123 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,123 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9af47b28-32d6-4d91-b198-acee5bfe01dd-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:25,125 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,126 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,127 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,128 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,128 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,129 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-db7aecb7-d25c-4a59-ad0e-2bacf5772cb8-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,130 INFO [Executor task launch worker for task 47] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,132 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,134 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,134 INFO [Executor task launch worker for task 47] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,135 WARN [Executor task launch worker for task 47] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,136 INFO [Executor task launch worker for task 47] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 40.0 (TID 47). 1423 bytes result sent to driver
2018-10-11 21:36:25,136 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 1.0 in stage 40.0 (TID 48, localhost, executor driver, partition 1, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:25,138 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 40.0 (TID 47) in 175 ms on localhost (executor driver) (1/8)
2018-10-11 21:36:25,138 INFO [Executor task launch worker for task 48] o.a.s.e.Executor [Logging.scala:54] Running task 1.0 in stage 40.0 (TID 48)
2018-10-11 21:36:25,140 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ae6870f1-1f08-443f-8701-2a8a8fc509da-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,142 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,145 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,146 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,147 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,147 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,148 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d164cc8-eee4-4f35-920d-9fa0984dbb40-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,149 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,152 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,153 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,154 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,154 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,155 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b48d131c-6bd6-478f-9abc-96bd891b0584-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,156 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,157 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,158 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,159 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,159 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,159 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa252a4c-1080-4e94-9dc5-d3e35b8e70ea-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,160 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,162 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,164 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,164 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,165 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,165 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-586baf62-c0dd-443b-a8db-5822599f893a-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,167 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,169 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,170 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,171 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,171 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,171 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5ae7e692-d21c-4b25-9cdf-83f59ef40407-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,172 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,175 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,176 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,177 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,178 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,178 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5d04bbb7-c918-4d38-9348-7833d532fb6c-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,180 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,181 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,182 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,182 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,183 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,183 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-178dd25a-2181-4b65-970b-0683491eca83-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,184 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,186 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,187 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,188 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,188 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,188 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-90d7046c-97fc-4066-9720-fc9230ba0c54-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:25,189 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,191 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,192 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,192 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,192 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,193 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2bc1210e-eaa6-47cf-ba41-7e0b223d190f-c000.snappy.parquet, range: 0-13695, partition values: [empty row]
2018-10-11 21:36:25,194 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,196 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,198 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,198 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,199 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,199 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e7cb913a-50de-4609-aada-ae4eafd1de31-c000.snappy.parquet, range: 0-13694, partition values: [empty row]
2018-10-11 21:36:25,201 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,203 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,205 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,205 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,206 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,206 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f1d46819-9d92-46ba-9025-2adb0818719c-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:25,208 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,209 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,210 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,211 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,211 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,211 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b85e87fe-5382-4ea3-895f-13f9f032ef4b-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:25,213 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,214 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,215 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,215 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,216 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,216 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3831987b-b4b7-4617-a261-5bf721ece358-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:25,217 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,219 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,220 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,220 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,221 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,221 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-21e4180a-5c27-419b-ab1c-78aa6b2cb324-c000.snappy.parquet, range: 0-13691, partition values: [empty row]
2018-10-11 21:36:25,222 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,224 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,225 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,226 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,226 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,227 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c33bde23-fc3e-4548-a230-3eb78aac57fb-c000.snappy.parquet, range: 0-13690, partition values: [empty row]
2018-10-11 21:36:25,228 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,229 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,230 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,231 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,231 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,231 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0232792-2703-4cd0-a57b-e87871b66a88-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,232 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,234 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,235 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,236 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,236 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,236 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6f55e7b2-ed9f-4a3f-9a54-06887558e22d-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,237 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,239 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,240 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,241 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,241 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,241 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4384b7b2-8b67-4077-a182-89d2e37a4833-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,243 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,245 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,247 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,247 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,248 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,248 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19b79ab5-3105-4028-bbd0-c35b902de4a4-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,250 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,252 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,253 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,254 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,254 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,255 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6d920ff1-e3fc-40ac-a84e-386f13c16e67-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,256 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,258 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,260 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,260 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,261 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,261 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d63edb8d-fac0-4840-b019-b0b6e7a639d6-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,262 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,264 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,266 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,266 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,266 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,267 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09613236-c7ea-4e03-a2f6-c2dd85f36bed-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,268 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,270 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,271 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,272 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,272 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,273 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aafd0534-3203-4405-a282-194c9a55812b-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,274 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,276 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,276 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,277 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,277 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,277 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-018f2c55-3be6-41b0-b9cd-c3e9619e0cda-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,278 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,280 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,281 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,281 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,281 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,282 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b9578903-dcf4-4d51-9126-5a90dd27a1ef-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:25,283 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,284 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,285 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,286 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,286 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,287 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9f7a9a62-4bad-4d7b-8940-549d39238770-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:25,288 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,289 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,290 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,291 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,291 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,291 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8b0e504c-c3cf-496a-8730-d46b0b7df1b9-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:25,292 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,294 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,295 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,295 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,295 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,295 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-79b8fe3f-ef98-4c64-bedc-86c3fb9283fd-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:25,296 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,297 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,298 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,298 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,299 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,299 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57dddecf-aa33-4874-99e6-9748f780fe78-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:25,300 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,301 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,302 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,303 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,303 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,304 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7bfc2fc2-2b64-4ee4-83ae-ad0b645e1322-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:25,305 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,307 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,308 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,308 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,309 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,309 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a928c40-d93a-4600-a43e-d6259a8a7c26-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:25,310 INFO [Executor task launch worker for task 48] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,312 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,313 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,314 INFO [Executor task launch worker for task 48] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,314 WARN [Executor task launch worker for task 48] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,315 INFO [Executor task launch worker for task 48] o.a.s.e.Executor [Logging.scala:54] Finished task 1.0 in stage 40.0 (TID 48). 1423 bytes result sent to driver
2018-10-11 21:36:25,315 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 2.0 in stage 40.0 (TID 49, localhost, executor driver, partition 2, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:25,316 INFO [Executor task launch worker for task 49] o.a.s.e.Executor [Logging.scala:54] Running task 2.0 in stage 40.0 (TID 49)
2018-10-11 21:36:25,316 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 1.0 in stage 40.0 (TID 48) in 180 ms on localhost (executor driver) (2/8)
2018-10-11 21:36:25,317 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-62170ceb-3786-48a8-93d4-b567247e8037-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:25,318 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,320 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,320 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,321 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,321 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,321 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8643ee33-f4b3-4b9d-a32e-39e43cfe4ba7-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:25,323 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,324 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,325 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,325 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,326 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,326 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0680d8e3-d6a3-46d1-b2d8-76df28d3b02a-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:25,327 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,328 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,329 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,329 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,330 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,330 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aae2da02-ce8a-4dad-87c4-6a12978949be-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:25,331 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,332 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,333 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,333 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,333 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,333 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8eae07df-593a-43cc-ac4a-2be464a53d3f-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:25,334 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,336 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,337 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,337 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,338 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,338 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a663e89f-b19d-4f56-bfd7-d844367b5dc2-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:25,340 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,341 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,342 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,343 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,343 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,343 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2318bf98-30f1-4cb7-89ff-f54e8f26ee77-c000.snappy.parquet, range: 0-13670, partition values: [empty row]
2018-10-11 21:36:25,344 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,346 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,347 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,348 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,348 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,349 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-854fe582-8eec-43ce-b79a-f5a9b836d2fd-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,350 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,353 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,354 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,355 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,355 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,356 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5793a580-0e06-4393-afec-1a4eefbbd626-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,358 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,360 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,362 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,363 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,363 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,364 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e7b564a-5bd2-4701-bdea-b107946c9b0c-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,365 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,368 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,370 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,371 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,372 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,372 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d10c8980-7d22-4a6d-99ec-053805bdf9ec-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,374 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,377 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,378 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,378 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,379 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,379 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-59858d75-788e-4333-9577-bd4947258eff-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,381 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,383 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,384 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,385 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,385 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,385 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-274749d2-43f1-4c5d-81fe-d888dc9dff47-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,387 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,389 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,391 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,391 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,392 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,392 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ddde392-a677-4c60-92fc-49cd2be2c0f1-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:25,393 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,395 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,396 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,396 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,397 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,397 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b273f389-8ffb-4cbf-b108-6506095ae632-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:25,398 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,400 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,401 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,401 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,402 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,402 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7233e2d1-9862-45dc-9824-4e41f2c2867e-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:25,403 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,406 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,407 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,407 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,408 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,408 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-86a299de-f8fc-46fe-86a9-60a4b26a9b38-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:25,410 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,412 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,413 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,413 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,413 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,413 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-defaedb6-dcf3-4709-916c-a3e955107f58-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:25,414 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,415 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,416 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,416 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,417 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,417 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a837d014-9d5d-454e-a7f6-8f2fb06c37d0-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:25,419 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,421 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,423 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,423 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,424 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,424 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-45a99307-69fd-4a6c-8a0c-91d96b83817a-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:25,425 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,426 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,427 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,427 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,428 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,428 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-29b6c682-0e21-4836-904c-33bbd018067c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:25,429 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,431 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,432 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,433 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,433 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,434 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bbb741a3-045d-451c-a803-96b4d0493b04-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:25,435 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,437 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,438 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,438 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,439 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,439 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e33d9f4-1cd5-43f7-b792-6d744d0a7505-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:25,441 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,443 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,444 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,444 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,445 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,445 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-71b8e55a-40c5-4b5c-9dae-af4d5551ef2c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:25,447 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,449 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,450 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,451 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,451 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,452 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e2d6c34-9f3a-4d0c-9b3f-d0afa5e7e097-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,453 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,456 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,458 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,458 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,459 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,459 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aadda0f-0463-4bfe-9f32-62f122ef3ff4-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,461 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,463 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,464 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,465 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,465 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,465 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bea86d33-775a-4e73-97cd-927d76b9352f-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,467 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,469 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,471 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,471 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,472 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,472 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e0852e5-e63c-45eb-912a-d3964270da43-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,473 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,475 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,477 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,477 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,478 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,478 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4cc621ee-7546-4d9f-8ba0-c4c66f0adeec-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,480 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,482 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,483 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,483 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,484 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,484 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58131256-2685-47d1-abb5-a4357c095a67-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,486 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,488 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,490 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,490 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,491 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,491 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a5a52bbe-0c0c-41d6-a8be-9839d1629b37-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:25,493 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,495 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,496 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,496 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,497 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,497 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-869a108e-1223-444d-9a58-a852afc71fdc-c000.snappy.parquet, range: 0-13658, partition values: [empty row]
2018-10-11 21:36:25,499 INFO [Executor task launch worker for task 49] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,501 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,502 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,503 INFO [Executor task launch worker for task 49] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,503 WARN [Executor task launch worker for task 49] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,504 INFO [Executor task launch worker for task 49] o.a.s.e.Executor [Logging.scala:54] Finished task 2.0 in stage 40.0 (TID 49). 1423 bytes result sent to driver
2018-10-11 21:36:25,505 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 3.0 in stage 40.0 (TID 50, localhost, executor driver, partition 3, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:25,505 INFO [Executor task launch worker for task 50] o.a.s.e.Executor [Logging.scala:54] Running task 3.0 in stage 40.0 (TID 50)
2018-10-11 21:36:25,505 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 2.0 in stage 40.0 (TID 49) in 190 ms on localhost (executor driver) (3/8)
2018-10-11 21:36:25,507 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-666a2e39-edd2-4992-85ee-cd58cc019c85-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:25,508 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,510 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,511 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,511 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,512 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,512 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca11e9c0-3060-4a86-91dd-db9ce2ca7e5f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:25,513 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,515 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,517 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,517 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,518 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,518 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4a6558fd-1e62-4e07-9df1-400871fc5cec-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:25,519 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,521 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,523 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,523 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,524 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,524 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19122b21-1ceb-4cf4-8593-9fa7c8624d2d-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:25,526 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,528 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,529 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,529 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,530 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,530 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d7ef5b02-cddb-495a-aad3-f3679ca76b8f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:25,531 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,534 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,535 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,535 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,536 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,537 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-694f93cb-34fe-4247-821f-f5ca506e1459-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:25,538 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,541 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,542 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,543 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,543 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,544 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-701ae0c0-29df-4b97-839c-9c83df4e1b68-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:25,545 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,547 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,548 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,549 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,549 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,550 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5b6171fb-919a-4c7b-98aa-5dfe9d454475-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:25,551 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,553 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,555 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,555 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,556 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,556 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d21bcc-601f-4e42-b182-081c470c659d-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:25,558 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,560 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,561 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,562 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,562 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,562 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-600d8f03-7aac-4957-baf4-c554ff43494a-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:25,564 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,566 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,567 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,568 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,568 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,569 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e02ee93e-e796-42b0-bf02-5f75dfdc1c6c-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:25,570 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,572 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,574 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,575 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,575 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,575 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a90b0f72-7d9a-4a49-a68b-f290f02aa529-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:25,576 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,578 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,579 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,579 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,580 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,580 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f41d2183-74d9-4df5-be81-a2fb23bf9de7-c000.snappy.parquet, range: 0-11392, partition values: [empty row]
2018-10-11 21:36:25,581 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,583 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,584 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,584 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,584 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,585 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3fa69d1c-3edf-45d2-b0ac-1272168f52c9-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:25,586 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,588 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,589 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,590 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,590 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,590 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a0b202fc-2bf1-495f-9764-acfb0429a5c8-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:25,591 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,593 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,594 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,594 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,595 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,595 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-936820e3-b496-4cc2-8571-9635f4fdbfa3-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:25,596 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,597 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,599 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,599 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,600 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,600 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-eb4c07ef-088b-4a15-be2f-b273fd27d040-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:25,601 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,603 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,604 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,605 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,606 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,606 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4d67cb5-e9dc-4b19-acea-905574bf8522-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:25,608 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,609 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,611 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,611 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,611 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,612 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-224b809f-76f3-4919-a9b8-43b820662112-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:25,613 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,615 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,617 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,618 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,618 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,618 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7501282c-59f0-4b1a-88f5-82366eca768c-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:25,620 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,622 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,623 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,624 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,624 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,624 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3cad3b64-e9c9-41b4-a422-dc665b78c2c1-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:25,625 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,627 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,628 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,628 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,628 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,628 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-390c15b5-d721-4535-be3f-12b05cfbe30a-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:25,630 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,632 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,633 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,634 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,634 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,635 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d28ee0-4831-43b1-a27f-dab415ede7a0-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:25,636 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,637 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,639 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,639 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,640 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,640 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-56056fe1-8392-42d6-b2c1-4463f7381740-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:25,641 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,643 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,644 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,644 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,645 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,645 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dd3999fe-68a8-43bd-b968-d1abe12b7d68-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:25,646 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,648 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,649 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,649 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,649 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,650 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-40f0422a-c31f-4b28-870e-1da30f3d4173-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:25,651 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,653 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,655 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,655 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,656 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,657 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-81102eef-ee91-4905-a219-3a7d6ad68001-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:25,658 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,660 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,661 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,661 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,661 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,662 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a7708ac5-ff0d-4d9b-b829-432325d537bd-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:25,663 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,664 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,665 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,665 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,666 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,666 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-de923178-e27e-43df-9376-a8765b42b212-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:25,667 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,669 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,669 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,670 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,670 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,670 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2366af80-dffe-477b-8217-7f704825188b-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:25,671 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,672 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,673 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,674 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,674 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,674 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad60e729-defb-4a4e-99d9-9ca74fcf6a2d-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:25,675 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,677 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,678 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,678 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,678 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,679 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0cf40412-95c9-405c-9299-90b8afa2a414-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,680 INFO [Executor task launch worker for task 50] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,681 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,682 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,682 INFO [Executor task launch worker for task 50] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,682 WARN [Executor task launch worker for task 50] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,683 INFO [Executor task launch worker for task 50] o.a.s.e.Executor [Logging.scala:54] Finished task 3.0 in stage 40.0 (TID 50). 1423 bytes result sent to driver
2018-10-11 21:36:25,684 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 4.0 in stage 40.0 (TID 51, localhost, executor driver, partition 4, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:25,684 INFO [Executor task launch worker for task 51] o.a.s.e.Executor [Logging.scala:54] Running task 4.0 in stage 40.0 (TID 51)
2018-10-11 21:36:25,684 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 3.0 in stage 40.0 (TID 50) in 179 ms on localhost (executor driver) (4/8)
2018-10-11 21:36:25,685 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8f84cc33-f1d5-478f-87ce-0663d825408c-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,686 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,688 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,689 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,689 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,690 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,690 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64602a48-9868-4fdb-bbce-e23fdb1ff4ad-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,691 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,692 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,693 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,693 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,694 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,694 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-22885ce6-f042-455f-87e9-76587998ab27-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,695 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,696 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,697 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,697 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,697 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,698 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4f0d15a7-be71-41c6-9bcc-c559eb6c8a2a-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,698 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,699 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,700 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,700 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,701 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,701 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3bf97aa1-dd22-42f0-8f17-b66b1cd270a5-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,702 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,703 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,704 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,704 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,704 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,704 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57c3815d-7209-4ed6-90cc-5597db2fc409-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:25,705 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,706 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,707 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,708 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,708 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,708 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fff88882-0587-48ef-95f1-5dd0508833d6-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,709 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,710 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,711 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,711 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,712 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,712 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0df7b64b-c870-4b00-b1dc-59fec5d6c208-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,713 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,715 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,716 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,717 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,717 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,717 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-384bf5ec-e9b8-429f-8281-82432437fb02-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,719 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,720 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,722 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,722 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,722 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,723 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f08e80af-add7-42b2-b4ad-2a22d1ce2a56-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,724 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,726 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,727 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,727 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,728 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,728 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-615e7057-357a-4616-a738-affe25bf8b3a-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,729 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,731 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,732 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,733 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,733 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,733 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-73bd7a91-071c-42d6-a80a-c13db79d72d2-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,734 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,736 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,737 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,737 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,738 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,738 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dbf5cfde-c2ae-4ddf-bdee-0e703dcab699-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,739 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,740 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,741 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,741 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,742 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,742 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-70719782-4616-4b3d-82dd-7bc33b67022f-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,743 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,744 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,745 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,745 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,745 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,745 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1e620604-bee6-486e-b171-860be8b8b75e-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,747 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,748 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,749 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,749 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,749 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,750 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05b626a4-35ba-4e5f-9c93-d7f62d4d3027-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,751 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,752 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,753 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,754 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,754 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,754 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0ac09a5-d2c0-4844-95ed-3706f6eb6930-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,756 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,757 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,758 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,759 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,759 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,759 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e6fcf0f4-d681-401e-b5b6-7075100f24ee-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,760 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,761 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,762 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,762 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,762 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,763 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-00ef3320-f515-47a4-b26f-5b5caf94f65d-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,764 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,765 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,766 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,766 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,766 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,766 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ff4df6d7-b381-401d-987b-233a02e4d909-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,767 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,769 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,769 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,770 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,770 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,770 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f5fe52bb-22d0-4408-b9d4-9b3a37c89088-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,771 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,773 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,774 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,774 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,774 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,774 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a3db9e49-1db1-4e94-a28e-f73df6d27fa8-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,775 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,776 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,777 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,778 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,778 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,778 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-66b4c136-f4b2-4428-9331-2adf6a097511-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:25,779 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,780 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,781 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,781 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,781 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,782 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e3141f79-7a0f-4730-8468-e54a6c54c533-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:25,782 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,784 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,785 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,785 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,785 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,786 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0b0c8187-cf2f-44d8-88fc-49ce4dd62874-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:25,787 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,789 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,790 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,791 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,791 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,791 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-08d62d99-4b8f-4e06-8620-bba67557d59d-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:25,793 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,795 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,797 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,797 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,798 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,798 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64403a21-b2b7-43f7-a22d-5f7706bad082-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:25,799 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,801 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,803 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,804 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,804 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,804 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c1e3a260-62f4-4aeb-8a26-82616f973030-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:25,806 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,808 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,809 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,809 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,809 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,810 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9bff7912-93ac-4fc7-a1db-f8e566a70655-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:25,811 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,812 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,813 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,813 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,813 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,814 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-47728647-7a80-4592-ba7c-c3ebcb1e5224-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:25,814 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,816 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,818 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,818 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,818 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,819 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9b3bccf5-ad25-404d-9310-88a915f9b7a2-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:25,820 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,821 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,822 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,822 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,822 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,822 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-510a5951-088e-4dab-8dd0-c02c47869d79-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:25,823 INFO [Executor task launch worker for task 51] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,825 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,826 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,826 INFO [Executor task launch worker for task 51] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,827 WARN [Executor task launch worker for task 51] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,827 INFO [Executor task launch worker for task 51] o.a.s.e.Executor [Logging.scala:54] Finished task 4.0 in stage 40.0 (TID 51). 1380 bytes result sent to driver
2018-10-11 21:36:25,828 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 5.0 in stage 40.0 (TID 52, localhost, executor driver, partition 5, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:25,828 INFO [Executor task launch worker for task 52] o.a.s.e.Executor [Logging.scala:54] Running task 5.0 in stage 40.0 (TID 52)
2018-10-11 21:36:25,828 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 4.0 in stage 40.0 (TID 51) in 144 ms on localhost (executor driver) (5/8)
2018-10-11 21:36:25,829 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6650061f-a0ab-42f3-be0d-e29bba951689-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:25,834 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,836 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,837 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,838 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,838 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,838 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a73de2ae-2a91-46fe-a61e-d06104068675-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:25,839 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,840 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,841 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,842 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,842 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,842 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-60348a41-e7a8-4c02-9dbf-a4848152bf0a-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:25,843 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,845 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,846 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,846 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,847 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,847 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca9d3388-a32e-42aa-9b9a-f2ba826d1b2e-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:25,848 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,850 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,851 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,851 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,851 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,851 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46ee92fd-a03b-4ad7-a847-fac273921f57-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:25,853 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,854 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,855 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,856 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,856 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,856 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4317e6e3-7c07-49f4-9bea-564768382089-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:25,857 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,858 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,860 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,860 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,860 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,860 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dc5fe75d-ae31-4bd0-a866-d3274b2d629b-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:25,861 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,862 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,863 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,864 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,864 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,864 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-49c1ab9d-a225-4ba8-911f-7c8138662881-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:25,865 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,866 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,867 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,867 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,868 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,868 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a910a7f7-d2c2-4fb9-9907-6f0a12ef5816-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,869 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,870 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,871 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,871 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,872 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,872 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af526506-da3a-461a-b7d6-7576a7ca2dc6-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,873 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,874 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,875 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,875 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,875 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,875 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f7492e12-3b8e-43e6-b84c-e42d363116f9-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,876 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,877 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,878 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,878 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,879 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,879 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-272bed24-7d51-4fc6-9a8a-aba3fb69c4b8-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,880 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,882 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,883 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,883 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,884 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,884 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ecac671-fc2e-4df4-9315-bf4c516db607-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,885 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,886 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,887 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,887 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,888 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,888 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f2c3b673-16d6-4469-a5ed-13de2cc92f10-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,889 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,891 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,893 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,893 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,893 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,894 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a6947b4-4ad6-4ab6-8907-a8320e284bcb-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:25,895 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,897 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,898 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,898 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,898 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,898 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3a0fc732-ce1b-4da0-9cd5-162ab4573e6e-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,899 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,901 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,902 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,902 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,903 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,903 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a087b5c1-75ef-4738-bcac-2ee86aa294bd-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,904 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,905 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,906 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,906 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,906 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,906 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e0eae261-dec5-4554-b126-6e1829f78050-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,907 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,908 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,909 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,909 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,910 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,910 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-987c95c1-23dd-407a-98e6-0e238fe1473b-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,911 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,912 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,913 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,913 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,913 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,913 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb60375b-4e46-4d45-99c6-540adc7208c3-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,914 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,916 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,917 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,917 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,917 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,917 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-319157ef-58f7-4406-b9b1-6baf5b53b0a8-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,918 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,920 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,921 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,921 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,921 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,921 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8022bee4-ba1f-4f2d-87cc-7ae422111479-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,922 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,923 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,924 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,925 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,925 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,925 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-861da466-0eee-47d7-9ff5-971c6d7f5a93-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,926 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,928 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,929 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,929 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,930 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,930 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8a3516de-8b82-44d1-82a8-69aa660cbbdf-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,931 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,933 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,934 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,935 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,935 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,935 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f419bf0e-cdd8-4f6c-8beb-f4dedb61a260-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,936 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,938 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,939 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,940 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,940 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,940 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e766d71-e245-4297-857b-bdd21b58b6f4-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,941 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,942 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,943 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,943 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,944 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,944 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4c33fd2a-9846-4f6b-a1f0-dccf06433485-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,945 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,946 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,947 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,947 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,947 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,948 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8fc6b69-477b-42e7-9d1a-3c499d4d10b2-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:25,948 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,950 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,951 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,951 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,951 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,952 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9039fff7-2957-4b0e-992e-e87565413ec8-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,952 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,954 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,955 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,955 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,956 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,956 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ef4e6a53-30c6-43c0-8eff-8a34e34edf6b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,957 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,959 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,961 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,961 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,962 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,962 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a398596-1df5-4ca7-911f-e74abd3762dd-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,963 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,965 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,967 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,967 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,968 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,968 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58f4dff5-7214-4777-ba17-784e2cef12f4-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,970 INFO [Executor task launch worker for task 52] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,972 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,974 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,974 INFO [Executor task launch worker for task 52] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,975 WARN [Executor task launch worker for task 52] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,976 INFO [Executor task launch worker for task 52] o.a.s.e.Executor [Logging.scala:54] Finished task 5.0 in stage 40.0 (TID 52). 2989 bytes result sent to driver
2018-10-11 21:36:25,977 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 6.0 in stage 40.0 (TID 53, localhost, executor driver, partition 6, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:25,977 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 5.0 in stage 40.0 (TID 52) in 149 ms on localhost (executor driver) (6/8)
2018-10-11 21:36:25,977 INFO [Executor task launch worker for task 53] o.a.s.e.Executor [Logging.scala:54] Running task 6.0 in stage 40.0 (TID 53)
2018-10-11 21:36:25,979 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2422c402-71d9-41d7-9987-1d4ab7fc19fb-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,981 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,983 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,984 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,985 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:25,985 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,985 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0ff4b192-b8e8-4b6c-9828-f5d9ec00cbce-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,987 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,989 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,990 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,990 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,991 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,991 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a2d93e67-4023-47f3-9d4a-f50a9c8541fe-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,992 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,993 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,994 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,994 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,994 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,995 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a29231b4-e038-4ee0-88ef-1d68cfe9d2be-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:25,996 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:25,997 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:25,998 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:25,998 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:25,999 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:25,999 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-55b99cce-2e82-46d0-8e53-bee4918b429e-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,000 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,002 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,003 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,003 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,004 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,005 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9dc2a350-c1b5-4c5c-8f81-b54fcb2ffa3a-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,006 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,009 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,010 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,010 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,011 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,011 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1be87a1d-cfca-46a7-b3c5-5377826665a1-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,012 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,014 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,015 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,016 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,016 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,016 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ac9002e-c497-481d-b337-445644058c65-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,018 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,019 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,020 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,020 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,021 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,021 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7453f0e9-fcdc-45d9-aa4e-04e2742b8b2b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,022 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,024 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,025 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,026 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,026 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,026 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3ce2c6c2-8244-4825-9d79-b2816de2acda-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,027 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,029 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,031 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,031 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,032 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,032 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e32c7dc-dcea-456e-a878-c9c835248ea9-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,033 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,036 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,037 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,038 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,038 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,039 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b2dcecd1-372e-4f0b-9562-f96ebf3af8f6-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,040 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,043 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,044 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,045 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,045 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,046 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b266ce1f-aa2d-4c98-8bd7-38ee65369d89-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,047 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,049 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,051 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,051 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,052 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,052 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6c58af95-e7c8-4556-989d-8a8fc881fc54-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:26,053 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,056 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,057 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,058 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,058 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,059 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-da19fb6b-0851-4b26-8454-89e7d9d29409-c000.snappy.parquet, range: 0-11359, partition values: [empty row]
2018-10-11 21:36:26,061 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,063 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,065 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,065 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,066 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,066 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2646c07b-af07-4e07-8e28-e3b57bbb514c-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,068 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,070 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,071 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,071 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,071 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,072 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7fed2b39-6f8a-4f9e-8921-caa7441424ec-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,073 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,075 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,076 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,076 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,077 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,077 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f4b5fa6b-5fa7-4ccf-b1ac-7fe087ca4972-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,079 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,080 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,081 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,082 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,082 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,083 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e8617665-882c-4cd9-af6c-d70cf9357935-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,084 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,086 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,088 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,088 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,089 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,089 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9c42e760-f3d5-4337-93cc-2faeb5608d5f-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,091 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,093 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,094 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,095 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,096 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,096 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-992bc073-6936-47bb-9ce0-4bd6ff5312f3-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,098 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,100 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,102 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,103 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,103 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,104 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bdc96751-af2e-4c0d-aad9-9b6309410e26-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,105 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,108 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,109 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,110 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,110 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,111 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2a8062ca-53c1-4efb-887d-a5f15b2e2675-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:26,113 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,115 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,117 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,117 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,118 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,118 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-461919ce-4e71-49bc-ab59-c7e999f082df-c000.snappy.parquet, range: 0-11357, partition values: [empty row]
2018-10-11 21:36:26,120 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,122 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,124 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,124 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,125 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,125 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d184ff46-c4c0-4adb-993f-d1625bf18c93-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:26,126 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,128 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,129 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,129 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,130 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,130 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a542cfa-fb0f-420c-808c-c23861bc5e49-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:26,132 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,133 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,135 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,135 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,135 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,136 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3744292d-429a-4b99-8f87-0174a77a5f02-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:26,137 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,139 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,140 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,140 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,141 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,141 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ce13747d-84bc-4b83-bcbf-ed87cb408c63-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:26,142 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,143 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,144 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,144 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,144 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,145 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa3ef260-acd7-4ea3-b2bb-08ce39dc72a4-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:26,145 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,147 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,148 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,148 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,148 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,148 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09afdad3-a22e-46d0-8913-a98493735de8-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:26,149 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,150 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,151 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,152 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,152 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,152 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4de0605-1810-4659-8565-775942a08750-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:26,153 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,155 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,156 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,156 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,156 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,157 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c9cfad0a-8f5c-4599-8a7c-0d13b6068b66-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:26,157 INFO [Executor task launch worker for task 53] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,159 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,160 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,160 INFO [Executor task launch worker for task 53] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,161 WARN [Executor task launch worker for task 53] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,161 INFO [Executor task launch worker for task 53] o.a.s.e.Executor [Logging.scala:54] Finished task 6.0 in stage 40.0 (TID 53). 2905 bytes result sent to driver
2018-10-11 21:36:26,162 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 7.0 in stage 40.0 (TID 54, localhost, executor driver, partition 7, PROCESS_LOCAL, 10904 bytes)
2018-10-11 21:36:26,162 INFO [Executor task launch worker for task 54] o.a.s.e.Executor [Logging.scala:54] Running task 7.0 in stage 40.0 (TID 54)
2018-10-11 21:36:26,162 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 6.0 in stage 40.0 (TID 53) in 185 ms on localhost (executor driver) (7/8)
2018-10-11 21:36:26,163 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aa132c9-1c2c-4bed-8054-4f01f905b173-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:26,165 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,166 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,167 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,167 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,168 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,168 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1f52f9b8-ea21-4d2b-a7d2-11744187f778-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:26,169 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,170 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,172 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,172 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,172 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,173 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-51f29362-71b4-40bb-9ac9-c8fd50e8b54c-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,174 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,175 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,176 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,176 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,177 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,177 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca8ad858-bbe8-4c3b-a0f5-ea89627075a3-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,178 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,179 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,180 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,180 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,181 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,181 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-000c5114-25aa-43b6-ae19-ab0069bbeadf-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,182 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,183 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,184 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,184 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,184 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,185 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af314aee-5298-45df-899e-f5cf50b26e6a-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,185 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,187 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,188 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,188 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,188 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,189 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-06648b17-d906-45c9-b987-0c863fb63c7b-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,190 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,191 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,192 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,192 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,192 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,193 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-deaed750-dc34-4faa-bfea-c7b9e6d08139-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,193 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,195 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,196 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,196 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,196 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,196 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-edc1f6e1-52af-4b5a-9623-413fdd0995f1-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:26,197 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,199 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,200 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,200 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,200 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,200 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46686477-1f45-40b8-b264-443e98dbf08f-c000.snappy.parquet, range: 0-11350, partition values: [empty row]
2018-10-11 21:36:26,201 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,203 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,204 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,204 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,205 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,205 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d8e5bc47-ef5e-484a-98b7-34e73c59faf3-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:26,206 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,208 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,209 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,209 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,210 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,210 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-217b388a-5636-42f5-a2f7-f7e650de6c64-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:26,211 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,212 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,213 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,214 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,214 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,214 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-824fe1c8-dff2-491e-86b8-2e2542684692-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:26,215 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,217 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,218 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,218 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,219 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,219 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f812e08f-6f88-4c2d-96ec-83fdeb1e434a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:26,221 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,224 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,226 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,227 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,241 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,242 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9020ecd0-b01d-408c-baa5-a3b077d92f4a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:26,243 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,245 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,246 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,246 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,247 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,247 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5dccade-9231-4d83-bfc0-eeddbd723979-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:26,248 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,249 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,250 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,251 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,251 WARN [Executor task launch worker for task 54] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,251 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ce47add-8d58-4bb1-b893-375d2709dd55-c000.snappy.parquet, range: 0-882, partition values: [empty row]
2018-10-11 21:36:26,252 INFO [Executor task launch worker for task 54] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,254 INFO [Executor task launch worker for task 54] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:26,256 INFO [Executor task launch worker for task 54] o.a.s.e.Executor [Logging.scala:54] Finished task 7.0 in stage 40.0 (TID 54). 1423 bytes result sent to driver
2018-10-11 21:36:26,256 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 7.0 in stage 40.0 (TID 54) in 94 ms on localhost (executor driver) (8/8)
2018-10-11 21:36:26,257 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 40.0, whose tasks have all completed, from pool 
2018-10-11 21:36:26,257 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 40 (collect at SparkDataManager.scala:77) finished in 1.298 s
2018-10-11 21:36:26,257 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 29 finished: collect at SparkDataManager.scala:77, took 1.300479 s
2018-10-11 21:36:26,258 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,258 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,258 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,258 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,259 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,259 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,259 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,259 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,259 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,259 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,260 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,260 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,260 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,260 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,260 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,260 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,261 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,261 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,261 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,261 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,262 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:26,262 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:26,265 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:26,266 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:26,273 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:26,273 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:26,320 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:26,321 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:26,322 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<statement: struct<id: string, text: string>, startTime: bigint, endTime: bigint, description: string, details: string ... 1 more field>
2018-10-11 21:36:26,322 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:26,334 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_62 stored as values in memory (estimated size 228.2 KB, free 1390.7 MB)
2018-10-11 21:36:26,342 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_62_piece0 stored as bytes in memory (estimated size 21.7 KB, free 1390.6 MB)
2018-10-11 21:36:26,343 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_62_piece0 in memory on 192.168.0.206:34485 (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:26,343 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 62 from collect at SparkDataManager.scala:77
2018-10-11 21:36:26,344 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:26,353 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:77
2018-10-11 21:36:26,354 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 30 (collect at SparkDataManager.scala:77) with 8 output partitions
2018-10-11 21:36:26,354 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 41 (collect at SparkDataManager.scala:77)
2018-10-11 21:36:26,354 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:26,354 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:26,355 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 41 (MapPartitionsRDD[132] at collect at SparkDataManager.scala:77), which has no missing parents
2018-10-11 21:36:26,356 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_63 stored as values in memory (estimated size 16.2 KB, free 1390.6 MB)
2018-10-11 21:36:26,357 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_63_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1390.6 MB)
2018-10-11 21:36:26,358 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_63_piece0 in memory on 192.168.0.206:34485 (size: 6.7 KB, free: 1391.3 MB)
2018-10-11 21:36:26,358 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 63 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:26,358 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 8 missing tasks from ResultStage 41 (MapPartitionsRDD[132] at collect at SparkDataManager.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-10-11 21:36:26,358 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 41.0 with 8 tasks
2018-10-11 21:36:26,359 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 41.0 (TID 55, localhost, executor driver, partition 0, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:26,359 INFO [Executor task launch worker for task 55] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 41.0 (TID 55)
2018-10-11 21:36:26,361 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e31abc9-a228-4133-b02e-0f6cd7638510-c000.snappy.parquet, range: 0-13723, partition values: [empty row]
2018-10-11 21:36:26,362 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,364 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,365 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,365 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,365 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,366 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b1982672-2d41-4bff-8f45-9e6f8c921d5d-c000.snappy.parquet, range: 0-13722, partition values: [empty row]
2018-10-11 21:36:26,367 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,368 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,369 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,369 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,370 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,370 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e5316a0-c580-49fd-91d0-550955cd9bbc-c000.snappy.parquet, range: 0-13721, partition values: [empty row]
2018-10-11 21:36:26,371 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,373 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,374 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,374 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,375 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,375 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e15f7475-849c-45f2-9dcf-5bb2d5b69bd4-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:26,376 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,378 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,379 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,379 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,379 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,379 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a46c4b1e-6e4f-4382-a659-662347269774-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:26,380 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,382 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,383 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,383 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,383 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,383 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c7514033-0170-45b0-ad15-cb3097418dfb-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:26,384 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,386 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,387 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,387 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,387 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,387 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3723b14f-e12a-4f5a-b016-809b54de139c-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:26,389 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,390 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,391 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,391 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,392 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,392 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-07c85578-afbb-490a-bd9d-3837ab2a7baa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:26,393 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,394 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,395 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,395 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,395 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,396 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-603f4016-bd22-4ed6-b2c7-be69003d48fa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:26,397 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,398 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,399 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,399 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,399 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,400 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a6043ae9-7f4d-4793-ad20-a149ce2339bb-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:26,401 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,402 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,404 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,405 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,405 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,406 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8d65fb3-afa1-45c2-9354-709d30d7ae22-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:26,407 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,408 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,409 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,410 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,410 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,410 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0e52f33e-2792-451e-bfca-4fb2393d1315-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:26,411 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,412 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,413 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,413 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,414 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,414 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e001f918-e226-4b11-a792-e057310c71dc-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:26,415 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,416 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,417 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,417 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,417 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,418 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a17d8d1-3cec-4462-90f4-c0c77d76c5ce-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:26,418 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,420 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,421 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,421 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,421 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,421 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7992780f-0fa1-4c14-beca-90350009f018-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:26,422 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,423 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,424 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,425 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,425 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,425 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d57f919b-2b39-4c28-b8f3-61f31041cb83-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:26,426 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,428 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,429 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,429 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,430 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,430 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-735e4a0c-71b2-4dfe-9652-eddb95823bd0-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:26,431 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,434 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,435 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,436 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,436 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,436 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e132893-66ca-44f4-810d-b31bd307f3bf-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:26,438 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,440 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,442 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,443 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,443 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,444 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e32d4e0b-152b-41f5-9d5d-6319207c046a-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:26,445 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,448 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,449 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,450 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,451 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,451 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d24eaa68-02d1-43c1-b2da-ef94148610ed-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:26,453 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,455 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,457 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,457 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,458 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,458 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad3d0b86-19cf-474c-b3d4-796d7afaa4ae-c000.snappy.parquet, range: 0-13699, partition values: [empty row]
2018-10-11 21:36:26,459 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,461 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,463 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,464 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,464 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,464 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0247bc41-a2af-4b30-a5e0-5bc327f47cc2-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:26,465 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,467 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,469 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,469 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,470 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,470 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-26318166-52ec-4313-a4e5-3dc130f82d18-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:26,471 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,474 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,475 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,476 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,476 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,476 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2227df14-fd69-4e52-88fc-ffa906e8e96a-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:26,478 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,480 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,481 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,482 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,482 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,482 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-16e0e1c7-4724-40d4-8fe9-64c573377aac-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:26,484 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,486 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,487 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,487 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,487 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,488 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-12cc2ef7-5c98-45c7-bfca-9f6852b4f9cb-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:26,489 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,490 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,491 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,492 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,492 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,492 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5704978-3346-4cf0-a83a-fd67411e60c0-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:26,493 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,495 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,496 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,496 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,497 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,497 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-611aa927-9eff-4acc-a956-1419efe00229-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:26,498 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,499 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,500 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,500 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,500 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,500 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b6173355-0a8b-414c-9ed0-27985f88914d-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:26,501 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,503 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,504 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,504 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,504 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,505 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb54fd41-c4a7-41e8-a051-2493b4fc4fed-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:26,506 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,508 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,509 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,509 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,509 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,509 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9af47b28-32d6-4d91-b198-acee5bfe01dd-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:26,510 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,512 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,513 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,513 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,513 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,513 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-db7aecb7-d25c-4a59-ad0e-2bacf5772cb8-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,514 INFO [Executor task launch worker for task 55] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,516 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,516 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,517 INFO [Executor task launch worker for task 55] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,517 WARN [Executor task launch worker for task 55] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,518 INFO [Executor task launch worker for task 55] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 41.0 (TID 55). 1423 bytes result sent to driver
2018-10-11 21:36:26,519 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 1.0 in stage 41.0 (TID 56, localhost, executor driver, partition 1, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:26,519 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 41.0 (TID 55) in 160 ms on localhost (executor driver) (1/8)
2018-10-11 21:36:26,519 INFO [Executor task launch worker for task 56] o.a.s.e.Executor [Logging.scala:54] Running task 1.0 in stage 41.0 (TID 56)
2018-10-11 21:36:26,521 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ae6870f1-1f08-443f-8701-2a8a8fc509da-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,522 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,523 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,525 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,525 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,525 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,526 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d164cc8-eee4-4f35-920d-9fa0984dbb40-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,527 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,529 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,531 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,531 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,531 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,532 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b48d131c-6bd6-478f-9abc-96bd891b0584-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,533 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,534 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,536 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,536 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,537 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,537 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa252a4c-1080-4e94-9dc5-d3e35b8e70ea-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,539 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,541 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,543 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,543 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,543 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,544 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-586baf62-c0dd-443b-a8db-5822599f893a-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,546 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,548 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,549 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,549 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,550 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,550 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5ae7e692-d21c-4b25-9cdf-83f59ef40407-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,551 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,553 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,554 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,554 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,554 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,554 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5d04bbb7-c918-4d38-9348-7833d532fb6c-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,555 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,557 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,558 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,558 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,559 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,559 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-178dd25a-2181-4b65-970b-0683491eca83-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,560 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,561 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,562 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,562 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,562 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,563 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-90d7046c-97fc-4066-9720-fc9230ba0c54-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:26,564 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,566 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,567 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,567 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,567 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,567 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2bc1210e-eaa6-47cf-ba41-7e0b223d190f-c000.snappy.parquet, range: 0-13695, partition values: [empty row]
2018-10-11 21:36:26,568 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,570 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,572 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,572 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,573 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,573 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e7cb913a-50de-4609-aada-ae4eafd1de31-c000.snappy.parquet, range: 0-13694, partition values: [empty row]
2018-10-11 21:36:26,574 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,576 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,577 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,577 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,577 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,578 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f1d46819-9d92-46ba-9025-2adb0818719c-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:26,578 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,580 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,580 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,581 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,581 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,581 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b85e87fe-5382-4ea3-895f-13f9f032ef4b-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:26,582 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,584 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,585 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,585 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,585 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,586 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3831987b-b4b7-4617-a261-5bf721ece358-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:26,587 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,588 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,589 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,590 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,590 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,590 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-21e4180a-5c27-419b-ab1c-78aa6b2cb324-c000.snappy.parquet, range: 0-13691, partition values: [empty row]
2018-10-11 21:36:26,591 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,592 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,593 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,593 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,594 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,594 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c33bde23-fc3e-4548-a230-3eb78aac57fb-c000.snappy.parquet, range: 0-13690, partition values: [empty row]
2018-10-11 21:36:26,595 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,596 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,597 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,597 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,597 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,598 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0232792-2703-4cd0-a57b-e87871b66a88-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,598 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,600 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,601 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,601 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,601 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,602 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6f55e7b2-ed9f-4a3f-9a54-06887558e22d-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,602 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,604 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,606 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,606 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,607 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,607 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4384b7b2-8b67-4077-a182-89d2e37a4833-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,608 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,609 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,610 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,611 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,611 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,611 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19b79ab5-3105-4028-bbd0-c35b902de4a4-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,612 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,613 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,614 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,614 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,615 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,615 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6d920ff1-e3fc-40ac-a84e-386f13c16e67-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,616 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,617 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,618 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,618 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,618 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,618 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d63edb8d-fac0-4840-b019-b0b6e7a639d6-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,620 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,622 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,623 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,623 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,624 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,624 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09613236-c7ea-4e03-a2f6-c2dd85f36bed-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,625 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,626 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,627 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,627 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,628 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,628 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aafd0534-3203-4405-a282-194c9a55812b-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,629 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,631 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,632 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,633 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,633 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,633 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-018f2c55-3be6-41b0-b9cd-c3e9619e0cda-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,634 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,636 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,636 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,637 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,637 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,637 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b9578903-dcf4-4d51-9126-5a90dd27a1ef-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:26,638 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,639 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,640 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,640 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,641 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,641 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9f7a9a62-4bad-4d7b-8940-549d39238770-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:26,642 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,643 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,644 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,645 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,645 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,645 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8b0e504c-c3cf-496a-8730-d46b0b7df1b9-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:26,646 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,647 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,648 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,649 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,649 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,649 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-79b8fe3f-ef98-4c64-bedc-86c3fb9283fd-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:26,650 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,651 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,652 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,652 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,652 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,653 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57dddecf-aa33-4874-99e6-9748f780fe78-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:26,654 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,655 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,657 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,657 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,658 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,658 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7bfc2fc2-2b64-4ee4-83ae-ad0b645e1322-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:26,659 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,660 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,661 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,662 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,662 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,662 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a928c40-d93a-4600-a43e-d6259a8a7c26-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:26,664 INFO [Executor task launch worker for task 56] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,665 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,666 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,666 INFO [Executor task launch worker for task 56] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,667 WARN [Executor task launch worker for task 56] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,667 INFO [Executor task launch worker for task 56] o.a.s.e.Executor [Logging.scala:54] Finished task 1.0 in stage 41.0 (TID 56). 3166 bytes result sent to driver
2018-10-11 21:36:26,668 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 2.0 in stage 41.0 (TID 57, localhost, executor driver, partition 2, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:26,668 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 1.0 in stage 41.0 (TID 56) in 149 ms on localhost (executor driver) (2/8)
2018-10-11 21:36:26,668 INFO [Executor task launch worker for task 57] o.a.s.e.Executor [Logging.scala:54] Running task 2.0 in stage 41.0 (TID 57)
2018-10-11 21:36:26,670 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-62170ceb-3786-48a8-93d4-b567247e8037-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:26,671 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,673 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,675 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,675 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,676 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,676 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8643ee33-f4b3-4b9d-a32e-39e43cfe4ba7-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:26,677 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,679 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,680 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,680 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,680 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,680 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0680d8e3-d6a3-46d1-b2d8-76df28d3b02a-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:26,682 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,684 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,685 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,685 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,686 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,686 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aae2da02-ce8a-4dad-87c4-6a12978949be-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:26,688 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,690 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,692 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,692 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,693 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,693 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8eae07df-593a-43cc-ac4a-2be464a53d3f-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:26,694 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,696 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,697 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,697 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,698 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,698 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a663e89f-b19d-4f56-bfd7-d844367b5dc2-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:26,699 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,701 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,702 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,703 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,703 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,703 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2318bf98-30f1-4cb7-89ff-f54e8f26ee77-c000.snappy.parquet, range: 0-13670, partition values: [empty row]
2018-10-11 21:36:26,704 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,706 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,707 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,708 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,708 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,708 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-854fe582-8eec-43ce-b79a-f5a9b836d2fd-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,709 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,711 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,712 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,712 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,713 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,713 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5793a580-0e06-4393-afec-1a4eefbbd626-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,714 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,716 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,718 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,718 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,718 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,719 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e7b564a-5bd2-4701-bdea-b107946c9b0c-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,720 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,721 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,722 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,723 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,723 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,723 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d10c8980-7d22-4a6d-99ec-053805bdf9ec-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,725 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,726 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,728 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,728 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,728 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,728 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-59858d75-788e-4333-9577-bd4947258eff-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,729 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,730 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,731 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,732 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,732 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,732 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-274749d2-43f1-4c5d-81fe-d888dc9dff47-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,733 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,734 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,735 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,735 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,736 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,736 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ddde392-a677-4c60-92fc-49cd2be2c0f1-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:26,738 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,740 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,741 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,742 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,742 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,742 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b273f389-8ffb-4cbf-b108-6506095ae632-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:26,743 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,745 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,746 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,746 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,746 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,746 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7233e2d1-9862-45dc-9824-4e41f2c2867e-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:26,747 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,749 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,750 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,751 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,751 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,751 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-86a299de-f8fc-46fe-86a9-60a4b26a9b38-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:26,752 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,754 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,754 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,755 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,755 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,755 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-defaedb6-dcf3-4709-916c-a3e955107f58-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:26,756 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,757 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,758 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,758 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,759 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,759 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a837d014-9d5d-454e-a7f6-8f2fb06c37d0-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:26,760 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,761 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,763 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,763 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,763 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,764 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-45a99307-69fd-4a6c-8a0c-91d96b83817a-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:26,765 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,766 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,768 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,768 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,768 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,768 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-29b6c682-0e21-4836-904c-33bbd018067c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:26,770 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,772 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,773 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,773 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,774 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,774 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bbb741a3-045d-451c-a803-96b4d0493b04-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:26,775 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,776 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,776 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,777 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,777 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,777 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e33d9f4-1cd5-43f7-b792-6d744d0a7505-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:26,778 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,779 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,780 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,781 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,781 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,781 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-71b8e55a-40c5-4b5c-9dae-af4d5551ef2c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:26,782 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,783 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,784 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,784 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,784 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,784 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e2d6c34-9f3a-4d0c-9b3f-d0afa5e7e097-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,785 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,786 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,787 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,787 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,788 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,788 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aadda0f-0463-4bfe-9f32-62f122ef3ff4-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,789 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,791 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,792 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,792 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,793 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,793 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bea86d33-775a-4e73-97cd-927d76b9352f-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,794 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,795 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,796 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,796 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,796 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,797 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e0852e5-e63c-45eb-912a-d3964270da43-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,797 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,799 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,800 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,800 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,801 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,801 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4cc621ee-7546-4d9f-8ba0-c4c66f0adeec-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,802 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,805 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,806 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,807 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,807 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,807 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58131256-2685-47d1-abb5-a4357c095a67-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,809 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,810 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,812 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,812 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,813 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,813 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a5a52bbe-0c0c-41d6-a8be-9839d1629b37-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:26,814 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,817 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,818 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,818 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,819 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,819 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-869a108e-1223-444d-9a58-a852afc71fdc-c000.snappy.parquet, range: 0-13658, partition values: [empty row]
2018-10-11 21:36:26,821 INFO [Executor task launch worker for task 57] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,823 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,824 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,824 INFO [Executor task launch worker for task 57] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,825 WARN [Executor task launch worker for task 57] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,825 INFO [Executor task launch worker for task 57] o.a.s.e.Executor [Logging.scala:54] Finished task 2.0 in stage 41.0 (TID 57). 3099 bytes result sent to driver
2018-10-11 21:36:26,826 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 3.0 in stage 41.0 (TID 58, localhost, executor driver, partition 3, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:26,826 INFO [Executor task launch worker for task 58] o.a.s.e.Executor [Logging.scala:54] Running task 3.0 in stage 41.0 (TID 58)
2018-10-11 21:36:26,826 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 2.0 in stage 41.0 (TID 57) in 158 ms on localhost (executor driver) (3/8)
2018-10-11 21:36:26,828 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-666a2e39-edd2-4992-85ee-cd58cc019c85-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:26,829 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,831 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,832 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,832 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,833 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,833 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca11e9c0-3060-4a86-91dd-db9ce2ca7e5f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:26,834 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,835 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,836 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,836 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,836 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,837 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4a6558fd-1e62-4e07-9df1-400871fc5cec-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:26,837 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,839 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,840 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,840 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,840 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,840 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19122b21-1ceb-4cf4-8593-9fa7c8624d2d-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:26,841 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,842 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,843 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,843 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,844 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,844 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d7ef5b02-cddb-495a-aad3-f3679ca76b8f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:26,845 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,846 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,847 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,847 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,847 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,847 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-694f93cb-34fe-4247-821f-f5ca506e1459-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:26,849 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,850 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,851 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,852 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,852 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,852 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-701ae0c0-29df-4b97-839c-9c83df4e1b68-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:26,853 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,855 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,857 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,858 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,858 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,858 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5b6171fb-919a-4c7b-98aa-5dfe9d454475-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:26,860 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,862 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,863 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,863 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,863 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,863 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d21bcc-601f-4e42-b182-081c470c659d-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:26,865 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,866 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,867 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,867 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,867 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,868 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-600d8f03-7aac-4957-baf4-c554ff43494a-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:26,868 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,870 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,871 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,872 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,872 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,872 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e02ee93e-e796-42b0-bf02-5f75dfdc1c6c-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:26,874 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,875 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,876 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,876 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,877 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,877 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a90b0f72-7d9a-4a49-a68b-f290f02aa529-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:26,878 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,880 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,881 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,882 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,882 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,882 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f41d2183-74d9-4df5-be81-a2fb23bf9de7-c000.snappy.parquet, range: 0-11392, partition values: [empty row]
2018-10-11 21:36:26,884 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,886 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,887 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,888 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,888 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,889 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3fa69d1c-3edf-45d2-b0ac-1272168f52c9-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:26,890 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,892 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,892 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,893 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,893 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,893 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a0b202fc-2bf1-495f-9764-acfb0429a5c8-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:26,895 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,897 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,898 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,898 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,898 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,898 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-936820e3-b496-4cc2-8571-9635f4fdbfa3-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:26,899 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,900 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,901 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,901 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,902 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,902 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-eb4c07ef-088b-4a15-be2f-b273fd27d040-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:26,903 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,904 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,905 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,905 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,905 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,905 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4d67cb5-e9dc-4b19-acea-905574bf8522-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:26,906 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,908 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,909 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,909 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,909 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,909 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-224b809f-76f3-4919-a9b8-43b820662112-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:26,910 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,911 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,912 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,912 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,912 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,913 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7501282c-59f0-4b1a-88f5-82366eca768c-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:26,913 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,915 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,915 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,916 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,916 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,916 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3cad3b64-e9c9-41b4-a422-dc665b78c2c1-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:26,917 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,918 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,919 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,919 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,920 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,920 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-390c15b5-d721-4535-be3f-12b05cfbe30a-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:26,921 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,922 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,923 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,924 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,924 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,924 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d28ee0-4831-43b1-a27f-dab415ede7a0-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:26,925 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,927 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,928 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,929 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,929 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,929 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-56056fe1-8392-42d6-b2c1-4463f7381740-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:26,930 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,932 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,933 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,933 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,934 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,934 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dd3999fe-68a8-43bd-b968-d1abe12b7d68-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:26,935 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,936 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,937 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,938 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,938 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,939 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-40f0422a-c31f-4b28-870e-1da30f3d4173-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:26,940 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,942 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,944 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,944 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,945 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,945 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-81102eef-ee91-4905-a219-3a7d6ad68001-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:26,946 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,947 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,949 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,949 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,949 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,950 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a7708ac5-ff0d-4d9b-b829-432325d537bd-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:26,951 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,953 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,955 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,955 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,956 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,956 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-de923178-e27e-43df-9376-a8765b42b212-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:26,957 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,958 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,960 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,960 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,960 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,961 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2366af80-dffe-477b-8217-7f704825188b-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:26,962 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,964 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,966 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,966 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,967 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,967 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad60e729-defb-4a4e-99d9-9ca74fcf6a2d-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:26,969 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,971 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,972 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,973 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:26,973 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,974 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0cf40412-95c9-405c-9299-90b8afa2a414-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:26,975 INFO [Executor task launch worker for task 58] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,977 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,978 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,979 INFO [Executor task launch worker for task 58] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,980 WARN [Executor task launch worker for task 58] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,981 INFO [Executor task launch worker for task 58] o.a.s.e.Executor [Logging.scala:54] Finished task 3.0 in stage 41.0 (TID 58). 1380 bytes result sent to driver
2018-10-11 21:36:26,981 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 4.0 in stage 41.0 (TID 59, localhost, executor driver, partition 4, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:26,982 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 3.0 in stage 41.0 (TID 58) in 156 ms on localhost (executor driver) (4/8)
2018-10-11 21:36:26,982 INFO [Executor task launch worker for task 59] o.a.s.e.Executor [Logging.scala:54] Running task 4.0 in stage 41.0 (TID 59)
2018-10-11 21:36:26,984 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8f84cc33-f1d5-478f-87ce-0663d825408c-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:26,985 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,987 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,988 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,988 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,989 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,989 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64602a48-9868-4fdb-bbce-e23fdb1ff4ad-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:26,990 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,991 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,992 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,992 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,993 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,993 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-22885ce6-f042-455f-87e9-76587998ab27-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:26,994 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,995 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,995 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,996 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,996 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:26,996 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4f0d15a7-be71-41c6-9bcc-c559eb6c8a2a-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:26,997 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:26,998 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:26,999 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:26,999 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:26,999 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,000 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3bf97aa1-dd22-42f0-8f17-b66b1cd270a5-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:27,000 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,002 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,003 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,004 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,004 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,004 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57c3815d-7209-4ed6-90cc-5597db2fc409-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:27,006 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,007 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,008 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,008 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,009 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,009 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fff88882-0587-48ef-95f1-5dd0508833d6-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,010 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,011 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,011 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,012 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,012 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,012 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0df7b64b-c870-4b00-b1dc-59fec5d6c208-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,013 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,014 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,015 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,015 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,015 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,016 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-384bf5ec-e9b8-429f-8281-82432437fb02-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,016 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,018 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,019 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,019 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,019 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,019 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f08e80af-add7-42b2-b4ad-2a22d1ce2a56-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,020 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,021 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,022 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,023 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,023 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,023 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-615e7057-357a-4616-a738-affe25bf8b3a-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,024 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,025 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,026 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,026 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,026 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,026 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-73bd7a91-071c-42d6-a80a-c13db79d72d2-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,027 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,029 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,029 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,030 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,030 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,030 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dbf5cfde-c2ae-4ddf-bdee-0e703dcab699-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,031 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,032 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,034 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,034 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,034 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,034 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-70719782-4616-4b3d-82dd-7bc33b67022f-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,035 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,036 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,037 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,038 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,038 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,038 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1e620604-bee6-486e-b171-860be8b8b75e-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,039 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,040 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,041 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,041 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,041 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,041 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05b626a4-35ba-4e5f-9c93-d7f62d4d3027-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,042 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,044 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,045 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,045 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,045 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,045 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0ac09a5-d2c0-4844-95ed-3706f6eb6930-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,046 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,047 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,048 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,048 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,049 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,049 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e6fcf0f4-d681-401e-b5b6-7075100f24ee-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,049 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,051 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,052 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,052 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,052 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,052 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-00ef3320-f515-47a4-b26f-5b5caf94f65d-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,054 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,055 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,056 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,057 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,057 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,057 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ff4df6d7-b381-401d-987b-233a02e4d909-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,058 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,059 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,060 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,060 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,060 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,060 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f5fe52bb-22d0-4408-b9d4-9b3a37c89088-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,061 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,062 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,063 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,064 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,064 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,064 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a3db9e49-1db1-4e94-a28e-f73df6d27fa8-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,065 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,066 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,067 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,067 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,068 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,068 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-66b4c136-f4b2-4428-9331-2adf6a097511-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:27,068 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,070 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,071 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,071 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,071 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,072 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e3141f79-7a0f-4730-8468-e54a6c54c533-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:27,073 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,075 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,076 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,076 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,077 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,077 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0b0c8187-cf2f-44d8-88fc-49ce4dd62874-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:27,078 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,080 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,081 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,081 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,082 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,082 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-08d62d99-4b8f-4e06-8620-bba67557d59d-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:27,083 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,084 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,085 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,086 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,086 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,087 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64403a21-b2b7-43f7-a22d-5f7706bad082-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:27,088 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,090 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,092 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,092 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,093 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,093 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c1e3a260-62f4-4aeb-8a26-82616f973030-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:27,094 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,096 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,097 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,098 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,098 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,098 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9bff7912-93ac-4fc7-a1db-f8e566a70655-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:27,099 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,102 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,103 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,104 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,104 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,104 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-47728647-7a80-4592-ba7c-c3ebcb1e5224-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:27,106 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,108 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,110 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,110 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,111 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,111 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9b3bccf5-ad25-404d-9310-88a915f9b7a2-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:27,112 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,114 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,116 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,117 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,117 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,117 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-510a5951-088e-4dab-8dd0-c02c47869d79-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:27,118 INFO [Executor task launch worker for task 59] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,121 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,122 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,123 INFO [Executor task launch worker for task 59] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,123 WARN [Executor task launch worker for task 59] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,124 INFO [Executor task launch worker for task 59] o.a.s.e.Executor [Logging.scala:54] Finished task 4.0 in stage 41.0 (TID 59). 1380 bytes result sent to driver
2018-10-11 21:36:27,125 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 5.0 in stage 41.0 (TID 60, localhost, executor driver, partition 5, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:27,126 INFO [Executor task launch worker for task 60] o.a.s.e.Executor [Logging.scala:54] Running task 5.0 in stage 41.0 (TID 60)
2018-10-11 21:36:27,126 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 4.0 in stage 41.0 (TID 59) in 145 ms on localhost (executor driver) (5/8)
2018-10-11 21:36:27,127 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6650061f-a0ab-42f3-be0d-e29bba951689-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:27,129 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,131 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,133 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,133 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,134 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,134 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a73de2ae-2a91-46fe-a61e-d06104068675-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:27,135 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,138 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,139 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,140 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,140 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,141 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-60348a41-e7a8-4c02-9dbf-a4848152bf0a-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:27,142 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,144 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,145 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,146 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,146 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,147 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca9d3388-a32e-42aa-9b9a-f2ba826d1b2e-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:27,148 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,151 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,152 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,152 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,153 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,153 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46ee92fd-a03b-4ad7-a847-fac273921f57-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:27,154 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,156 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,157 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,158 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,158 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,159 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4317e6e3-7c07-49f4-9bea-564768382089-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:27,160 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,162 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,163 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,164 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,164 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,164 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dc5fe75d-ae31-4bd0-a866-d3274b2d629b-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:27,166 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,168 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,170 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,171 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,172 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,172 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-49c1ab9d-a225-4ba8-911f-7c8138662881-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:27,174 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,176 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,178 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,178 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,179 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,179 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a910a7f7-d2c2-4fb9-9907-6f0a12ef5816-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,180 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,181 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,183 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,183 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,184 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,184 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af526506-da3a-461a-b7d6-7576a7ca2dc6-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,186 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,187 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,188 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,189 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,189 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,189 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f7492e12-3b8e-43e6-b84c-e42d363116f9-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,190 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,192 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,193 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,194 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,194 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,194 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-272bed24-7d51-4fc6-9a8a-aba3fb69c4b8-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,196 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,197 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,199 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,199 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,200 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,200 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ecac671-fc2e-4df4-9315-bf4c516db607-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,201 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,204 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,206 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,206 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,207 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,207 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f2c3b673-16d6-4469-a5ed-13de2cc92f10-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,208 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,210 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,211 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,211 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,212 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,212 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a6947b4-4ad6-4ab6-8907-a8320e284bcb-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:27,213 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,214 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,215 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,215 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,216 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,216 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3a0fc732-ce1b-4da0-9cd5-162ab4573e6e-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,217 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,218 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,219 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,219 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,220 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,220 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a087b5c1-75ef-4738-bcac-2ee86aa294bd-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,221 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,223 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,224 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,224 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,224 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,224 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e0eae261-dec5-4554-b126-6e1829f78050-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,225 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,227 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,228 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,228 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,229 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,229 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-987c95c1-23dd-407a-98e6-0e238fe1473b-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,230 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,231 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,232 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,232 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,233 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,233 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb60375b-4e46-4d45-99c6-540adc7208c3-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,234 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,235 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,236 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,236 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,237 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,237 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-319157ef-58f7-4406-b9b1-6baf5b53b0a8-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,238 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,239 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,240 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,241 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,241 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,241 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8022bee4-ba1f-4f2d-87cc-7ae422111479-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,243 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,245 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,247 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,247 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,248 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,248 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-861da466-0eee-47d7-9ff5-971c6d7f5a93-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,249 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,251 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,253 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,254 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,254 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,254 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8a3516de-8b82-44d1-82a8-69aa660cbbdf-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,256 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,258 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,260 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,260 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,261 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,261 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f419bf0e-cdd8-4f6c-8beb-f4dedb61a260-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,262 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,264 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,266 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,266 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,267 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,267 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e766d71-e245-4297-857b-bdd21b58b6f4-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,268 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,270 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,272 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,272 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,273 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,273 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4c33fd2a-9846-4f6b-a1f0-dccf06433485-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,274 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,276 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,277 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,277 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,278 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,278 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8fc6b69-477b-42e7-9d1a-3c499d4d10b2-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:27,279 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,281 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,282 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,282 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,283 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,283 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9039fff7-2957-4b0e-992e-e87565413ec8-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,284 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,286 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,288 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,288 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,289 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,289 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ef4e6a53-30c6-43c0-8eff-8a34e34edf6b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,290 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,292 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,293 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,293 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,293 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,293 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a398596-1df5-4ca7-911f-e74abd3762dd-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,294 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,296 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,296 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,297 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,297 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,297 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58f4dff5-7214-4777-ba17-784e2cef12f4-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,298 INFO [Executor task launch worker for task 60] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,300 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,301 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,302 INFO [Executor task launch worker for task 60] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,302 WARN [Executor task launch worker for task 60] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,303 INFO [Executor task launch worker for task 60] o.a.s.e.Executor [Logging.scala:54] Finished task 5.0 in stage 41.0 (TID 60). 1423 bytes result sent to driver
2018-10-11 21:36:27,304 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 6.0 in stage 41.0 (TID 61, localhost, executor driver, partition 6, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:27,304 INFO [Executor task launch worker for task 61] o.a.s.e.Executor [Logging.scala:54] Running task 6.0 in stage 41.0 (TID 61)
2018-10-11 21:36:27,304 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 5.0 in stage 41.0 (TID 60) in 179 ms on localhost (executor driver) (6/8)
2018-10-11 21:36:27,306 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2422c402-71d9-41d7-9987-1d4ab7fc19fb-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,307 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,309 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,310 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,310 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,310 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,310 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0ff4b192-b8e8-4b6c-9828-f5d9ec00cbce-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,311 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,312 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,313 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,314 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,314 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,314 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a2d93e67-4023-47f3-9d4a-f50a9c8541fe-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,315 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,317 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,318 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,318 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,319 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,319 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a29231b4-e038-4ee0-88ef-1d68cfe9d2be-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,320 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,321 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,322 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,322 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,322 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,323 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-55b99cce-2e82-46d0-8e53-bee4918b429e-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,324 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,325 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,326 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,326 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,327 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,327 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9dc2a350-c1b5-4c5c-8f81-b54fcb2ffa3a-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,328 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,329 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,330 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,330 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,331 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,331 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1be87a1d-cfca-46a7-b3c5-5377826665a1-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,332 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,334 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,335 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,335 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,336 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,336 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ac9002e-c497-481d-b337-445644058c65-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,337 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,339 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,340 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,341 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,341 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,341 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7453f0e9-fcdc-45d9-aa4e-04e2742b8b2b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,343 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,345 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,346 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,346 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,347 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,347 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3ce2c6c2-8244-4825-9d79-b2816de2acda-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,348 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,349 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,350 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,350 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,350 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,350 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e32c7dc-dcea-456e-a878-c9c835248ea9-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,351 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,353 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,355 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,355 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,356 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,356 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b2dcecd1-372e-4f0b-9562-f96ebf3af8f6-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,358 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,360 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,362 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,363 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,363 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,363 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b266ce1f-aa2d-4c98-8bd7-38ee65369d89-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,364 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,366 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,368 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,368 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,368 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,368 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6c58af95-e7c8-4556-989d-8a8fc881fc54-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:27,369 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,370 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,371 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,372 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,372 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,372 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-da19fb6b-0851-4b26-8454-89e7d9d29409-c000.snappy.parquet, range: 0-11359, partition values: [empty row]
2018-10-11 21:36:27,373 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,374 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,375 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,375 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,376 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,376 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2646c07b-af07-4e07-8e28-e3b57bbb514c-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,377 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,379 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,380 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,381 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,381 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,381 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7fed2b39-6f8a-4f9e-8921-caa7441424ec-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,382 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,384 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,385 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,385 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,386 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,386 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f4b5fa6b-5fa7-4ccf-b1ac-7fe087ca4972-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,387 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,389 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,390 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,390 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,391 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,391 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e8617665-882c-4cd9-af6c-d70cf9357935-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,391 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,393 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,393 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,394 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,394 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,394 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9c42e760-f3d5-4337-93cc-2faeb5608d5f-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,395 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,396 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,397 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,397 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,398 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,398 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-992bc073-6936-47bb-9ce0-4bd6ff5312f3-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,399 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,400 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,400 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,401 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,401 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,401 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bdc96751-af2e-4c0d-aad9-9b6309410e26-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,402 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,404 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,406 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,406 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,407 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,407 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2a8062ca-53c1-4efb-887d-a5f15b2e2675-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:27,408 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,410 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,412 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,412 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,413 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,413 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-461919ce-4e71-49bc-ab59-c7e999f082df-c000.snappy.parquet, range: 0-11357, partition values: [empty row]
2018-10-11 21:36:27,414 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,415 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,416 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,416 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,417 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,417 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d184ff46-c4c0-4adb-993f-d1625bf18c93-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:27,418 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,419 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,420 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,420 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,420 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,421 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a542cfa-fb0f-420c-808c-c23861bc5e49-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:27,421 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,422 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,423 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,424 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,424 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,424 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3744292d-429a-4b99-8f87-0174a77a5f02-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:27,425 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,427 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,428 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,428 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,429 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,429 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ce13747d-84bc-4b83-bcbf-ed87cb408c63-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:27,430 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,431 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,432 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,432 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,432 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,432 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa3ef260-acd7-4ea3-b2bb-08ce39dc72a4-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:27,433 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,434 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,435 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,435 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,436 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,436 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09afdad3-a22e-46d0-8913-a98493735de8-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:27,437 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,438 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,439 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,439 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,439 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,439 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4de0605-1810-4659-8565-775942a08750-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:27,440 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,441 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,442 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,442 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,443 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,443 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c9cfad0a-8f5c-4599-8a7c-0d13b6068b66-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:27,444 INFO [Executor task launch worker for task 61] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,445 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,446 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,446 INFO [Executor task launch worker for task 61] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,446 WARN [Executor task launch worker for task 61] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,447 INFO [Executor task launch worker for task 61] o.a.s.e.Executor [Logging.scala:54] Finished task 6.0 in stage 41.0 (TID 61). 1423 bytes result sent to driver
2018-10-11 21:36:27,447 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 7.0 in stage 41.0 (TID 62, localhost, executor driver, partition 7, PROCESS_LOCAL, 10904 bytes)
2018-10-11 21:36:27,448 INFO [Executor task launch worker for task 62] o.a.s.e.Executor [Logging.scala:54] Running task 7.0 in stage 41.0 (TID 62)
2018-10-11 21:36:27,448 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 6.0 in stage 41.0 (TID 61) in 145 ms on localhost (executor driver) (7/8)
2018-10-11 21:36:27,449 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aa132c9-1c2c-4bed-8054-4f01f905b173-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:27,450 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,452 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,453 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,453 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,454 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,454 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1f52f9b8-ea21-4d2b-a7d2-11744187f778-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:27,455 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,456 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,457 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,457 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,458 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,458 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-51f29362-71b4-40bb-9ac9-c8fd50e8b54c-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,459 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,460 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,461 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,461 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,461 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,461 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca8ad858-bbe8-4c3b-a0f5-ea89627075a3-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,462 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,463 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,464 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,465 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,465 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,465 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-000c5114-25aa-43b6-ae19-ab0069bbeadf-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,466 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,467 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,468 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,468 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,469 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,469 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af314aee-5298-45df-899e-f5cf50b26e6a-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,469 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,471 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,471 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,472 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,472 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,472 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-06648b17-d906-45c9-b987-0c863fb63c7b-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,473 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,474 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,475 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,475 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,475 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,476 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-deaed750-dc34-4faa-bfea-c7b9e6d08139-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,476 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,478 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,478 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,479 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,479 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,479 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-edc1f6e1-52af-4b5a-9623-413fdd0995f1-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:27,480 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,481 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,490 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,490 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,490 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,490 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1332
2018-10-11 21:36:27,491 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46686477-1f45-40b8-b264-443e98dbf08f-c000.snappy.parquet, range: 0-11350, partition values: [empty row]
2018-10-11 21:36:27,491 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1347
2018-10-11 21:36:27,491 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1350
2018-10-11 21:36:27,491 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1326
2018-10-11 21:36:27,491 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1327
2018-10-11 21:36:27,491 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1344
2018-10-11 21:36:27,491 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_60_piece0 on 192.168.0.206:34485 in memory (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:27,491 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,492 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1323
2018-10-11 21:36:27,492 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1321
2018-10-11 21:36:27,492 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1331
2018-10-11 21:36:27,492 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1336
2018-10-11 21:36:27,493 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_61_piece0 on 192.168.0.206:34485 in memory (size: 6.7 KB, free: 1391.4 MB)
2018-10-11 21:36:27,493 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,493 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1341
2018-10-11 21:36:27,493 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1349
2018-10-11 21:36:27,493 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1324
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1346
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1334
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1342
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1337
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1339
2018-10-11 21:36:27,494 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1345
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1329
2018-10-11 21:36:27,494 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1328
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1325
2018-10-11 21:36:27,494 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,494 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1338
2018-10-11 21:36:27,495 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d8e5bc47-ef5e-484a-98b7-34e73c59faf3-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1335
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1322
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1351
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1343
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1330
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1340
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1348
2018-10-11 21:36:27,495 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1333
2018-10-11 21:36:27,495 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,497 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,497 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,498 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,498 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,498 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-217b388a-5636-42f5-a2f7-f7e650de6c64-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:27,499 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,500 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,501 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,501 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,501 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,501 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-824fe1c8-dff2-491e-86b8-2e2542684692-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:27,502 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,504 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,505 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,505 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,505 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,506 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f812e08f-6f88-4c2d-96ec-83fdeb1e434a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:27,507 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,508 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,509 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,509 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,509 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,509 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9020ecd0-b01d-408c-baa5-a3b077d92f4a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:27,510 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,512 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,513 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,513 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,514 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,514 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5dccade-9231-4d83-bfc0-eeddbd723979-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:27,515 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,516 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,517 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,517 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,518 WARN [Executor task launch worker for task 62] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,518 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ce47add-8d58-4bb1-b893-375d2709dd55-c000.snappy.parquet, range: 0-882, partition values: [empty row]
2018-10-11 21:36:27,518 INFO [Executor task launch worker for task 62] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,520 INFO [Executor task launch worker for task 62] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:27,521 INFO [Executor task launch worker for task 62] o.a.s.e.Executor [Logging.scala:54] Finished task 7.0 in stage 41.0 (TID 62). 1466 bytes result sent to driver
2018-10-11 21:36:27,522 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 7.0 in stage 41.0 (TID 62) in 74 ms on localhost (executor driver) (8/8)
2018-10-11 21:36:27,522 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 41.0, whose tasks have all completed, from pool 
2018-10-11 21:36:27,522 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 41 (collect at SparkDataManager.scala:77) finished in 1.166 s
2018-10-11 21:36:27,522 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 30 finished: collect at SparkDataManager.scala:77, took 1.168665 s
2018-10-11 21:36:27,524 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,524 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,524 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,525 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,525 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,525 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,525 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,525 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,526 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,526 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,526 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,526 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,526 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,526 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,527 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,527 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,527 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,527 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,527 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,527 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,528 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 21:36:27,528 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:27,529 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:27,531 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:27,532 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:27,544 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:27,544 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:27,647 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:27,648 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:27,648 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:27,648 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:27,665 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 12.400677 ms
2018-10-11 21:36:27,696 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 22.21141 ms
2018-10-11 21:36:27,699 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_64 stored as values in memory (estimated size 230.0 KB, free 1390.7 MB)
2018-10-11 21:36:27,705 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_64_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.6 MB)
2018-10-11 21:36:27,706 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_64_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:27,707 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 64 from collect at SparkDataManager.scala:66
2018-10-11 21:36:27,707 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 16791983 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:27,714 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:66
2018-10-11 21:36:27,715 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 31 (collect at SparkDataManager.scala:66) with 1 output partitions
2018-10-11 21:36:27,715 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 42 (collect at SparkDataManager.scala:66)
2018-10-11 21:36:27,715 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:27,715 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:27,716 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 42 (MapPartitionsRDD[135] at collect at SparkDataManager.scala:66), which has no missing parents
2018-10-11 21:36:27,717 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_65 stored as values in memory (estimated size 31.8 KB, free 1390.6 MB)
2018-10-11 21:36:27,718 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_65_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1390.6 MB)
2018-10-11 21:36:27,719 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_65_piece0 in memory on 192.168.0.206:34485 (size: 10.5 KB, free: 1391.3 MB)
2018-10-11 21:36:27,719 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 65 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:27,720 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[135] at collect at SparkDataManager.scala:66) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:27,720 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 42.0 with 1 tasks
2018-10-11 21:36:27,720 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 42.0 (TID 63, localhost, executor driver, partition 0, PROCESS_LOCAL, 8839 bytes)
2018-10-11 21:36:27,721 INFO [Executor task launch worker for task 63] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 42.0 (TID 63)
2018-10-11 21:36:27,725 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:27,727 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:27,730 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,732 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,733 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,734 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,734 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,735 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:27,736 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:27,740 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,742 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,743 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,743 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,744 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,744 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,744 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:27,745 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:27,750 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,752 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,753 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,753 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,754 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,754 WARN [Executor task launch worker for task 63] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,754 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:27,755 INFO [Executor task launch worker for task 63] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:27,761 INFO [Executor task launch worker for task 63] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:27,765 INFO [Executor task launch worker for task 63] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 42.0 (TID 63). 2168 bytes result sent to driver
2018-10-11 21:36:27,765 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 42.0 (TID 63) in 45 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:27,765 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 42.0, whose tasks have all completed, from pool 
2018-10-11 21:36:27,766 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 42 (collect at SparkDataManager.scala:66) finished in 0.050 s
2018-10-11 21:36:27,766 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 31 finished: collect at SparkDataManager.scala:66, took 0.051859 s
2018-10-11 21:36:27,767 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,768 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,768 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,768 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:27,769 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:27,772 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:27,773 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:27,780 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:27,781 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:27,825 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:27,825 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:27,826 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<statement: struct<id: string, text: string>, startTime: bigint, endTime: bigint, description: string, details: string ... 1 more field>
2018-10-11 21:36:27,826 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:27,836 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_66 stored as values in memory (estimated size 228.2 KB, free 1390.4 MB)
2018-10-11 21:36:27,843 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_66_piece0 stored as bytes in memory (estimated size 21.7 KB, free 1390.4 MB)
2018-10-11 21:36:27,843 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_66_piece0 in memory on 192.168.0.206:34485 (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:27,844 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 66 from collect at SparkDataManager.scala:77
2018-10-11 21:36:27,844 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:27,855 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:77
2018-10-11 21:36:27,856 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 32 (collect at SparkDataManager.scala:77) with 8 output partitions
2018-10-11 21:36:27,856 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 43 (collect at SparkDataManager.scala:77)
2018-10-11 21:36:27,856 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:27,856 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:27,857 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 43 (MapPartitionsRDD[138] at collect at SparkDataManager.scala:77), which has no missing parents
2018-10-11 21:36:27,858 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_67 stored as values in memory (estimated size 16.2 KB, free 1390.3 MB)
2018-10-11 21:36:27,859 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_67_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1390.3 MB)
2018-10-11 21:36:27,860 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_67_piece0 in memory on 192.168.0.206:34485 (size: 6.7 KB, free: 1391.3 MB)
2018-10-11 21:36:27,860 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 67 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:27,860 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 8 missing tasks from ResultStage 43 (MapPartitionsRDD[138] at collect at SparkDataManager.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-10-11 21:36:27,860 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 43.0 with 8 tasks
2018-10-11 21:36:27,861 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 43.0 (TID 64, localhost, executor driver, partition 0, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:27,861 INFO [Executor task launch worker for task 64] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 43.0 (TID 64)
2018-10-11 21:36:27,863 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e31abc9-a228-4133-b02e-0f6cd7638510-c000.snappy.parquet, range: 0-13723, partition values: [empty row]
2018-10-11 21:36:27,864 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,866 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,867 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,867 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,868 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,868 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b1982672-2d41-4bff-8f45-9e6f8c921d5d-c000.snappy.parquet, range: 0-13722, partition values: [empty row]
2018-10-11 21:36:27,869 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,871 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,872 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,872 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,873 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,873 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e5316a0-c580-49fd-91d0-550955cd9bbc-c000.snappy.parquet, range: 0-13721, partition values: [empty row]
2018-10-11 21:36:27,874 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,875 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,876 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,876 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,876 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,876 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e15f7475-849c-45f2-9dcf-5bb2d5b69bd4-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:27,877 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,878 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,879 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,879 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,880 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,880 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a46c4b1e-6e4f-4382-a659-662347269774-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:27,881 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,883 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,884 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,884 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,885 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,885 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c7514033-0170-45b0-ad15-cb3097418dfb-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:27,885 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,887 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,888 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,888 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,889 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,889 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3723b14f-e12a-4f5a-b016-809b54de139c-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:27,890 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,891 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,892 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,892 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,892 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,893 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-07c85578-afbb-490a-bd9d-3837ab2a7baa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:27,893 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,895 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,895 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,896 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,896 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,896 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-603f4016-bd22-4ed6-b2c7-be69003d48fa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:27,897 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,899 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,900 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,901 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,901 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,901 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a6043ae9-7f4d-4793-ad20-a149ce2339bb-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:27,902 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,904 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,905 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,906 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,906 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,906 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8d65fb3-afa1-45c2-9354-709d30d7ae22-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:27,907 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,909 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,910 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,910 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,910 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,910 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0e52f33e-2792-451e-bfca-4fb2393d1315-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:27,911 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,912 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,913 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,914 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,914 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,914 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e001f918-e226-4b11-a792-e057310c71dc-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:27,915 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,917 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,918 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,918 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,918 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,919 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a17d8d1-3cec-4462-90f4-c0c77d76c5ce-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:27,919 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,921 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,922 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,922 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,922 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,922 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7992780f-0fa1-4c14-beca-90350009f018-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:27,924 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,925 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,927 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,927 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,927 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,927 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d57f919b-2b39-4c28-b8f3-61f31041cb83-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:27,928 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,930 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,931 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,931 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,932 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,932 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-735e4a0c-71b2-4dfe-9652-eddb95823bd0-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:27,933 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,935 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,936 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,937 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,937 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,937 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e132893-66ca-44f4-810d-b31bd307f3bf-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:27,939 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,941 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,943 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,943 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,943 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,944 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e32d4e0b-152b-41f5-9d5d-6319207c046a-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:27,945 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,947 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,948 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,949 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,949 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,950 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d24eaa68-02d1-43c1-b2da-ef94148610ed-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:27,951 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,953 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,954 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,955 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:27,955 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,955 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad3d0b86-19cf-474c-b3d4-796d7afaa4ae-c000.snappy.parquet, range: 0-13699, partition values: [empty row]
2018-10-11 21:36:27,956 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,958 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,960 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,960 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,961 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,961 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0247bc41-a2af-4b30-a5e0-5bc327f47cc2-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:27,962 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,964 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,965 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,966 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,966 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,966 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-26318166-52ec-4313-a4e5-3dc130f82d18-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:27,968 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,970 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,971 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,972 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,972 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,973 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2227df14-fd69-4e52-88fc-ffa906e8e96a-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:27,974 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,976 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,977 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,978 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,978 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,979 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-16e0e1c7-4724-40d4-8fe9-64c573377aac-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:27,980 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,982 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,983 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,984 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,984 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,984 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-12cc2ef7-5c98-45c7-bfca-9f6852b4f9cb-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:27,986 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,987 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,988 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,989 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,989 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,990 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5704978-3346-4cf0-a83a-fd67411e60c0-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:27,991 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,992 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,993 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,993 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,994 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,994 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-611aa927-9eff-4acc-a956-1419efe00229-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:27,996 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:27,997 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:27,998 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:27,998 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:27,998 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:27,999 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b6173355-0a8b-414c-9ed0-27985f88914d-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:27,999 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,001 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,002 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,003 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,003 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,004 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb54fd41-c4a7-41e8-a051-2493b4fc4fed-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:28,005 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,007 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,008 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,008 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,008 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,009 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9af47b28-32d6-4d91-b198-acee5bfe01dd-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:28,009 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,010 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,011 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,011 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,012 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,012 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-db7aecb7-d25c-4a59-ad0e-2bacf5772cb8-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,013 INFO [Executor task launch worker for task 64] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,014 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,015 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,016 INFO [Executor task launch worker for task 64] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,016 WARN [Executor task launch worker for task 64] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,017 INFO [Executor task launch worker for task 64] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 43.0 (TID 64). 1466 bytes result sent to driver
2018-10-11 21:36:28,017 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 1.0 in stage 43.0 (TID 65, localhost, executor driver, partition 1, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:28,018 INFO [Executor task launch worker for task 65] o.a.s.e.Executor [Logging.scala:54] Running task 1.0 in stage 43.0 (TID 65)
2018-10-11 21:36:28,018 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 43.0 (TID 64) in 157 ms on localhost (executor driver) (1/8)
2018-10-11 21:36:28,020 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ae6870f1-1f08-443f-8701-2a8a8fc509da-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,021 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,022 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,023 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,023 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,024 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,024 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d164cc8-eee4-4f35-920d-9fa0984dbb40-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,025 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,026 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,027 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,027 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,028 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,028 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b48d131c-6bd6-478f-9abc-96bd891b0584-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,029 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,030 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,031 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,031 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,032 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,032 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa252a4c-1080-4e94-9dc5-d3e35b8e70ea-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,033 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,034 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,035 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,035 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,035 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,035 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-586baf62-c0dd-443b-a8db-5822599f893a-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,036 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,037 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,038 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,039 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,039 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,039 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5ae7e692-d21c-4b25-9cdf-83f59ef40407-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,040 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,042 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,043 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,044 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,044 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,044 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5d04bbb7-c918-4d38-9348-7833d532fb6c-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,045 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,046 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,047 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,047 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,047 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,047 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-178dd25a-2181-4b65-970b-0683491eca83-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,048 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,049 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,050 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,050 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,051 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,051 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-90d7046c-97fc-4066-9720-fc9230ba0c54-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:28,052 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,054 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,055 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,056 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,056 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,056 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2bc1210e-eaa6-47cf-ba41-7e0b223d190f-c000.snappy.parquet, range: 0-13695, partition values: [empty row]
2018-10-11 21:36:28,057 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,059 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,060 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,060 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,060 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,061 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e7cb913a-50de-4609-aada-ae4eafd1de31-c000.snappy.parquet, range: 0-13694, partition values: [empty row]
2018-10-11 21:36:28,061 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,063 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,063 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,064 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,064 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,064 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f1d46819-9d92-46ba-9025-2adb0818719c-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:28,065 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,066 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,067 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,067 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,067 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,068 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b85e87fe-5382-4ea3-895f-13f9f032ef4b-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:28,069 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,070 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,071 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,071 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,072 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,072 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3831987b-b4b7-4617-a261-5bf721ece358-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:28,073 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,074 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,075 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,075 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,075 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,076 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-21e4180a-5c27-419b-ab1c-78aa6b2cb324-c000.snappy.parquet, range: 0-13691, partition values: [empty row]
2018-10-11 21:36:28,076 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,077 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,078 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,079 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,079 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,079 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c33bde23-fc3e-4548-a230-3eb78aac57fb-c000.snappy.parquet, range: 0-13690, partition values: [empty row]
2018-10-11 21:36:28,080 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,081 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,082 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,082 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,083 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,083 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0232792-2703-4cd0-a57b-e87871b66a88-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,084 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,085 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,086 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,087 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,087 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,087 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6f55e7b2-ed9f-4a3f-9a54-06887558e22d-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,088 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,090 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,091 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,091 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,091 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,091 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4384b7b2-8b67-4077-a182-89d2e37a4833-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,092 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,093 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,094 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,095 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,095 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,095 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19b79ab5-3105-4028-bbd0-c35b902de4a4-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,096 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,097 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,098 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,098 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,098 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,098 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6d920ff1-e3fc-40ac-a84e-386f13c16e67-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,099 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,100 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,101 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,101 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,102 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,102 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d63edb8d-fac0-4840-b019-b0b6e7a639d6-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,103 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,104 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,105 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,105 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,106 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,106 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09613236-c7ea-4e03-a2f6-c2dd85f36bed-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,107 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,108 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,109 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,109 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,110 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,110 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aafd0534-3203-4405-a282-194c9a55812b-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,110 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,112 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,113 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,113 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,113 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,113 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-018f2c55-3be6-41b0-b9cd-c3e9619e0cda-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,114 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,116 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,117 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,117 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,117 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,118 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b9578903-dcf4-4d51-9126-5a90dd27a1ef-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:28,118 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,120 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,120 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,121 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,121 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,121 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9f7a9a62-4bad-4d7b-8940-549d39238770-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:28,122 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,123 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,124 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,124 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,125 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,125 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8b0e504c-c3cf-496a-8730-d46b0b7df1b9-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:28,126 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,127 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,128 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,128 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,128 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,128 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-79b8fe3f-ef98-4c64-bedc-86c3fb9283fd-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:28,129 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,130 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,131 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,132 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,132 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,132 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57dddecf-aa33-4874-99e6-9748f780fe78-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:28,133 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,134 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,135 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,135 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,135 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,135 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7bfc2fc2-2b64-4ee4-83ae-ad0b645e1322-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:28,136 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,138 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,139 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,140 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,140 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,140 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a928c40-d93a-4600-a43e-d6259a8a7c26-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:28,141 INFO [Executor task launch worker for task 65] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,143 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,143 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,144 INFO [Executor task launch worker for task 65] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,144 WARN [Executor task launch worker for task 65] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,145 INFO [Executor task launch worker for task 65] o.a.s.e.Executor [Logging.scala:54] Finished task 1.0 in stage 43.0 (TID 65). 1423 bytes result sent to driver
2018-10-11 21:36:28,145 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 2.0 in stage 43.0 (TID 66, localhost, executor driver, partition 2, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:28,146 INFO [Executor task launch worker for task 66] o.a.s.e.Executor [Logging.scala:54] Running task 2.0 in stage 43.0 (TID 66)
2018-10-11 21:36:28,146 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 1.0 in stage 43.0 (TID 65) in 129 ms on localhost (executor driver) (2/8)
2018-10-11 21:36:28,147 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-62170ceb-3786-48a8-93d4-b567247e8037-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:28,149 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,150 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,152 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,152 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,152 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,153 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8643ee33-f4b3-4b9d-a32e-39e43cfe4ba7-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:28,154 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,156 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,157 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,158 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,158 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,159 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0680d8e3-d6a3-46d1-b2d8-76df28d3b02a-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:28,160 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,162 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,163 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,163 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,163 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,163 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aae2da02-ce8a-4dad-87c4-6a12978949be-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:28,164 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,166 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,167 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,167 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,168 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,168 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8eae07df-593a-43cc-ac4a-2be464a53d3f-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:28,169 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,171 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,172 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,173 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,173 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,173 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a663e89f-b19d-4f56-bfd7-d844367b5dc2-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:28,174 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,176 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,177 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,177 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,178 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,178 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2318bf98-30f1-4cb7-89ff-f54e8f26ee77-c000.snappy.parquet, range: 0-13670, partition values: [empty row]
2018-10-11 21:36:28,179 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,180 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,181 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,181 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,182 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,182 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-854fe582-8eec-43ce-b79a-f5a9b836d2fd-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,183 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,184 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,185 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,185 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,186 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,186 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5793a580-0e06-4393-afec-1a4eefbbd626-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,187 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,188 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,189 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,190 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,190 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,190 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e7b564a-5bd2-4701-bdea-b107946c9b0c-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,191 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,192 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,193 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,193 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,194 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,194 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d10c8980-7d22-4a6d-99ec-053805bdf9ec-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,195 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,196 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,197 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,197 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,197 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,197 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-59858d75-788e-4333-9577-bd4947258eff-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,199 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,201 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,202 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,202 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,203 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,203 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-274749d2-43f1-4c5d-81fe-d888dc9dff47-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,204 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,206 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,207 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,207 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,208 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,208 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ddde392-a677-4c60-92fc-49cd2be2c0f1-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:28,209 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,211 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,212 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,212 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,213 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,213 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b273f389-8ffb-4cbf-b108-6506095ae632-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:28,214 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,215 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,216 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,216 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,217 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,217 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7233e2d1-9862-45dc-9824-4e41f2c2867e-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:28,218 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,219 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,220 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,221 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,221 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,221 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-86a299de-f8fc-46fe-86a9-60a4b26a9b38-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:28,223 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,225 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,226 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,226 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,227 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,227 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-defaedb6-dcf3-4709-916c-a3e955107f58-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:28,228 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,229 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,231 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,231 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,232 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,232 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a837d014-9d5d-454e-a7f6-8f2fb06c37d0-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:28,233 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,235 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,236 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,236 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,237 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,237 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-45a99307-69fd-4a6c-8a0c-91d96b83817a-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:28,238 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,239 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,240 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,241 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,241 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,241 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-29b6c682-0e21-4836-904c-33bbd018067c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:28,242 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,245 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,246 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,247 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,248 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,248 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bbb741a3-045d-451c-a803-96b4d0493b04-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:28,250 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,251 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,252 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,253 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,253 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,254 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e33d9f4-1cd5-43f7-b792-6d744d0a7505-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:28,256 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,257 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,258 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,258 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,259 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,259 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-71b8e55a-40c5-4b5c-9dae-af4d5551ef2c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:28,260 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,261 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,262 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,262 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,262 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,263 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e2d6c34-9f3a-4d0c-9b3f-d0afa5e7e097-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,263 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,265 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,265 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,266 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,266 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,266 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aadda0f-0463-4bfe-9f32-62f122ef3ff4-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,267 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,268 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,270 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,270 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,270 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,271 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bea86d33-775a-4e73-97cd-927d76b9352f-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,272 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,273 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,274 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,274 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,275 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,275 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e0852e5-e63c-45eb-912a-d3964270da43-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,276 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,277 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,278 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,278 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,279 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,279 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4cc621ee-7546-4d9f-8ba0-c4c66f0adeec-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,280 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,281 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,282 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,282 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,282 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,283 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58131256-2685-47d1-abb5-a4357c095a67-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,284 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,285 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,286 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,287 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,287 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,287 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a5a52bbe-0c0c-41d6-a8be-9839d1629b37-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:28,289 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,291 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,292 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,292 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,292 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,293 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-869a108e-1223-444d-9a58-a852afc71fdc-c000.snappy.parquet, range: 0-13658, partition values: [empty row]
2018-10-11 21:36:28,293 INFO [Executor task launch worker for task 66] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,295 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,296 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,296 INFO [Executor task launch worker for task 66] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,296 WARN [Executor task launch worker for task 66] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,297 INFO [Executor task launch worker for task 66] o.a.s.e.Executor [Logging.scala:54] Finished task 2.0 in stage 43.0 (TID 66). 1423 bytes result sent to driver
2018-10-11 21:36:28,298 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 3.0 in stage 43.0 (TID 67, localhost, executor driver, partition 3, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:28,298 INFO [Executor task launch worker for task 67] o.a.s.e.Executor [Logging.scala:54] Running task 3.0 in stage 43.0 (TID 67)
2018-10-11 21:36:28,298 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 2.0 in stage 43.0 (TID 66) in 153 ms on localhost (executor driver) (3/8)
2018-10-11 21:36:28,300 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-666a2e39-edd2-4992-85ee-cd58cc019c85-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:28,301 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,303 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,304 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,305 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,305 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,306 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca11e9c0-3060-4a86-91dd-db9ce2ca7e5f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:28,307 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,309 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,310 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,311 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,311 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,311 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4a6558fd-1e62-4e07-9df1-400871fc5cec-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:28,313 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,315 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,316 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,316 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,317 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,317 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19122b21-1ceb-4cf4-8593-9fa7c8624d2d-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:28,318 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,320 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,322 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,322 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,322 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,323 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d7ef5b02-cddb-495a-aad3-f3679ca76b8f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:28,324 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,326 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,327 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,328 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,328 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,329 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-694f93cb-34fe-4247-821f-f5ca506e1459-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:28,330 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,332 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,333 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,334 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,334 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,334 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-701ae0c0-29df-4b97-839c-9c83df4e1b68-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:28,336 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,338 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,339 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,340 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,340 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,340 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5b6171fb-919a-4c7b-98aa-5dfe9d454475-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:28,342 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,344 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,345 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,345 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,346 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,346 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d21bcc-601f-4e42-b182-081c470c659d-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:28,347 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,349 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,351 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,351 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,352 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,352 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-600d8f03-7aac-4957-baf4-c554ff43494a-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:28,353 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,355 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,357 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,357 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,358 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,358 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e02ee93e-e796-42b0-bf02-5f75dfdc1c6c-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:28,359 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,361 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,363 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,363 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,364 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,364 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a90b0f72-7d9a-4a49-a68b-f290f02aa529-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:28,365 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,367 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,369 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,370 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,371 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,371 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f41d2183-74d9-4df5-be81-a2fb23bf9de7-c000.snappy.parquet, range: 0-11392, partition values: [empty row]
2018-10-11 21:36:28,373 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,375 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,377 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,377 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,378 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,378 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3fa69d1c-3edf-45d2-b0ac-1272168f52c9-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:28,379 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,382 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,383 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,384 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,384 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,384 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a0b202fc-2bf1-495f-9764-acfb0429a5c8-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:28,386 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,388 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,390 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,390 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,391 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,391 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-936820e3-b496-4cc2-8571-9635f4fdbfa3-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:28,392 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,395 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,396 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,397 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,397 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,397 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-eb4c07ef-088b-4a15-be2f-b273fd27d040-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:28,399 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,401 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,403 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,403 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,404 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,404 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4d67cb5-e9dc-4b19-acea-905574bf8522-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:28,405 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,408 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,410 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,410 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,411 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,411 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-224b809f-76f3-4919-a9b8-43b820662112-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:28,412 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,414 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,416 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,417 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,417 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,417 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7501282c-59f0-4b1a-88f5-82366eca768c-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:28,419 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,421 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,423 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,423 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,423 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,424 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3cad3b64-e9c9-41b4-a422-dc665b78c2c1-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:28,425 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,427 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,429 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,430 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,430 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,430 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-390c15b5-d721-4535-be3f-12b05cfbe30a-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:28,432 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,434 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,435 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,436 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,436 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,437 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d28ee0-4831-43b1-a27f-dab415ede7a0-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:28,438 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,440 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,442 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,443 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,443 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,443 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-56056fe1-8392-42d6-b2c1-4463f7381740-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:28,444 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,447 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,448 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,449 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,449 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,450 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dd3999fe-68a8-43bd-b968-d1abe12b7d68-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:28,451 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,454 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,456 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,456 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,457 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,457 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-40f0422a-c31f-4b28-870e-1da30f3d4173-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:28,458 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,461 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,463 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,463 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,464 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,464 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-81102eef-ee91-4905-a219-3a7d6ad68001-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:28,465 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,468 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,470 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,470 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,471 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,471 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a7708ac5-ff0d-4d9b-b829-432325d537bd-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:28,473 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,475 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,477 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,478 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,478 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,479 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-de923178-e27e-43df-9376-a8765b42b212-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:28,480 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,483 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,485 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,486 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,487 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,487 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2366af80-dffe-477b-8217-7f704825188b-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:28,489 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,492 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,493 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,494 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,495 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,495 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad60e729-defb-4a4e-99d9-9ca74fcf6a2d-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:28,497 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,500 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,501 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,502 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,502 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,503 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0cf40412-95c9-405c-9299-90b8afa2a414-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,505 INFO [Executor task launch worker for task 67] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,507 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,509 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,509 INFO [Executor task launch worker for task 67] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,510 WARN [Executor task launch worker for task 67] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,512 INFO [Executor task launch worker for task 67] o.a.s.e.Executor [Logging.scala:54] Finished task 3.0 in stage 43.0 (TID 67). 1380 bytes result sent to driver
2018-10-11 21:36:28,512 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 4.0 in stage 43.0 (TID 68, localhost, executor driver, partition 4, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:28,513 INFO [Executor task launch worker for task 68] o.a.s.e.Executor [Logging.scala:54] Running task 4.0 in stage 43.0 (TID 68)
2018-10-11 21:36:28,513 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 3.0 in stage 43.0 (TID 67) in 216 ms on localhost (executor driver) (4/8)
2018-10-11 21:36:28,515 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8f84cc33-f1d5-478f-87ce-0663d825408c-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,516 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,519 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,520 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,521 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,521 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,521 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64602a48-9868-4fdb-bbce-e23fdb1ff4ad-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,523 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,525 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,527 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,528 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,528 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,528 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-22885ce6-f042-455f-87e9-76587998ab27-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,530 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,532 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,534 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,534 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,535 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,535 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4f0d15a7-be71-41c6-9bcc-c559eb6c8a2a-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,536 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,539 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,541 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,542 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,542 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,543 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3bf97aa1-dd22-42f0-8f17-b66b1cd270a5-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,544 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,546 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,548 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,549 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,549 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,550 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57c3815d-7209-4ed6-90cc-5597db2fc409-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:28,551 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,554 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,556 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,556 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,557 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,557 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fff88882-0587-48ef-95f1-5dd0508833d6-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,558 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,560 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,562 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,562 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,563 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,563 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0df7b64b-c870-4b00-b1dc-59fec5d6c208-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,564 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,566 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,568 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,568 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,569 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,569 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-384bf5ec-e9b8-429f-8281-82432437fb02-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,571 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,573 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,575 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,576 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,576 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,576 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f08e80af-add7-42b2-b4ad-2a22d1ce2a56-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,578 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,580 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,582 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,582 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,582 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,583 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-615e7057-357a-4616-a738-affe25bf8b3a-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,584 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,586 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,587 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,588 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,588 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,588 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-73bd7a91-071c-42d6-a80a-c13db79d72d2-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,590 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,592 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,594 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,594 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,595 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,595 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dbf5cfde-c2ae-4ddf-bdee-0e703dcab699-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,596 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,599 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,601 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,601 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,601 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,602 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-70719782-4616-4b3d-82dd-7bc33b67022f-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,603 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,606 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,608 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,609 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,609 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,609 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1e620604-bee6-486e-b171-860be8b8b75e-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,611 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,613 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,615 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,615 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,616 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,616 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05b626a4-35ba-4e5f-9c93-d7f62d4d3027-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,618 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,620 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,622 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,622 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,623 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,623 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0ac09a5-d2c0-4844-95ed-3706f6eb6930-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,625 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,627 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,629 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,629 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,630 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,630 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e6fcf0f4-d681-401e-b5b6-7075100f24ee-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,631 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,634 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,635 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,636 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,636 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,637 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-00ef3320-f515-47a4-b26f-5b5caf94f65d-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,638 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,640 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,642 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,642 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,643 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,643 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ff4df6d7-b381-401d-987b-233a02e4d909-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,645 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,647 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,649 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,649 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,650 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,650 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f5fe52bb-22d0-4408-b9d4-9b3a37c89088-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,651 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,654 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,656 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,656 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,656 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,657 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a3db9e49-1db1-4e94-a28e-f73df6d27fa8-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,658 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,660 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,662 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,662 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,663 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,663 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-66b4c136-f4b2-4428-9331-2adf6a097511-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:28,664 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,666 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,668 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,668 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,669 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,669 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e3141f79-7a0f-4730-8468-e54a6c54c533-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:28,670 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,672 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,674 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,674 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,675 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,675 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0b0c8187-cf2f-44d8-88fc-49ce4dd62874-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:28,676 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,678 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,680 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,680 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,680 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,681 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-08d62d99-4b8f-4e06-8620-bba67557d59d-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:28,682 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,684 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,685 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,686 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,686 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,687 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64403a21-b2b7-43f7-a22d-5f7706bad082-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:28,688 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,690 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,692 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,692 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,693 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,693 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c1e3a260-62f4-4aeb-8a26-82616f973030-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:28,694 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,696 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,698 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,698 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,699 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,699 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9bff7912-93ac-4fc7-a1db-f8e566a70655-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:28,700 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,702 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,704 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,704 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,705 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,705 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-47728647-7a80-4592-ba7c-c3ebcb1e5224-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:28,706 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,708 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,710 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,710 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,711 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,711 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9b3bccf5-ad25-404d-9310-88a915f9b7a2-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:28,712 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,714 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,716 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,716 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,717 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,717 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-510a5951-088e-4dab-8dd0-c02c47869d79-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:28,718 INFO [Executor task launch worker for task 68] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,720 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,722 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,722 INFO [Executor task launch worker for task 68] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,723 WARN [Executor task launch worker for task 68] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,725 INFO [Executor task launch worker for task 68] o.a.s.e.Executor [Logging.scala:54] Finished task 4.0 in stage 43.0 (TID 68). 1423 bytes result sent to driver
2018-10-11 21:36:28,726 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 5.0 in stage 43.0 (TID 69, localhost, executor driver, partition 5, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:28,726 INFO [Executor task launch worker for task 69] o.a.s.e.Executor [Logging.scala:54] Running task 5.0 in stage 43.0 (TID 69)
2018-10-11 21:36:28,726 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 4.0 in stage 43.0 (TID 68) in 214 ms on localhost (executor driver) (5/8)
2018-10-11 21:36:28,728 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6650061f-a0ab-42f3-be0d-e29bba951689-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:28,729 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,731 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,733 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,733 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,734 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,734 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a73de2ae-2a91-46fe-a61e-d06104068675-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:28,735 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,739 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,741 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,741 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,742 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,742 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-60348a41-e7a8-4c02-9dbf-a4848152bf0a-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:28,743 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,745 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,746 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,746 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,747 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,747 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca9d3388-a32e-42aa-9b9a-f2ba826d1b2e-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:28,748 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,753 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,754 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,755 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,755 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,755 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46ee92fd-a03b-4ad7-a847-fac273921f57-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:28,756 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,758 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,759 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,760 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,760 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,760 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4317e6e3-7c07-49f4-9bea-564768382089-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:28,761 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,768 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,772 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,772 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,773 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,773 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dc5fe75d-ae31-4bd0-a866-d3274b2d629b-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:28,774 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,776 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,777 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,778 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,778 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,778 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-49c1ab9d-a225-4ba8-911f-7c8138662881-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:28,780 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,782 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,783 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,783 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,784 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,784 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a910a7f7-d2c2-4fb9-9907-6f0a12ef5816-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,785 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,787 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,789 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,789 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,790 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,790 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af526506-da3a-461a-b7d6-7576a7ca2dc6-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,791 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,793 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,794 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,795 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,795 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,795 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f7492e12-3b8e-43e6-b84c-e42d363116f9-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,796 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,799 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,801 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,801 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,801 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,802 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-272bed24-7d51-4fc6-9a8a-aba3fb69c4b8-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,806 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,809 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,809 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,810 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,810 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,810 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ecac671-fc2e-4df4-9315-bf4c516db607-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,811 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,813 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,815 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,815 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,816 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,816 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f2c3b673-16d6-4469-a5ed-13de2cc92f10-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,817 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,819 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,821 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,821 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,821 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,822 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a6947b4-4ad6-4ab6-8907-a8320e284bcb-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:28,823 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,825 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,826 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,826 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,827 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,827 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3a0fc732-ce1b-4da0-9cd5-162ab4573e6e-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,828 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,830 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,832 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,832 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,832 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,833 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a087b5c1-75ef-4738-bcac-2ee86aa294bd-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,834 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,839 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,841 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,841 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,841 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,842 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e0eae261-dec5-4554-b126-6e1829f78050-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,843 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,845 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,846 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,847 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,847 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,848 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-987c95c1-23dd-407a-98e6-0e238fe1473b-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,850 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,852 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,854 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,855 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,855 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,855 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb60375b-4e46-4d45-99c6-540adc7208c3-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,857 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,859 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,861 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,862 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,862 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,862 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-319157ef-58f7-4406-b9b1-6baf5b53b0a8-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,863 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,866 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,867 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,868 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,868 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,868 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8022bee4-ba1f-4f2d-87cc-7ae422111479-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,870 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,872 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,874 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,874 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,874 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,875 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-861da466-0eee-47d7-9ff5-971c6d7f5a93-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,876 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,878 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,880 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,880 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,881 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,881 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8a3516de-8b82-44d1-82a8-69aa660cbbdf-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,882 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,885 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,886 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,887 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:28,887 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,887 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f419bf0e-cdd8-4f6c-8beb-f4dedb61a260-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,888 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,891 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,892 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,892 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,893 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,893 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e766d71-e245-4297-857b-bdd21b58b6f4-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,894 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,897 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,898 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,898 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,899 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,899 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4c33fd2a-9846-4f6b-a1f0-dccf06433485-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,900 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,902 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,903 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,904 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,904 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,904 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8fc6b69-477b-42e7-9d1a-3c499d4d10b2-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:28,905 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,907 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,909 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,909 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,910 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,910 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9039fff7-2957-4b0e-992e-e87565413ec8-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,911 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,913 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,915 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,915 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,916 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,916 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ef4e6a53-30c6-43c0-8eff-8a34e34edf6b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,917 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,919 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,921 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,921 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,922 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,922 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a398596-1df5-4ca7-911f-e74abd3762dd-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,924 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,926 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,928 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,928 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,929 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,929 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58f4dff5-7214-4777-ba17-784e2cef12f4-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,930 INFO [Executor task launch worker for task 69] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,933 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,935 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,935 INFO [Executor task launch worker for task 69] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,936 WARN [Executor task launch worker for task 69] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,941 INFO [Executor task launch worker for task 69] o.a.s.e.Executor [Logging.scala:54] Finished task 5.0 in stage 43.0 (TID 69). 1423 bytes result sent to driver
2018-10-11 21:36:28,941 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 6.0 in stage 43.0 (TID 70, localhost, executor driver, partition 6, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:28,942 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 5.0 in stage 43.0 (TID 69) in 216 ms on localhost (executor driver) (6/8)
2018-10-11 21:36:28,943 INFO [Executor task launch worker for task 70] o.a.s.e.Executor [Logging.scala:54] Running task 6.0 in stage 43.0 (TID 70)
2018-10-11 21:36:28,945 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2422c402-71d9-41d7-9987-1d4ab7fc19fb-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,946 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,949 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,951 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,951 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,952 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,954 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0ff4b192-b8e8-4b6c-9828-f5d9ec00cbce-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,957 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,959 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,969 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,969 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,970 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,970 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a2d93e67-4023-47f3-9d4a-f50a9c8541fe-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,977 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:28,979 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:28,981 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:28,981 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:28,982 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:28,996 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a29231b4-e038-4ee0-88ef-1d68cfe9d2be-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:28,998 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,000 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,002 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,002 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,003 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,003 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-55b99cce-2e82-46d0-8e53-bee4918b429e-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,030 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,032 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,034 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,035 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,035 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,035 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9dc2a350-c1b5-4c5c-8f81-b54fcb2ffa3a-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,037 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,038 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,039 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,039 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,040 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,040 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1be87a1d-cfca-46a7-b3c5-5377826665a1-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,041 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,043 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,044 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,045 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,045 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,045 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ac9002e-c497-481d-b337-445644058c65-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,047 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,063 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,071 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,072 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,072 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,072 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7453f0e9-fcdc-45d9-aa4e-04e2742b8b2b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,074 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,076 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,077 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,097 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 19 ms. row count = 1
2018-10-11 21:36:29,098 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,098 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3ce2c6c2-8244-4825-9d79-b2816de2acda-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,100 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,102 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,104 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,104 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,105 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,105 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e32c7dc-dcea-456e-a878-c9c835248ea9-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,107 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,109 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,114 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,115 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,116 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,116 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b2dcecd1-372e-4f0b-9562-f96ebf3af8f6-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,117 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,120 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,121 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,122 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,122 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,123 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b266ce1f-aa2d-4c98-8bd7-38ee65369d89-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,137 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,140 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,142 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,142 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,143 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,143 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6c58af95-e7c8-4556-989d-8a8fc881fc54-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:29,145 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,147 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,149 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,149 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,150 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,163 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-da19fb6b-0851-4b26-8454-89e7d9d29409-c000.snappy.parquet, range: 0-11359, partition values: [empty row]
2018-10-11 21:36:29,165 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,167 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,169 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,169 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,170 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,170 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2646c07b-af07-4e07-8e28-e3b57bbb514c-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,172 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,175 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,176 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,177 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,177 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,177 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7fed2b39-6f8a-4f9e-8921-caa7441424ec-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,179 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,181 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,183 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,183 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,184 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,184 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f4b5fa6b-5fa7-4ccf-b1ac-7fe087ca4972-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,186 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,188 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,190 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,190 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,191 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,191 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e8617665-882c-4cd9-af6c-d70cf9357935-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,193 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,195 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,197 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,197 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,198 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,198 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9c42e760-f3d5-4337-93cc-2faeb5608d5f-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,200 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,202 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,204 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,204 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,205 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,205 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-992bc073-6936-47bb-9ce0-4bd6ff5312f3-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,207 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,209 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,211 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,211 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,212 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,212 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bdc96751-af2e-4c0d-aad9-9b6309410e26-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,213 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,216 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,217 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,218 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,218 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,218 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2a8062ca-53c1-4efb-887d-a5f15b2e2675-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:29,220 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,222 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,224 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,225 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,225 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,226 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-461919ce-4e71-49bc-ab59-c7e999f082df-c000.snappy.parquet, range: 0-11357, partition values: [empty row]
2018-10-11 21:36:29,227 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,229 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,231 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,232 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,232 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,233 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d184ff46-c4c0-4adb-993f-d1625bf18c93-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:29,234 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,237 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,238 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,239 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,239 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,240 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a542cfa-fb0f-420c-808c-c23861bc5e49-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:29,241 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,244 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,245 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,246 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,246 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,247 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3744292d-429a-4b99-8f87-0174a77a5f02-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:29,248 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,251 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,253 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,254 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,255 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,255 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ce13747d-84bc-4b83-bcbf-ed87cb408c63-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:29,257 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,260 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,261 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,262 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,263 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,263 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa3ef260-acd7-4ea3-b2bb-08ce39dc72a4-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:29,264 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,267 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,268 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,269 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,269 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,270 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09afdad3-a22e-46d0-8913-a98493735de8-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:29,271 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,304 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,304 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1399
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1395
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1386
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1404
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1396
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1384
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1390
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1403
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1388
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1361
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1409
2018-10-11 21:36:29,305 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1357
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1410
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1387
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1405
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1379
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1378
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1353
2018-10-11 21:36:29,306 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1368
2018-10-11 21:36:29,306 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,306 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,307 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,307 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4de0605-1810-4659-8565-775942a08750-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:29,308 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_63_piece0 on 192.168.0.206:34485 in memory (size: 6.7 KB, free: 1391.3 MB)
2018-10-11 21:36:29,308 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1352
2018-10-11 21:36:29,308 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1365
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1363
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1383
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1364
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1370
2018-10-11 21:36:29,309 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1389
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1397
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1408
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1374
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1355
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1359
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1380
2018-10-11 21:36:29,309 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1385
2018-10-11 21:36:29,310 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1376
2018-10-11 21:36:29,310 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1358
2018-10-11 21:36:29,310 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1372
2018-10-11 21:36:29,310 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1362
2018-10-11 21:36:29,310 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1413
2018-10-11 21:36:29,310 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1398
2018-10-11 21:36:29,311 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_64_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:29,311 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1382
2018-10-11 21:36:29,312 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,312 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1369
2018-10-11 21:36:29,313 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_62_piece0 on 192.168.0.206:34485 in memory (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1392
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1354
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1375
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1402
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1381
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1412
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1401
2018-10-11 21:36:29,314 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1371
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1391
2018-10-11 21:36:29,314 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1366
2018-10-11 21:36:29,315 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_65_piece0 on 192.168.0.206:34485 in memory (size: 10.5 KB, free: 1391.4 MB)
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1400
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1367
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1394
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1406
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1373
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1411
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1377
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1356
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1360
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1393
2018-10-11 21:36:29,316 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1407
2018-10-11 21:36:29,317 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,317 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,317 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c9cfad0a-8f5c-4599-8a7c-0d13b6068b66-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:29,318 INFO [Executor task launch worker for task 70] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,321 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,322 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,323 INFO [Executor task launch worker for task 70] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,323 WARN [Executor task launch worker for task 70] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,324 INFO [Executor task launch worker for task 70] o.a.s.e.Executor [Logging.scala:54] Finished task 6.0 in stage 43.0 (TID 70). 2638 bytes result sent to driver
2018-10-11 21:36:29,324 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 7.0 in stage 43.0 (TID 71, localhost, executor driver, partition 7, PROCESS_LOCAL, 10904 bytes)
2018-10-11 21:36:29,325 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 6.0 in stage 43.0 (TID 70) in 384 ms on localhost (executor driver) (7/8)
2018-10-11 21:36:29,325 INFO [Executor task launch worker for task 71] o.a.s.e.Executor [Logging.scala:54] Running task 7.0 in stage 43.0 (TID 71)
2018-10-11 21:36:29,327 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aa132c9-1c2c-4bed-8054-4f01f905b173-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:29,328 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,330 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,331 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,331 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,332 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,332 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1f52f9b8-ea21-4d2b-a7d2-11744187f778-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:29,333 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,335 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,336 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,337 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,337 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,337 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-51f29362-71b4-40bb-9ac9-c8fd50e8b54c-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,338 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,340 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,341 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,342 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,342 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,342 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca8ad858-bbe8-4c3b-a0f5-ea89627075a3-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,343 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,347 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,348 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,349 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,349 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,349 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-000c5114-25aa-43b6-ae19-ab0069bbeadf-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,355 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,356 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,357 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,358 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,358 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,358 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af314aee-5298-45df-899e-f5cf50b26e6a-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,359 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,361 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,375 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,375 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,375 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,376 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-06648b17-d906-45c9-b987-0c863fb63c7b-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,379 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,381 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,382 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,383 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,383 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,383 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-deaed750-dc34-4faa-bfea-c7b9e6d08139-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,385 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,391 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,393 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,393 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,393 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,394 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-edc1f6e1-52af-4b5a-9623-413fdd0995f1-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:29,395 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,405 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,406 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,406 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,407 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,410 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46686477-1f45-40b8-b264-443e98dbf08f-c000.snappy.parquet, range: 0-11350, partition values: [empty row]
2018-10-11 21:36:29,411 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,413 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,415 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,415 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,415 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,415 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d8e5bc47-ef5e-484a-98b7-34e73c59faf3-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:29,422 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,424 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,426 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,426 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,427 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,427 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-217b388a-5636-42f5-a2f7-f7e650de6c64-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:29,428 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,430 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,431 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,431 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,432 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,432 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-824fe1c8-dff2-491e-86b8-2e2542684692-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:29,433 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,435 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,436 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,437 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,437 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,437 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f812e08f-6f88-4c2d-96ec-83fdeb1e434a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:29,439 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,441 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,442 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,442 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,443 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,443 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9020ecd0-b01d-408c-baa5-a3b077d92f4a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:29,444 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,446 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,447 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,447 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,448 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,448 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5dccade-9231-4d83-bfc0-eeddbd723979-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:29,449 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,454 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,456 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,456 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,457 WARN [Executor task launch worker for task 71] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,457 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ce47add-8d58-4bb1-b893-375d2709dd55-c000.snappy.parquet, range: 0-882, partition values: [empty row]
2018-10-11 21:36:29,458 INFO [Executor task launch worker for task 71] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,460 INFO [Executor task launch worker for task 71] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:29,462 INFO [Executor task launch worker for task 71] o.a.s.e.Executor [Logging.scala:54] Finished task 7.0 in stage 43.0 (TID 71). 4318 bytes result sent to driver
2018-10-11 21:36:29,463 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 7.0 in stage 43.0 (TID 71) in 139 ms on localhost (executor driver) (8/8)
2018-10-11 21:36:29,463 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 43.0, whose tasks have all completed, from pool 
2018-10-11 21:36:29,464 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 43 (collect at SparkDataManager.scala:77) finished in 1.606 s
2018-10-11 21:36:29,464 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 32 finished: collect at SparkDataManager.scala:77, took 1.608698 s
2018-10-11 21:36:29,466 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,466 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,467 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,467 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,467 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,468 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,468 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,468 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,469 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,469 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,469 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,469 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,470 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,470 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,470 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,471 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,471 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,471 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,471 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,472 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,472 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:29,473 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:29,485 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:29,487 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:29,503 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:29,504 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:29,547 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:29,548 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:29,548 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<statement: struct<id: string, text: string>, startTime: bigint, endTime: bigint, description: string, details: string ... 1 more field>
2018-10-11 21:36:29,548 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:29,557 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_68 stored as values in memory (estimated size 228.2 KB, free 1390.7 MB)
2018-10-11 21:36:29,564 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_68_piece0 stored as bytes in memory (estimated size 21.7 KB, free 1390.6 MB)
2018-10-11 21:36:29,564 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_68_piece0 in memory on 192.168.0.206:34485 (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:29,565 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 68 from collect at SparkDataManager.scala:77
2018-10-11 21:36:29,566 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:29,578 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:77
2018-10-11 21:36:29,579 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 33 (collect at SparkDataManager.scala:77) with 8 output partitions
2018-10-11 21:36:29,579 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 44 (collect at SparkDataManager.scala:77)
2018-10-11 21:36:29,579 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:29,580 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:29,580 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 44 (MapPartitionsRDD[141] at collect at SparkDataManager.scala:77), which has no missing parents
2018-10-11 21:36:29,582 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_69 stored as values in memory (estimated size 16.2 KB, free 1390.6 MB)
2018-10-11 21:36:29,583 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_69_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1390.6 MB)
2018-10-11 21:36:29,583 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_69_piece0 in memory on 192.168.0.206:34485 (size: 6.7 KB, free: 1391.3 MB)
2018-10-11 21:36:29,584 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 69 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:29,584 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 8 missing tasks from ResultStage 44 (MapPartitionsRDD[141] at collect at SparkDataManager.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-10-11 21:36:29,584 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 44.0 with 8 tasks
2018-10-11 21:36:29,586 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 44.0 (TID 72, localhost, executor driver, partition 0, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:29,586 INFO [Executor task launch worker for task 72] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 44.0 (TID 72)
2018-10-11 21:36:29,588 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e31abc9-a228-4133-b02e-0f6cd7638510-c000.snappy.parquet, range: 0-13723, partition values: [empty row]
2018-10-11 21:36:29,590 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,592 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,593 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,594 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,594 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,594 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b1982672-2d41-4bff-8f45-9e6f8c921d5d-c000.snappy.parquet, range: 0-13722, partition values: [empty row]
2018-10-11 21:36:29,596 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,598 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,600 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,600 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,601 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,601 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e5316a0-c580-49fd-91d0-550955cd9bbc-c000.snappy.parquet, range: 0-13721, partition values: [empty row]
2018-10-11 21:36:29,607 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,610 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,611 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,612 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,612 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,612 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e15f7475-849c-45f2-9dcf-5bb2d5b69bd4-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:29,613 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,615 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,617 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,617 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,618 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,618 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a46c4b1e-6e4f-4382-a659-662347269774-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:29,619 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,621 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,623 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,623 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,623 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,624 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c7514033-0170-45b0-ad15-cb3097418dfb-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:29,625 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,627 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,629 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,629 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,629 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,630 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3723b14f-e12a-4f5a-b016-809b54de139c-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:29,631 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,633 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,635 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,635 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,636 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,636 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-07c85578-afbb-490a-bd9d-3837ab2a7baa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:29,637 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,639 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,640 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,641 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,641 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,642 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-603f4016-bd22-4ed6-b2c7-be69003d48fa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:29,643 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,645 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,646 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,647 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,647 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,647 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a6043ae9-7f4d-4793-ad20-a149ce2339bb-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:29,649 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,651 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,652 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,652 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,653 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,653 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8d65fb3-afa1-45c2-9354-709d30d7ae22-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:29,655 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,657 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,659 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,659 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,659 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,660 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0e52f33e-2792-451e-bfca-4fb2393d1315-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:29,661 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,663 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,664 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,665 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,665 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,665 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e001f918-e226-4b11-a792-e057310c71dc-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:29,667 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,668 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,669 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,670 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,670 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,670 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a17d8d1-3cec-4462-90f4-c0c77d76c5ce-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:29,671 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,674 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,675 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,675 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,676 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,676 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7992780f-0fa1-4c14-beca-90350009f018-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:29,678 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,680 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,681 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,681 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,682 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,682 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d57f919b-2b39-4c28-b8f3-61f31041cb83-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:29,683 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,686 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,687 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,688 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,688 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,689 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-735e4a0c-71b2-4dfe-9652-eddb95823bd0-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:29,690 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,693 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,694 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,694 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,695 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,695 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e132893-66ca-44f4-810d-b31bd307f3bf-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:29,696 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,698 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,699 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,700 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,700 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,701 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e32d4e0b-152b-41f5-9d5d-6319207c046a-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:29,702 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,703 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,704 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,705 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,705 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,705 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d24eaa68-02d1-43c1-b2da-ef94148610ed-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:29,706 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,708 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,709 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,709 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,709 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,710 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad3d0b86-19cf-474c-b3d4-796d7afaa4ae-c000.snappy.parquet, range: 0-13699, partition values: [empty row]
2018-10-11 21:36:29,711 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,712 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,713 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,713 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,714 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,714 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0247bc41-a2af-4b30-a5e0-5bc327f47cc2-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:29,715 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,717 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,717 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,718 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,718 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,718 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-26318166-52ec-4313-a4e5-3dc130f82d18-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:29,719 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,720 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,722 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,722 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,722 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,722 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2227df14-fd69-4e52-88fc-ffa906e8e96a-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:29,724 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,725 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,727 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,727 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,727 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,727 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-16e0e1c7-4724-40d4-8fe9-64c573377aac-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:29,728 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,730 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,731 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,731 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,732 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,732 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-12cc2ef7-5c98-45c7-bfca-9f6852b4f9cb-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:29,733 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,735 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,736 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,737 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,737 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,737 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5704978-3346-4cf0-a83a-fd67411e60c0-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:29,739 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,741 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,742 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,743 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,743 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,743 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-611aa927-9eff-4acc-a956-1419efe00229-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:29,744 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,746 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,747 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,748 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,748 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,748 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b6173355-0a8b-414c-9ed0-27985f88914d-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:29,749 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,751 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,753 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,753 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,753 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,754 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb54fd41-c4a7-41e8-a051-2493b4fc4fed-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:29,755 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,757 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,758 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,758 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,759 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,759 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9af47b28-32d6-4d91-b198-acee5bfe01dd-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:29,760 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,762 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,763 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,764 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,764 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,764 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-db7aecb7-d25c-4a59-ad0e-2bacf5772cb8-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,766 INFO [Executor task launch worker for task 72] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,767 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,769 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,769 INFO [Executor task launch worker for task 72] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,769 WARN [Executor task launch worker for task 72] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,770 INFO [Executor task launch worker for task 72] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 44.0 (TID 72). 1423 bytes result sent to driver
2018-10-11 21:36:29,771 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 1.0 in stage 44.0 (TID 73, localhost, executor driver, partition 1, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:29,771 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 44.0 (TID 72) in 186 ms on localhost (executor driver) (1/8)
2018-10-11 21:36:29,771 INFO [Executor task launch worker for task 73] o.a.s.e.Executor [Logging.scala:54] Running task 1.0 in stage 44.0 (TID 73)
2018-10-11 21:36:29,773 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ae6870f1-1f08-443f-8701-2a8a8fc509da-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,774 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,776 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,777 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,777 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,778 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,778 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d164cc8-eee4-4f35-920d-9fa0984dbb40-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,779 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,781 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,782 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,782 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,782 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,783 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b48d131c-6bd6-478f-9abc-96bd891b0584-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,784 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,785 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,787 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,787 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,787 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,788 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa252a4c-1080-4e94-9dc5-d3e35b8e70ea-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,789 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,790 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,792 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,792 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,792 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,793 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-586baf62-c0dd-443b-a8db-5822599f893a-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,794 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,795 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,797 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,797 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,797 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,798 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5ae7e692-d21c-4b25-9cdf-83f59ef40407-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,799 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,801 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,802 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,802 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,803 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,803 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5d04bbb7-c918-4d38-9348-7833d532fb6c-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,804 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,806 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,808 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,808 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,808 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,808 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-178dd25a-2181-4b65-970b-0683491eca83-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,809 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,810 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,811 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,811 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,812 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,812 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-90d7046c-97fc-4066-9720-fc9230ba0c54-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:29,813 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,815 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,816 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,816 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,816 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,816 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2bc1210e-eaa6-47cf-ba41-7e0b223d190f-c000.snappy.parquet, range: 0-13695, partition values: [empty row]
2018-10-11 21:36:29,817 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,819 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,820 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,820 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,821 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,821 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e7cb913a-50de-4609-aada-ae4eafd1de31-c000.snappy.parquet, range: 0-13694, partition values: [empty row]
2018-10-11 21:36:29,822 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,823 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,825 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,825 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,825 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,826 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f1d46819-9d92-46ba-9025-2adb0818719c-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:29,827 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,828 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,830 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,830 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,830 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,830 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b85e87fe-5382-4ea3-895f-13f9f032ef4b-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:29,832 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,833 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,835 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,835 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,835 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,836 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3831987b-b4b7-4617-a261-5bf721ece358-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:29,837 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,839 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,840 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,840 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,840 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,840 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-21e4180a-5c27-419b-ab1c-78aa6b2cb324-c000.snappy.parquet, range: 0-13691, partition values: [empty row]
2018-10-11 21:36:29,842 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,844 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,845 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,845 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,846 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,846 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c33bde23-fc3e-4548-a230-3eb78aac57fb-c000.snappy.parquet, range: 0-13690, partition values: [empty row]
2018-10-11 21:36:29,847 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,849 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,850 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,851 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,851 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,851 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0232792-2703-4cd0-a57b-e87871b66a88-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,852 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,855 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,856 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,856 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,856 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,856 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6f55e7b2-ed9f-4a3f-9a54-06887558e22d-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,857 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,858 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,859 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,860 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,860 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,860 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4384b7b2-8b67-4077-a182-89d2e37a4833-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,861 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,862 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,863 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,863 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,864 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,864 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19b79ab5-3105-4028-bbd0-c35b902de4a4-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,865 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,867 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,868 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,868 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,868 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,869 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6d920ff1-e3fc-40ac-a84e-386f13c16e67-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,870 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,871 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,872 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,873 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,873 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,873 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d63edb8d-fac0-4840-b019-b0b6e7a639d6-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,874 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,876 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,878 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,878 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,878 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,879 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09613236-c7ea-4e03-a2f6-c2dd85f36bed-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,880 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,881 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,883 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,883 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,884 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,884 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aafd0534-3203-4405-a282-194c9a55812b-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,885 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,887 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,889 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,889 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,890 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,890 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-018f2c55-3be6-41b0-b9cd-c3e9619e0cda-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,892 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,893 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,895 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,895 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,896 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,896 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b9578903-dcf4-4d51-9126-5a90dd27a1ef-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:29,897 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,899 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,901 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,901 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,901 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,902 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9f7a9a62-4bad-4d7b-8940-549d39238770-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:29,903 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,905 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,906 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,907 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,907 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,907 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8b0e504c-c3cf-496a-8730-d46b0b7df1b9-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:29,909 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,911 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,912 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,913 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,913 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,913 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-79b8fe3f-ef98-4c64-bedc-86c3fb9283fd-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:29,914 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,916 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,917 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,918 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,918 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,918 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57dddecf-aa33-4874-99e6-9748f780fe78-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:29,919 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,921 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,923 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,923 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,924 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,924 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7bfc2fc2-2b64-4ee4-83ae-ad0b645e1322-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:29,925 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,927 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,929 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,929 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,929 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,930 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a928c40-d93a-4600-a43e-d6259a8a7c26-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:29,931 INFO [Executor task launch worker for task 73] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,933 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,934 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,935 INFO [Executor task launch worker for task 73] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,935 WARN [Executor task launch worker for task 73] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,936 INFO [Executor task launch worker for task 73] o.a.s.e.Executor [Logging.scala:54] Finished task 1.0 in stage 44.0 (TID 73). 1423 bytes result sent to driver
2018-10-11 21:36:29,937 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 2.0 in stage 44.0 (TID 74, localhost, executor driver, partition 2, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:29,938 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 1.0 in stage 44.0 (TID 73) in 167 ms on localhost (executor driver) (2/8)
2018-10-11 21:36:29,939 INFO [Executor task launch worker for task 74] o.a.s.e.Executor [Logging.scala:54] Running task 2.0 in stage 44.0 (TID 74)
2018-10-11 21:36:29,940 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-62170ceb-3786-48a8-93d4-b567247e8037-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:29,942 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,944 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,945 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,945 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,946 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,946 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8643ee33-f4b3-4b9d-a32e-39e43cfe4ba7-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:29,947 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,948 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,949 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,949 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,950 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,950 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0680d8e3-d6a3-46d1-b2d8-76df28d3b02a-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:29,951 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,953 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,954 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,954 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,954 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,954 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aae2da02-ce8a-4dad-87c4-6a12978949be-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:29,955 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,957 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,958 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,958 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,958 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,958 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8eae07df-593a-43cc-ac4a-2be464a53d3f-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:29,959 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,961 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,961 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,962 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,962 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,962 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a663e89f-b19d-4f56-bfd7-d844367b5dc2-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:29,963 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,965 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,966 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,966 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,966 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,966 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2318bf98-30f1-4cb7-89ff-f54e8f26ee77-c000.snappy.parquet, range: 0-13670, partition values: [empty row]
2018-10-11 21:36:29,967 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,969 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,970 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,971 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,971 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,971 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-854fe582-8eec-43ce-b79a-f5a9b836d2fd-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:29,973 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,974 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,975 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,976 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,976 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,976 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5793a580-0e06-4393-afec-1a4eefbbd626-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:29,977 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,979 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,980 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,980 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,980 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,981 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e7b564a-5bd2-4701-bdea-b107946c9b0c-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:29,982 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,983 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,984 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,984 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,985 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,985 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d10c8980-7d22-4a6d-99ec-053805bdf9ec-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:29,986 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,987 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,988 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,989 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:29,989 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,989 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-59858d75-788e-4333-9577-bd4947258eff-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:29,990 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,991 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,992 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,993 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,993 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,993 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-274749d2-43f1-4c5d-81fe-d888dc9dff47-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:29,994 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:29,996 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:29,998 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:29,998 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:29,999 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:29,999 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ddde392-a677-4c60-92fc-49cd2be2c0f1-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:30,000 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,002 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,004 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,005 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,005 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,006 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b273f389-8ffb-4cbf-b108-6506095ae632-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:30,008 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,010 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,011 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,011 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,012 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,012 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7233e2d1-9862-45dc-9824-4e41f2c2867e-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:30,013 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,014 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,016 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,016 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,016 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,016 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-86a299de-f8fc-46fe-86a9-60a4b26a9b38-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:30,018 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,019 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,021 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,021 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,021 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,022 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-defaedb6-dcf3-4709-916c-a3e955107f58-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:30,023 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,025 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,026 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,027 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,027 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,027 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a837d014-9d5d-454e-a7f6-8f2fb06c37d0-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:30,028 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,030 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,031 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,032 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,032 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,032 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-45a99307-69fd-4a6c-8a0c-91d96b83817a-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:30,034 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,036 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,037 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,037 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,038 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,038 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-29b6c682-0e21-4836-904c-33bbd018067c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:30,039 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,041 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,042 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,043 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,043 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,044 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bbb741a3-045d-451c-a803-96b4d0493b04-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:30,045 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,047 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,048 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,049 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,049 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,050 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e33d9f4-1cd5-43f7-b792-6d744d0a7505-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:30,051 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,053 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,055 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,055 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,056 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,056 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-71b8e55a-40c5-4b5c-9dae-af4d5551ef2c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:30,058 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,060 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,061 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,061 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,062 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,062 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e2d6c34-9f3a-4d0c-9b3f-d0afa5e7e097-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,063 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,065 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,066 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,067 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,067 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,068 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aadda0f-0463-4bfe-9f32-62f122ef3ff4-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,069 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,071 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,072 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,072 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,072 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,073 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bea86d33-775a-4e73-97cd-927d76b9352f-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,074 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,076 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,077 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,077 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,078 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,078 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e0852e5-e63c-45eb-912a-d3964270da43-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,079 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,081 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,082 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,083 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,083 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,083 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4cc621ee-7546-4d9f-8ba0-c4c66f0adeec-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,085 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,087 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,088 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,089 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,089 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,090 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58131256-2685-47d1-abb5-a4357c095a67-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,091 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,093 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,094 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,094 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,094 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,094 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a5a52bbe-0c0c-41d6-a8be-9839d1629b37-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:30,095 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,097 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,097 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,098 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,098 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,098 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-869a108e-1223-444d-9a58-a852afc71fdc-c000.snappy.parquet, range: 0-13658, partition values: [empty row]
2018-10-11 21:36:30,099 INFO [Executor task launch worker for task 74] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,101 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,102 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,102 INFO [Executor task launch worker for task 74] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,103 WARN [Executor task launch worker for task 74] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,104 INFO [Executor task launch worker for task 74] o.a.s.e.Executor [Logging.scala:54] Finished task 2.0 in stage 44.0 (TID 74). 4466 bytes result sent to driver
2018-10-11 21:36:30,104 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 3.0 in stage 44.0 (TID 75, localhost, executor driver, partition 3, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:30,104 INFO [Executor task launch worker for task 75] o.a.s.e.Executor [Logging.scala:54] Running task 3.0 in stage 44.0 (TID 75)
2018-10-11 21:36:30,104 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 2.0 in stage 44.0 (TID 74) in 168 ms on localhost (executor driver) (3/8)
2018-10-11 21:36:30,106 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-666a2e39-edd2-4992-85ee-cd58cc019c85-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:30,107 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,108 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,109 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,109 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,110 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,110 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca11e9c0-3060-4a86-91dd-db9ce2ca7e5f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:30,111 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,112 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,113 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,113 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,113 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,113 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4a6558fd-1e62-4e07-9df1-400871fc5cec-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:30,114 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,116 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,117 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,117 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,117 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,118 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19122b21-1ceb-4cf4-8593-9fa7c8624d2d-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:30,119 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,120 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,121 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,121 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,122 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,122 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d7ef5b02-cddb-495a-aad3-f3679ca76b8f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:30,123 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,124 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,125 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,126 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,126 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,126 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-694f93cb-34fe-4247-821f-f5ca506e1459-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:30,127 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,129 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,129 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,130 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,130 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,130 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-701ae0c0-29df-4b97-839c-9c83df4e1b68-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:30,131 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,133 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,134 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,134 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,134 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,135 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5b6171fb-919a-4c7b-98aa-5dfe9d454475-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:30,136 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,138 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,139 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,140 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,140 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,140 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d21bcc-601f-4e42-b182-081c470c659d-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:30,141 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,142 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,143 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,143 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,144 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,144 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-600d8f03-7aac-4957-baf4-c554ff43494a-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:30,145 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,147 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,148 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,148 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,149 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,149 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e02ee93e-e796-42b0-bf02-5f75dfdc1c6c-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:30,150 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,152 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,153 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,153 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,154 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,154 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a90b0f72-7d9a-4a49-a68b-f290f02aa529-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:30,156 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,157 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,159 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,159 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,160 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,160 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f41d2183-74d9-4df5-be81-a2fb23bf9de7-c000.snappy.parquet, range: 0-11392, partition values: [empty row]
2018-10-11 21:36:30,161 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,163 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,165 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,165 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,166 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,166 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3fa69d1c-3edf-45d2-b0ac-1272168f52c9-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:30,167 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,169 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,170 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,170 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,170 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,171 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a0b202fc-2bf1-495f-9764-acfb0429a5c8-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:30,172 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,174 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,175 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,175 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,175 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,176 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-936820e3-b496-4cc2-8571-9635f4fdbfa3-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:30,177 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,178 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,179 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,179 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,179 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,180 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-eb4c07ef-088b-4a15-be2f-b273fd27d040-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:30,181 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,182 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,183 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,183 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,183 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,183 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4d67cb5-e9dc-4b19-acea-905574bf8522-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:30,184 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,186 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,187 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,187 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,187 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,188 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-224b809f-76f3-4919-a9b8-43b820662112-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:30,189 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,190 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,191 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,192 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,192 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,192 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7501282c-59f0-4b1a-88f5-82366eca768c-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:30,194 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,195 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,196 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,197 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,197 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,197 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3cad3b64-e9c9-41b4-a422-dc665b78c2c1-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:30,198 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,199 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,200 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,200 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,201 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,201 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-390c15b5-d721-4535-be3f-12b05cfbe30a-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:30,202 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,204 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,205 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,205 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,206 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,206 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d28ee0-4831-43b1-a27f-dab415ede7a0-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:30,207 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,209 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,210 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,210 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,211 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,211 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-56056fe1-8392-42d6-b2c1-4463f7381740-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:30,212 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,213 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,214 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,214 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,215 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,215 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dd3999fe-68a8-43bd-b968-d1abe12b7d68-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:30,216 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,217 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,218 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,218 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,218 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,218 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-40f0422a-c31f-4b28-870e-1da30f3d4173-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:30,219 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,221 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,221 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,222 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,222 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,222 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-81102eef-ee91-4905-a219-3a7d6ad68001-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:30,223 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,224 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,225 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,225 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,225 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,225 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a7708ac5-ff0d-4d9b-b829-432325d537bd-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:30,226 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,227 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,228 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,228 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,229 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,229 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-de923178-e27e-43df-9376-a8765b42b212-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:30,230 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,231 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,232 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,232 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,232 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,232 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2366af80-dffe-477b-8217-7f704825188b-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:30,233 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,234 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,235 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,235 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,235 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,236 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad60e729-defb-4a4e-99d9-9ca74fcf6a2d-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:30,236 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,238 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,239 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,239 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,239 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,239 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0cf40412-95c9-405c-9299-90b8afa2a414-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,240 INFO [Executor task launch worker for task 75] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,242 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,243 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,244 INFO [Executor task launch worker for task 75] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,244 WARN [Executor task launch worker for task 75] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,245 INFO [Executor task launch worker for task 75] o.a.s.e.Executor [Logging.scala:54] Finished task 3.0 in stage 44.0 (TID 75). 2858 bytes result sent to driver
2018-10-11 21:36:30,245 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 4.0 in stage 44.0 (TID 76, localhost, executor driver, partition 4, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:30,246 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 3.0 in stage 44.0 (TID 75) in 142 ms on localhost (executor driver) (4/8)
2018-10-11 21:36:30,249 INFO [Executor task launch worker for task 76] o.a.s.e.Executor [Logging.scala:54] Running task 4.0 in stage 44.0 (TID 76)
2018-10-11 21:36:30,251 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8f84cc33-f1d5-478f-87ce-0663d825408c-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,252 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,253 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,254 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,255 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,255 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,255 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64602a48-9868-4fdb-bbce-e23fdb1ff4ad-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,257 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,258 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,259 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,260 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,260 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,260 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-22885ce6-f042-455f-87e9-76587998ab27-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,261 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,263 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,264 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,265 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,265 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,265 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4f0d15a7-be71-41c6-9bcc-c559eb6c8a2a-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,266 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,268 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,270 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,270 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,271 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,271 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3bf97aa1-dd22-42f0-8f17-b66b1cd270a5-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,272 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,273 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,274 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,275 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,275 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,275 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57c3815d-7209-4ed6-90cc-5597db2fc409-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:30,276 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,277 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,278 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,278 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,279 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,279 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fff88882-0587-48ef-95f1-5dd0508833d6-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,279 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,281 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,281 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,281 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,282 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,282 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0df7b64b-c870-4b00-b1dc-59fec5d6c208-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,283 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,284 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,285 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,286 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,287 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,287 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-384bf5ec-e9b8-429f-8281-82432437fb02-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,289 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,291 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,292 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,292 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,293 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,293 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f08e80af-add7-42b2-b4ad-2a22d1ce2a56-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,294 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,296 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,297 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,298 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,298 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,298 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-615e7057-357a-4616-a738-affe25bf8b3a-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,299 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,301 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,303 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,304 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,304 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,304 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-73bd7a91-071c-42d6-a80a-c13db79d72d2-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,305 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,308 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,309 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,310 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,310 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,310 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dbf5cfde-c2ae-4ddf-bdee-0e703dcab699-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,311 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,313 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,314 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,315 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,315 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,315 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-70719782-4616-4b3d-82dd-7bc33b67022f-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,317 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,318 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,320 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,320 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,320 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,320 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1e620604-bee6-486e-b171-860be8b8b75e-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,321 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,323 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,324 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,324 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,325 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,325 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05b626a4-35ba-4e5f-9c93-d7f62d4d3027-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,326 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,327 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,328 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,328 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,328 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,328 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0ac09a5-d2c0-4844-95ed-3706f6eb6930-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,329 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,331 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,332 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,332 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,332 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,332 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e6fcf0f4-d681-401e-b5b6-7075100f24ee-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,333 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,334 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,335 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,335 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,335 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,336 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-00ef3320-f515-47a4-b26f-5b5caf94f65d-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,337 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,339 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,340 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,340 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,341 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,341 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ff4df6d7-b381-401d-987b-233a02e4d909-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,342 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,344 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,345 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,345 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,345 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,345 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f5fe52bb-22d0-4408-b9d4-9b3a37c89088-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,346 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,348 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,349 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,350 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,350 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,350 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a3db9e49-1db1-4e94-a28e-f73df6d27fa8-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,351 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,353 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,354 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,354 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,355 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,355 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-66b4c136-f4b2-4428-9331-2adf6a097511-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:30,356 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,357 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,358 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,359 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,359 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,359 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e3141f79-7a0f-4730-8468-e54a6c54c533-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:30,360 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,362 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,363 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,364 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,364 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,364 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0b0c8187-cf2f-44d8-88fc-49ce4dd62874-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:30,365 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,367 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,368 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,368 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,369 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,369 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-08d62d99-4b8f-4e06-8620-bba67557d59d-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:30,370 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,372 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,373 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,373 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,373 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,373 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64403a21-b2b7-43f7-a22d-5f7706bad082-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:30,374 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,376 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,376 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,377 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,377 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,377 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c1e3a260-62f4-4aeb-8a26-82616f973030-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:30,378 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,379 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,380 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,380 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,380 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,380 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9bff7912-93ac-4fc7-a1db-f8e566a70655-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:30,381 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,382 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,383 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,383 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,383 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,383 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-47728647-7a80-4592-ba7c-c3ebcb1e5224-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:30,384 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,385 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,386 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,386 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,386 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,386 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9b3bccf5-ad25-404d-9310-88a915f9b7a2-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:30,387 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,388 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,389 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,389 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,389 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,389 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-510a5951-088e-4dab-8dd0-c02c47869d79-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:30,390 INFO [Executor task launch worker for task 76] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,391 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,392 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,392 INFO [Executor task launch worker for task 76] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,392 WARN [Executor task launch worker for task 76] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,393 INFO [Executor task launch worker for task 76] o.a.s.e.Executor [Logging.scala:54] Finished task 4.0 in stage 44.0 (TID 76). 1423 bytes result sent to driver
2018-10-11 21:36:30,393 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 5.0 in stage 44.0 (TID 77, localhost, executor driver, partition 5, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:30,393 INFO [Executor task launch worker for task 77] o.a.s.e.Executor [Logging.scala:54] Running task 5.0 in stage 44.0 (TID 77)
2018-10-11 21:36:30,393 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 4.0 in stage 44.0 (TID 76) in 148 ms on localhost (executor driver) (5/8)
2018-10-11 21:36:30,394 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6650061f-a0ab-42f3-be0d-e29bba951689-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:30,395 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,396 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,397 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,397 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,397 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,397 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a73de2ae-2a91-46fe-a61e-d06104068675-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:30,398 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,399 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,400 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,400 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,401 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,401 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-60348a41-e7a8-4c02-9dbf-a4848152bf0a-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:30,401 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,403 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,404 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,405 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,405 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,405 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca9d3388-a32e-42aa-9b9a-f2ba826d1b2e-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:30,406 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,407 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,408 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,408 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,409 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,409 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46ee92fd-a03b-4ad7-a847-fac273921f57-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:30,409 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,411 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,411 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,411 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,412 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,412 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4317e6e3-7c07-49f4-9bea-564768382089-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:30,413 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,414 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,414 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,414 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,415 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,415 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dc5fe75d-ae31-4bd0-a866-d3274b2d629b-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:30,415 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,417 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,418 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,419 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,419 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,419 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-49c1ab9d-a225-4ba8-911f-7c8138662881-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:30,420 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,422 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,423 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,424 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,424 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,424 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a910a7f7-d2c2-4fb9-9907-6f0a12ef5816-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,425 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,427 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,428 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,429 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,429 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,429 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af526506-da3a-461a-b7d6-7576a7ca2dc6-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,430 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,432 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,433 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,434 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,434 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,434 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f7492e12-3b8e-43e6-b84c-e42d363116f9-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,435 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,437 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,438 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,439 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,439 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,439 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-272bed24-7d51-4fc6-9a8a-aba3fb69c4b8-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,440 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,442 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,443 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,444 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,444 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,444 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ecac671-fc2e-4df4-9315-bf4c516db607-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,445 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,447 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,448 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,448 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,449 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,449 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f2c3b673-16d6-4469-a5ed-13de2cc92f10-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,450 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,451 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,453 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,453 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,453 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,454 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a6947b4-4ad6-4ab6-8907-a8320e284bcb-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:30,455 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,457 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,458 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,458 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,458 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,458 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3a0fc732-ce1b-4da0-9cd5-162ab4573e6e-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,459 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,460 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,462 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,462 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,462 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,462 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a087b5c1-75ef-4738-bcac-2ee86aa294bd-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,463 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,465 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,466 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,466 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,466 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,466 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e0eae261-dec5-4554-b126-6e1829f78050-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,467 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,469 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,470 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,470 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,471 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,471 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-987c95c1-23dd-407a-98e6-0e238fe1473b-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,472 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,474 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,475 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,475 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,476 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,476 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb60375b-4e46-4d45-99c6-540adc7208c3-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,476 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,478 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,478 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,479 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,479 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,479 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-319157ef-58f7-4406-b9b1-6baf5b53b0a8-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,480 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,481 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,482 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,483 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,483 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,483 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8022bee4-ba1f-4f2d-87cc-7ae422111479-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,484 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,486 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,487 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,487 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,488 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,488 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-861da466-0eee-47d7-9ff5-971c6d7f5a93-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,489 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,491 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,492 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,493 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,493 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,493 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8a3516de-8b82-44d1-82a8-69aa660cbbdf-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,494 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,495 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,496 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,496 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,497 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,497 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f419bf0e-cdd8-4f6c-8beb-f4dedb61a260-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,498 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,499 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,500 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,500 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,500 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,501 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e766d71-e245-4297-857b-bdd21b58b6f4-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,501 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,502 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,503 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,504 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,504 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,504 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4c33fd2a-9846-4f6b-a1f0-dccf06433485-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,505 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,506 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,507 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,507 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,508 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,508 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8fc6b69-477b-42e7-9d1a-3c499d4d10b2-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:30,509 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,510 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,511 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,511 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,512 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,512 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9039fff7-2957-4b0e-992e-e87565413ec8-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,513 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,514 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,515 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,515 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,515 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,516 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ef4e6a53-30c6-43c0-8eff-8a34e34edf6b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,516 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,518 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,519 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,520 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,520 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,520 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a398596-1df5-4ca7-911f-e74abd3762dd-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,521 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,523 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,524 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,524 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,524 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,524 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58f4dff5-7214-4777-ba17-784e2cef12f4-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,525 INFO [Executor task launch worker for task 77] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,527 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,527 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,528 INFO [Executor task launch worker for task 77] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,528 WARN [Executor task launch worker for task 77] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,529 INFO [Executor task launch worker for task 77] o.a.s.e.Executor [Logging.scala:54] Finished task 5.0 in stage 44.0 (TID 77). 1423 bytes result sent to driver
2018-10-11 21:36:30,529 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 6.0 in stage 44.0 (TID 78, localhost, executor driver, partition 6, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:30,530 INFO [Executor task launch worker for task 78] o.a.s.e.Executor [Logging.scala:54] Running task 6.0 in stage 44.0 (TID 78)
2018-10-11 21:36:30,530 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 5.0 in stage 44.0 (TID 77) in 137 ms on localhost (executor driver) (6/8)
2018-10-11 21:36:30,531 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2422c402-71d9-41d7-9987-1d4ab7fc19fb-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,532 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,534 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,535 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,535 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,536 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,536 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0ff4b192-b8e8-4b6c-9828-f5d9ec00cbce-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,537 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,539 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,540 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,541 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,541 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,541 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a2d93e67-4023-47f3-9d4a-f50a9c8541fe-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,542 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,544 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,545 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,546 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,546 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,546 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a29231b4-e038-4ee0-88ef-1d68cfe9d2be-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,547 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,549 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,550 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,550 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,550 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,551 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-55b99cce-2e82-46d0-8e53-bee4918b429e-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,552 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,554 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,555 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,555 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,555 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,555 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9dc2a350-c1b5-4c5c-8f81-b54fcb2ffa3a-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,556 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,557 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,558 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,558 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,559 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,559 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1be87a1d-cfca-46a7-b3c5-5377826665a1-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,559 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,561 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,562 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,562 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,562 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,562 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ac9002e-c497-481d-b337-445644058c65-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,563 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,565 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,566 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,567 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,567 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,567 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7453f0e9-fcdc-45d9-aa4e-04e2742b8b2b-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,568 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,569 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,570 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,570 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,571 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,571 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3ce2c6c2-8244-4825-9d79-b2816de2acda-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,572 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,574 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,575 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,575 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,575 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,575 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e32c7dc-dcea-456e-a878-c9c835248ea9-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,576 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,577 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,578 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,579 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,579 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,579 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b2dcecd1-372e-4f0b-9562-f96ebf3af8f6-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,580 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,581 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,582 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,582 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,582 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,583 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b266ce1f-aa2d-4c98-8bd7-38ee65369d89-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,583 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,585 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,586 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,586 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,586 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,586 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6c58af95-e7c8-4556-989d-8a8fc881fc54-c000.snappy.parquet, range: 0-11360, partition values: [empty row]
2018-10-11 21:36:30,587 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,589 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,590 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,590 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,591 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,591 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-da19fb6b-0851-4b26-8454-89e7d9d29409-c000.snappy.parquet, range: 0-11359, partition values: [empty row]
2018-10-11 21:36:30,592 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,593 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,594 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,594 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,594 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,594 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2646c07b-af07-4e07-8e28-e3b57bbb514c-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,595 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,596 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,598 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,598 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,598 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,599 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7fed2b39-6f8a-4f9e-8921-caa7441424ec-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,600 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,601 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,602 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,603 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,603 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,604 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f4b5fa6b-5fa7-4ccf-b1ac-7fe087ca4972-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,605 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,606 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,607 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,608 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,608 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,608 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e8617665-882c-4cd9-af6c-d70cf9357935-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,609 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,610 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,611 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,611 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,611 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,611 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9c42e760-f3d5-4337-93cc-2faeb5608d5f-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,612 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,614 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,615 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,615 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,616 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,616 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-992bc073-6936-47bb-9ce0-4bd6ff5312f3-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,617 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,619 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,620 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,620 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,621 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,621 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bdc96751-af2e-4c0d-aad9-9b6309410e26-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,622 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,623 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,624 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,624 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,625 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,625 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2a8062ca-53c1-4efb-887d-a5f15b2e2675-c000.snappy.parquet, range: 0-11358, partition values: [empty row]
2018-10-11 21:36:30,626 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,627 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,628 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,628 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,628 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,628 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-461919ce-4e71-49bc-ab59-c7e999f082df-c000.snappy.parquet, range: 0-11357, partition values: [empty row]
2018-10-11 21:36:30,630 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,631 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,633 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,633 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,633 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,634 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d184ff46-c4c0-4adb-993f-d1625bf18c93-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:30,635 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,636 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,638 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,638 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,638 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,638 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a542cfa-fb0f-420c-808c-c23861bc5e49-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:30,639 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,640 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,641 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,641 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,642 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,642 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3744292d-429a-4b99-8f87-0174a77a5f02-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:30,643 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,644 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,645 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,645 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,646 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,646 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ce13747d-84bc-4b83-bcbf-ed87cb408c63-c000.snappy.parquet, range: 0-11354, partition values: [empty row]
2018-10-11 21:36:30,647 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,648 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,648 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,649 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,649 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,649 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa3ef260-acd7-4ea3-b2bb-08ce39dc72a4-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:30,650 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,651 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,652 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,652 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,653 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,653 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09afdad3-a22e-46d0-8913-a98493735de8-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:30,654 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,656 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,657 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,657 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,657 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,657 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4de0605-1810-4659-8565-775942a08750-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:30,658 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,659 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,660 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,660 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,661 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,661 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c9cfad0a-8f5c-4599-8a7c-0d13b6068b66-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:30,662 INFO [Executor task launch worker for task 78] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,663 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,664 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,664 INFO [Executor task launch worker for task 78] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,664 WARN [Executor task launch worker for task 78] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,665 INFO [Executor task launch worker for task 78] o.a.s.e.Executor [Logging.scala:54] Finished task 6.0 in stage 44.0 (TID 78). 1423 bytes result sent to driver
2018-10-11 21:36:30,665 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 7.0 in stage 44.0 (TID 79, localhost, executor driver, partition 7, PROCESS_LOCAL, 10904 bytes)
2018-10-11 21:36:30,666 INFO [Executor task launch worker for task 79] o.a.s.e.Executor [Logging.scala:54] Running task 7.0 in stage 44.0 (TID 79)
2018-10-11 21:36:30,666 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 6.0 in stage 44.0 (TID 78) in 137 ms on localhost (executor driver) (7/8)
2018-10-11 21:36:30,667 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aa132c9-1c2c-4bed-8054-4f01f905b173-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:30,668 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,670 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,671 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,671 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,671 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,672 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1f52f9b8-ea21-4d2b-a7d2-11744187f778-c000.snappy.parquet, range: 0-11353, partition values: [empty row]
2018-10-11 21:36:30,673 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,674 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,676 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,676 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,676 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,676 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-51f29362-71b4-40bb-9ac9-c8fd50e8b54c-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,677 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,679 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,680 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,680 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,680 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,681 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca8ad858-bbe8-4c3b-a0f5-ea89627075a3-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,682 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,683 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,684 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,685 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,685 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,685 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-000c5114-25aa-43b6-ae19-ab0069bbeadf-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,686 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,687 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,688 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,689 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,689 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,689 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af314aee-5298-45df-899e-f5cf50b26e6a-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,690 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,692 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,693 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,693 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,694 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,694 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-06648b17-d906-45c9-b987-0c863fb63c7b-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,695 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,697 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,698 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,698 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,699 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,699 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-deaed750-dc34-4faa-bfea-c7b9e6d08139-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,700 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,702 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,703 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,704 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:30,704 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,704 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-edc1f6e1-52af-4b5a-9623-413fdd0995f1-c000.snappy.parquet, range: 0-11352, partition values: [empty row]
2018-10-11 21:36:30,705 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,707 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,708 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,708 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,708 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,708 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46686477-1f45-40b8-b264-443e98dbf08f-c000.snappy.parquet, range: 0-11350, partition values: [empty row]
2018-10-11 21:36:30,709 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,711 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,712 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,712 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,712 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,712 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d8e5bc47-ef5e-484a-98b7-34e73c59faf3-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:30,713 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,714 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,716 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,716 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,716 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,717 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-217b388a-5636-42f5-a2f7-f7e650de6c64-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:30,718 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,719 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,721 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,721 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,721 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,722 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-824fe1c8-dff2-491e-86b8-2e2542684692-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:30,723 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,725 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,726 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,726 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,726 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,726 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f812e08f-6f88-4c2d-96ec-83fdeb1e434a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:30,727 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,728 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,729 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,729 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,729 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,730 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9020ecd0-b01d-408c-baa5-a3b077d92f4a-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:30,730 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,732 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,732 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,733 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,733 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,733 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5dccade-9231-4d83-bfc0-eeddbd723979-c000.snappy.parquet, range: 0-11349, partition values: [empty row]
2018-10-11 21:36:30,734 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,735 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:30,736 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:30,737 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:30,737 WARN [Executor task launch worker for task 79] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,737 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7ce47add-8d58-4bb1-b893-375d2709dd55-c000.snappy.parquet, range: 0-882, partition values: [empty row]
2018-10-11 21:36:30,738 INFO [Executor task launch worker for task 79] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:30,740 INFO [Executor task launch worker for task 79] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:30,741 INFO [Executor task launch worker for task 79] o.a.s.e.Executor [Logging.scala:54] Finished task 7.0 in stage 44.0 (TID 79). 1423 bytes result sent to driver
2018-10-11 21:36:30,742 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 7.0 in stage 44.0 (TID 79) in 77 ms on localhost (executor driver) (8/8)
2018-10-11 21:36:30,742 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 44.0, whose tasks have all completed, from pool 
2018-10-11 21:36:30,742 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 44 (collect at SparkDataManager.scala:77) finished in 1.161 s
2018-10-11 21:36:30,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 33 finished: collect at SparkDataManager.scala:77, took 1.163983 s
2018-10-11 21:36:30,743 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,744 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,744 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,744 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,744 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,745 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,745 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,745 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,745 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,746 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,746 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,746 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,746 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,747 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,747 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,747 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,747 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,748 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,748 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,748 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 21:36:30,749 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,791 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:30,791 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:30,818 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:30,818 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:30,825 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,826 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,826 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:30,826 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:30,826 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:30,828 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:30,829 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:30,835 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:30,836 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:30,874 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1423
2018-10-11 21:36:30,874 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1456
2018-10-11 21:36:30,875 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_66_piece0 on 192.168.0.206:34485 in memory (size: 21.7 KB, free: 1391.3 MB)
2018-10-11 21:36:30,875 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1455
2018-10-11 21:36:30,875 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1435
2018-10-11 21:36:30,875 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1445
2018-10-11 21:36:30,876 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_68_piece0 on 192.168.0.206:34485 in memory (size: 21.7 KB, free: 1391.4 MB)
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1414
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1473
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1470
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1447
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1440
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1431
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1430
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1465
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1419
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1436
2018-10-11 21:36:30,876 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1415
2018-10-11 21:36:30,877 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_67_piece0 on 192.168.0.206:34485 in memory (size: 6.7 KB, free: 1391.4 MB)
2018-10-11 21:36:30,878 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_69_piece0 on 192.168.0.206:34485 in memory (size: 6.7 KB, free: 1391.4 MB)
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1450
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1458
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1446
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1421
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1427
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1472
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1417
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1449
2018-10-11 21:36:30,878 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1468
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1444
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1441
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1443
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1426
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1429
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1439
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1469
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1471
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1464
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1432
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1461
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1424
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1425
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1420
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1457
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1422
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1451
2018-10-11 21:36:30,879 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1474
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1416
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1466
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1460
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1428
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1452
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1418
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1437
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1438
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1463
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1475
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1454
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1462
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1442
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1434
2018-10-11 21:36:30,880 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1433
2018-10-11 21:36:30,881 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1448
2018-10-11 21:36:30,881 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1459
2018-10-11 21:36:30,881 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1467
2018-10-11 21:36:30,881 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1453
2018-10-11 21:36:30,957 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:30,957 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:30,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:30,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:30,984 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 15.476424 ms
2018-10-11 21:36:30,987 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_70 stored as values in memory (estimated size 230.0 KB, free 1390.9 MB)
2018-10-11 21:36:30,994 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_70_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.9 MB)
2018-10-11 21:36:30,994 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_70_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.4 MB)
2018-10-11 21:36:30,995 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 70 from count at SparkDataManager.scala:47
2018-10-11 21:36:30,995 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 16791983 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:31,003 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:31,004 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 144 (count at SparkDataManager.scala:47)
2018-10-11 21:36:31,005 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 34 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:31,005 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 46 (count at SparkDataManager.scala:47)
2018-10-11 21:36:31,005 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 45)
2018-10-11 21:36:31,005 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 45)
2018-10-11 21:36:31,006 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 45 (MapPartitionsRDD[144] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:31,007 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_71 stored as values in memory (estimated size 26.4 KB, free 1390.9 MB)
2018-10-11 21:36:31,008 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_71_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1390.9 MB)
2018-10-11 21:36:31,008 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_71_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.3 MB)
2018-10-11 21:36:31,008 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 71 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,009 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[144] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,009 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 45.0 with 1 tasks
2018-10-11 21:36:31,009 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 45.0 (TID 80, localhost, executor driver, partition 0, PROCESS_LOCAL, 8828 bytes)
2018-10-11 21:36:31,010 INFO [Executor task launch worker for task 80] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 45.0 (TID 80)
2018-10-11 21:36:31,014 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:31,016 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,020 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:31,023 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:31,023 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:31,024 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,024 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,025 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:31,026 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,029 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:31,032 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:31,033 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:31,034 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,034 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,034 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,034 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:31,035 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,040 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:31,042 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:31,042 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:31,043 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,043 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,043 WARN [Executor task launch worker for task 80] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,043 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:31,044 INFO [Executor task launch worker for task 80] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,047 INFO [Executor task launch worker for task 80] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:31,051 INFO [Executor task launch worker for task 80] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 45.0 (TID 80). 1651 bytes result sent to driver
2018-10-11 21:36:31,051 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 45.0 (TID 80) in 42 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,052 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 45.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,052 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 45 (count at SparkDataManager.scala:47) finished in 0.046 s
2018-10-11 21:36:31,052 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:31,052 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:31,052 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 46)
2018-10-11 21:36:31,052 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:31,053 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 46 (MapPartitionsRDD[147] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:31,054 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_72 stored as values in memory (estimated size 7.4 KB, free 1390.9 MB)
2018-10-11 21:36:31,055 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_72_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.9 MB)
2018-10-11 21:36:31,055 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_72_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:31,056 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 72 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,056 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[147] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,056 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 46.0 with 1 tasks
2018-10-11 21:36:31,057 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 46.0 (TID 81, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:31,057 INFO [Executor task launch worker for task 81] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 46.0 (TID 81)
2018-10-11 21:36:31,059 INFO [Executor task launch worker for task 81] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:31,059 INFO [Executor task launch worker for task 81] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:31,060 INFO [Executor task launch worker for task 81] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 46.0 (TID 81). 1732 bytes result sent to driver
2018-10-11 21:36:31,061 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 46.0 (TID 81) in 5 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,061 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 46.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,061 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 46 (count at SparkDataManager.scala:47) finished in 0.008 s
2018-10-11 21:36:31,061 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 34 finished: count at SparkDataManager.scala:47, took 0.057686 s
2018-10-11 21:36:31,063 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:31,063 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:31,066 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:31,067 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:31,079 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:31,079 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:31,200 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:31,200 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:31,200 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:31,201 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:31,233 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 19.695722 ms
2018-10-11 21:36:31,236 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_73 stored as values in memory (estimated size 230.0 KB, free 1390.6 MB)
2018-10-11 21:36:31,245 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_73_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.6 MB)
2018-10-11 21:36:31,246 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_73_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:31,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 73 from count at SparkDataManager.scala:47
2018-10-11 21:36:31,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 16791983 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:31,261 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:31,262 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 150 (count at SparkDataManager.scala:47)
2018-10-11 21:36:31,262 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 35 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:31,262 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 48 (count at SparkDataManager.scala:47)
2018-10-11 21:36:31,262 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 47)
2018-10-11 21:36:31,262 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 47)
2018-10-11 21:36:31,263 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 47 (MapPartitionsRDD[150] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:31,264 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_74 stored as values in memory (estimated size 26.4 KB, free 1390.6 MB)
2018-10-11 21:36:31,265 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_74_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1390.6 MB)
2018-10-11 21:36:31,265 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_74_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.3 MB)
2018-10-11 21:36:31,266 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 74 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,266 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[150] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,266 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 47.0 with 1 tasks
2018-10-11 21:36:31,267 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 47.0 (TID 82, localhost, executor driver, partition 0, PROCESS_LOCAL, 8828 bytes)
2018-10-11 21:36:31,267 INFO [Executor task launch worker for task 82] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 47.0 (TID 82)
2018-10-11 21:36:31,272 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:31,273 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,278 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:31,281 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:31,281 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:31,283 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,283 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,283 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:31,285 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,291 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:31,294 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:31,294 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:31,294 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,295 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,295 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,295 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:31,296 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,299 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:31,302 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:31,302 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:31,303 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,303 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,303 WARN [Executor task launch worker for task 82] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:31,303 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:31,304 INFO [Executor task launch worker for task 82] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:31,307 INFO [Executor task launch worker for task 82] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:31,313 INFO [Executor task launch worker for task 82] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 47.0 (TID 82). 1651 bytes result sent to driver
2018-10-11 21:36:31,314 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 47.0 (TID 82) in 47 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,314 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 47.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,314 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 47 (count at SparkDataManager.scala:47) finished in 0.051 s
2018-10-11 21:36:31,314 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:31,314 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:31,315 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 48)
2018-10-11 21:36:31,315 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:31,315 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 48 (MapPartitionsRDD[153] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:31,317 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_75 stored as values in memory (estimated size 7.4 KB, free 1390.6 MB)
2018-10-11 21:36:31,318 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_75_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.6 MB)
2018-10-11 21:36:31,318 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_75_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:31,319 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 75 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,319 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[153] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,319 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 48.0 with 1 tasks
2018-10-11 21:36:31,320 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 48.0 (TID 83, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:31,321 INFO [Executor task launch worker for task 83] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 48.0 (TID 83)
2018-10-11 21:36:31,325 INFO [Executor task launch worker for task 83] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:31,325 INFO [Executor task launch worker for task 83] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:31,326 INFO [Executor task launch worker for task 83] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 48.0 (TID 83). 1775 bytes result sent to driver
2018-10-11 21:36:31,327 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 48.0 (TID 83) in 7 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,327 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 48.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,328 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 48 (count at SparkDataManager.scala:47) finished in 0.012 s
2018-10-11 21:36:31,329 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 35 finished: count at SparkDataManager.scala:47, took 0.067387 s
2018-10-11 21:36:31,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 25.170731 ms
2018-10-11 21:36:31,430 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:31,430 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:31,461 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,462 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,462 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,486 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:55
2018-10-11 21:36:31,491 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 36 (insertInto at SparkDataManager.scala:55) with 1 output partitions
2018-10-11 21:36:31,491 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 49 (insertInto at SparkDataManager.scala:55)
2018-10-11 21:36:31,492 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:31,492 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:31,492 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 49 (MapPartitionsRDD[155] at insertInto at SparkDataManager.scala:55), which has no missing parents
2018-10-11 21:36:31,508 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_76 stored as values in memory (estimated size 131.8 KB, free 1390.4 MB)
2018-10-11 21:36:31,512 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_76_piece0 stored as bytes in memory (estimated size 46.7 KB, free 1390.4 MB)
2018-10-11 21:36:31,513 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_76_piece0 in memory on 192.168.0.206:34485 (size: 46.7 KB, free: 1391.3 MB)
2018-10-11 21:36:31,513 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 76 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,515 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[155] at insertInto at SparkDataManager.scala:55) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,515 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 49.0 with 1 tasks
2018-10-11 21:36:31,516 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 49.0 (TID 84, localhost, executor driver, partition 0, PROCESS_LOCAL, 9044 bytes)
2018-10-11 21:36:31,516 INFO [Executor task launch worker for task 84] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 49.0 (TID 84)
2018-10-11 21:36:31,524 INFO [Executor task launch worker for task 84] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,524 INFO [Executor task launch worker for task 84] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:31,525 INFO [Executor task launch worker for task 84] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:31,526 INFO [Executor task launch worker for task 84] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "database",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "executionDateTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "sparkConfig",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "queries",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "id",
          "type" : "short",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "businessQuestion",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "queryClass",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "statements",
          "type" : {
            "type" : "array",
            "elementType" : {
              "type" : "struct",
              "fields" : [ {
                "name" : "id",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              }, {
                "name" : "text",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              } ]
            },
            "containsNull" : true
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

       
2018-10-11 21:36:31,535 INFO [Executor task launch worker for task 84] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 902
2018-10-11 21:36:31,548 INFO [Executor task launch worker for task 84] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213631_0049_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs/_temporary/0/task_20181011213631_0049_m_000000
2018-10-11 21:36:31,548 INFO [Executor task launch worker for task 84] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213631_0049_m_000000_0: Committed
2018-10-11 21:36:31,549 INFO [Executor task launch worker for task 84] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 49.0 (TID 84). 1996 bytes result sent to driver
2018-10-11 21:36:31,550 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 49.0 (TID 84) in 34 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,550 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 49.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,556 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 49 (insertInto at SparkDataManager.scala:55) finished in 0.064 s
2018-10-11 21:36:31,557 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 36 finished: insertInto at SparkDataManager.scala:55, took 0.070123 s
2018-10-11 21:36:31,573 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:31,574 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:31,576 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:31,577 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:31,580 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:31,580 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:31,592 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:31,592 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:31,602 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:31,603 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:31,620 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs
2018-10-11 21:36:31,621 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs	
2018-10-11 21:36:31,637 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_runs
2018-10-11 21:36:31,637 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_runs to 19119
2018-10-11 21:36:31,679 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 21:36:31,683 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:31,683 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:31,687 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:36:31,688 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:31,689 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:31,692 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:31,692 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:31,706 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:31,706 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:31,706 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:31,707 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:31,717 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_77 stored as values in memory (estimated size 226.1 KB, free 1390.2 MB)
2018-10-11 21:36:31,726 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_77_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.2 MB)
2018-10-11 21:36:31,727 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_77_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.2 MB)
2018-10-11 21:36:31,728 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 77 from collect at Statement.scala:24
2018-10-11 21:36:31,729 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:31,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:31,746 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 159 (collect at Statement.scala:24)
2018-10-11 21:36:31,746 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 37 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:31,747 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 51 (collect at Statement.scala:24)
2018-10-11 21:36:31,747 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 50)
2018-10-11 21:36:31,747 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 50)
2018-10-11 21:36:31,747 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 50 (MapPartitionsRDD[159] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:31,748 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_78 stored as values in memory (estimated size 11.1 KB, free 1390.1 MB)
2018-10-11 21:36:31,749 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_78_piece0 stored as bytes in memory (estimated size 5.4 KB, free 1390.1 MB)
2018-10-11 21:36:31,750 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_78_piece0 in memory on 192.168.0.206:34485 (size: 5.4 KB, free: 1391.2 MB)
2018-10-11 21:36:31,750 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 78 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,750 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[159] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,750 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 50.0 with 1 tasks
2018-10-11 21:36:31,751 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 50.0 (TID 85, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:31,751 INFO [Executor task launch worker for task 85] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 50.0 (TID 85)
2018-10-11 21:36:31,753 INFO [Executor task launch worker for task 85] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:31,758 INFO [Executor task launch worker for task 85] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 50.0 (TID 85). 1680 bytes result sent to driver
2018-10-11 21:36:31,759 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 50.0 (TID 85) in 8 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,759 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 50.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,759 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 50 (collect at Statement.scala:24) finished in 0.011 s
2018-10-11 21:36:31,759 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:31,759 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:31,759 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 51)
2018-10-11 21:36:31,760 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:31,760 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 51 (MapPartitionsRDD[162] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:31,761 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_79 stored as values in memory (estimated size 7.4 KB, free 1390.1 MB)
2018-10-11 21:36:31,763 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_79_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.1 MB)
2018-10-11 21:36:31,763 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_79_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.2 MB)
2018-10-11 21:36:31,764 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 79 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,764 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[162] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,764 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 51.0 with 1 tasks
2018-10-11 21:36:31,765 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 51.0 (TID 86, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:31,765 INFO [Executor task launch worker for task 86] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 51.0 (TID 86)
2018-10-11 21:36:31,768 INFO [Executor task launch worker for task 86] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:31,768 INFO [Executor task launch worker for task 86] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:31,771 INFO [Executor task launch worker for task 86] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 51.0 (TID 86). 1825 bytes result sent to driver
2018-10-11 21:36:31,772 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 51.0 (TID 86) in 7 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,772 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 51.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,772 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 51 (collect at Statement.scala:24) finished in 0.012 s
2018-10-11 21:36:31,773 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 37 finished: collect at Statement.scala:24, took 0.027217 s
2018-10-11 21:36:31,773 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:36:31,807 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:31,808 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:31,840 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,841 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,842 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:31,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 38 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:31,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 52 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:31,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:31,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:31,866 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 52 (MapPartitionsRDD[164] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:31,879 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_80 stored as values in memory (estimated size 131.0 KB, free 1390.0 MB)
2018-10-11 21:36:31,881 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_80_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.0 MB)
2018-10-11 21:36:31,881 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_80_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:31,881 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 80 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:31,882 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[164] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:31,882 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 52.0 with 1 tasks
2018-10-11 21:36:31,882 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 52.0 (TID 87, localhost, executor driver, partition 0, PROCESS_LOCAL, 10313 bytes)
2018-10-11 21:36:31,882 INFO [Executor task launch worker for task 87] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 52.0 (TID 87)
2018-10-11 21:36:31,889 INFO [Executor task launch worker for task 87] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,889 INFO [Executor task launch worker for task 87] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:31,890 INFO [Executor task launch worker for task 87] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:31,891 INFO [Executor task launch worker for task 87] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:31,895 INFO [Executor task launch worker for task 87] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2249
2018-10-11 21:36:31,898 INFO [Executor task launch worker for task 87] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213631_0052_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213631_0052_m_000000
2018-10-11 21:36:31,898 INFO [Executor task launch worker for task 87] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213631_0052_m_000000_0: Committed
2018-10-11 21:36:31,898 INFO [Executor task launch worker for task 87] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 52.0 (TID 87). 1996 bytes result sent to driver
2018-10-11 21:36:31,899 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 52.0 (TID 87) in 17 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:31,899 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 52.0, whose tasks have all completed, from pool 
2018-10-11 21:36:31,899 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 52 (insertInto at SparkDataManager.scala:61) finished in 0.033 s
2018-10-11 21:36:31,899 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 38 finished: insertInto at SparkDataManager.scala:61, took 0.034832 s
2018-10-11 21:36:31,908 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:31,909 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:31,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:31,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:31,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:31,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:31,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:31,936 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:31,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:31,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:31,960 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:31,961 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:32,017 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:32,017 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2976571
2018-10-11 21:36:32,090 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:32,091 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:32,094 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:32,095 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:32,107 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:32,108 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:32,141 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:32,141 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:32,142 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:32,142 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:32,143 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:32,143 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:32,143 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:32,144 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:32,155 INFO [broadcast-exchange-2] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_81 stored as values in memory (estimated size 226.1 KB, free 1389.7 MB)
2018-10-11 21:36:32,164 INFO [broadcast-exchange-2] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_81_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1389.7 MB)
2018-10-11 21:36:32,165 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_81_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.2 MB)
2018-10-11 21:36:32,165 INFO [broadcast-exchange-2] o.a.s.SparkContext [Logging.scala:54] Created broadcast 81 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:32,166 INFO [broadcast-exchange-2] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:32,174 INFO [broadcast-exchange-2] o.a.s.SparkContext [Logging.scala:54] Starting job: run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:32,175 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 39 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2018-10-11 21:36:32,175 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 53 (run at ThreadPoolExecutor.java:1149)
2018-10-11 21:36:32,175 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:32,175 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:32,175 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 53 (MapPartitionsRDD[168] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2018-10-11 21:36:32,177 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_82 stored as values in memory (estimated size 7.5 KB, free 1389.7 MB)
2018-10-11 21:36:32,178 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_82_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1389.7 MB)
2018-10-11 21:36:32,178 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_82_piece0 in memory on 192.168.0.206:34485 (size: 3.9 KB, free: 1391.2 MB)
2018-10-11 21:36:32,179 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 82 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:32,179 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[168] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:32,179 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 53.0 with 1 tasks
2018-10-11 21:36:32,180 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 53.0 (TID 88, localhost, executor driver, partition 0, PROCESS_LOCAL, 8375 bytes)
2018-10-11 21:36:32,180 INFO [Executor task launch worker for task 88] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 53.0 (TID 88)
2018-10-11 21:36:32,182 INFO [Executor task launch worker for task 88] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:32,184 INFO [Executor task launch worker for task 88] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 53.0 (TID 88). 1460 bytes result sent to driver
2018-10-11 21:36:32,185 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 53.0 (TID 88) in 4 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:32,185 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 53.0, whose tasks have all completed, from pool 
2018-10-11 21:36:32,186 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 53 (run at ThreadPoolExecutor.java:1149) finished in 0.010 s
2018-10-11 21:36:32,186 INFO [broadcast-exchange-2] o.a.s.s.DAGScheduler [Logging.scala:54] Job 39 finished: run at ThreadPoolExecutor.java:1149, took 0.012039 s
2018-10-11 21:36:32,188 INFO [broadcast-exchange-2] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_83 stored as values in memory (estimated size 5.8 KB, free 1389.7 MB)
2018-10-11 21:36:32,189 INFO [broadcast-exchange-2] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_83_piece0 stored as bytes in memory (estimated size 201.0 B, free 1389.7 MB)
2018-10-11 21:36:32,189 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_83_piece0 in memory on 192.168.0.206:34485 (size: 201.0 B, free: 1391.2 MB)
2018-10-11 21:36:32,190 INFO [broadcast-exchange-2] o.a.s.SparkContext [Logging.scala:54] Created broadcast 83 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:32,193 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_84 stored as values in memory (estimated size 226.1 KB, free 1389.5 MB)
2018-10-11 21:36:32,202 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_84_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1389.5 MB)
2018-10-11 21:36:32,203 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_84_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.1 MB)
2018-10-11 21:36:32,206 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 84 from collect at Statement.scala:24
2018-10-11 21:36:32,207 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:32,229 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:32,230 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 174 (collect at Statement.scala:24)
2018-10-11 21:36:32,230 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 40 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:32,230 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 55 (collect at Statement.scala:24)
2018-10-11 21:36:32,230 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 54)
2018-10-11 21:36:32,230 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 54)
2018-10-11 21:36:32,231 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 54 (MapPartitionsRDD[174] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:32,235 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_85 stored as values in memory (estimated size 17.5 KB, free 1389.4 MB)
2018-10-11 21:36:32,237 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_85_piece0 stored as bytes in memory (estimated size 8.1 KB, free 1389.4 MB)
2018-10-11 21:36:32,237 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_85_piece0 in memory on 192.168.0.206:34485 (size: 8.1 KB, free: 1391.1 MB)
2018-10-11 21:36:32,238 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 85 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:32,239 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 54 (MapPartitionsRDD[174] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:32,239 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 54.0 with 1 tasks
2018-10-11 21:36:32,240 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 54.0 (TID 89, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:32,240 INFO [Executor task launch worker for task 89] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 54.0 (TID 89)
2018-10-11 21:36:32,244 INFO [Executor task launch worker for task 89] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:32,250 INFO [Executor task launch worker for task 89] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 54.0 (TID 89). 2669 bytes result sent to driver
2018-10-11 21:36:32,251 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 54.0 (TID 89) in 12 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:32,251 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 54.0, whose tasks have all completed, from pool 
2018-10-11 21:36:32,252 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 54 (collect at Statement.scala:24) finished in 0.021 s
2018-10-11 21:36:32,252 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:32,252 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:32,252 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 55)
2018-10-11 21:36:32,252 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:32,253 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 55 (MapPartitionsRDD[177] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:32,255 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_86 stored as values in memory (estimated size 7.4 KB, free 1389.4 MB)
2018-10-11 21:36:32,257 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_86_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1389.4 MB)
2018-10-11 21:36:32,258 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_86_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:32,258 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 86 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:32,259 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[177] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:32,259 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 55.0 with 1 tasks
2018-10-11 21:36:32,260 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 55.0 (TID 90, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:32,260 INFO [Executor task launch worker for task 90] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 55.0 (TID 90)
2018-10-11 21:36:32,262 INFO [Executor task launch worker for task 90] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:32,263 INFO [Executor task launch worker for task 90] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:32,265 INFO [Executor task launch worker for task 90] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 55.0 (TID 90). 1782 bytes result sent to driver
2018-10-11 21:36:32,265 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 55.0 (TID 90) in 6 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:32,266 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 55.0, whose tasks have all completed, from pool 
2018-10-11 21:36:32,266 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 55 (collect at Statement.scala:24) finished in 0.012 s
2018-10-11 21:36:32,266 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 40 finished: collect at Statement.scala:24, took 0.037272 s
2018-10-11 21:36:32,267 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:36:32,302 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:32,303 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:32,346 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:32,349 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:32,350 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:32,366 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:32,367 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 41 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:32,367 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 56 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:32,367 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:32,367 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:32,367 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 56 (MapPartitionsRDD[179] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:32,379 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_87 stored as values in memory (estimated size 131.0 KB, free 1389.3 MB)
2018-10-11 21:36:32,381 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_87_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1389.2 MB)
2018-10-11 21:36:32,381 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_87_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.1 MB)
2018-10-11 21:36:32,382 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 87 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:32,382 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[179] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:32,382 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 56.0 with 1 tasks
2018-10-11 21:36:32,383 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 56.0 (TID 91, localhost, executor driver, partition 0, PROCESS_LOCAL, 10852 bytes)
2018-10-11 21:36:32,383 INFO [Executor task launch worker for task 91] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 56.0 (TID 91)
2018-10-11 21:36:32,389 INFO [Executor task launch worker for task 91] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:32,389 INFO [Executor task launch worker for task 91] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:32,389 INFO [Executor task launch worker for task 91] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:32,389 INFO [Executor task launch worker for task 91] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:32,390 INFO [Executor task launch worker for task 91] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:32,391 INFO [Executor task launch worker for task 91] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:32,395 INFO [Executor task launch worker for task 91] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2789
2018-10-11 21:36:32,398 INFO [Executor task launch worker for task 91] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213632_0056_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213632_0056_m_000000
2018-10-11 21:36:32,398 INFO [Executor task launch worker for task 91] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213632_0056_m_000000_0: Committed
2018-10-11 21:36:32,398 INFO [Executor task launch worker for task 91] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 56.0 (TID 91). 1996 bytes result sent to driver
2018-10-11 21:36:32,399 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 56.0 (TID 91) in 17 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:32,399 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 56.0, whose tasks have all completed, from pool 
2018-10-11 21:36:32,399 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 56 (insertInto at SparkDataManager.scala:61) finished in 0.031 s
2018-10-11 21:36:32,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 41 finished: insertInto at SparkDataManager.scala:61, took 0.032644 s
2018-10-11 21:36:32,407 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:32,408 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:32,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:32,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:32,426 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:32,427 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:32,441 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:32,442 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:32,457 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:32,458 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:32,470 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:32,470 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:32,501 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:32,501 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 2990268
2018-10-11 21:36:32,541 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:36:32,542 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 21:36:32,542 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,584 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:32,584 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:32,616 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:32,617 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:32,630 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,630 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,631 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,632 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:32,633 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:32,637 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:32,638 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:32,653 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:32,654 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:32,798 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:32,798 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:32,799 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:32,799 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:32,832 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 18.68352 ms
2018-10-11 21:36:32,836 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_88 stored as values in memory (estimated size 230.0 KB, free 1389.0 MB)
2018-10-11 21:36:32,845 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_88_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1389.0 MB)
2018-10-11 21:36:32,846 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_88_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.1 MB)
2018-10-11 21:36:32,846 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 88 from count at SparkDataManager.scala:47
2018-10-11 21:36:32,847 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 20990639 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:32,864 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:32,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 183 (count at SparkDataManager.scala:47)
2018-10-11 21:36:32,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 42 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:32,865 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 58 (count at SparkDataManager.scala:47)
2018-10-11 21:36:32,866 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 57)
2018-10-11 21:36:32,866 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 57)
2018-10-11 21:36:32,866 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 57 (MapPartitionsRDD[183] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:32,868 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_89 stored as values in memory (estimated size 26.4 KB, free 1389.0 MB)
2018-10-11 21:36:32,870 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_89_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1389.0 MB)
2018-10-11 21:36:32,870 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_89_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.1 MB)
2018-10-11 21:36:32,871 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 89 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:32,871 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 57 (MapPartitionsRDD[183] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:32,871 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 57.0 with 1 tasks
2018-10-11 21:36:32,872 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 57.0 (TID 92, localhost, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes)
2018-10-11 21:36:32,872 INFO [Executor task launch worker for task 92] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 57.0 (TID 92)
2018-10-11 21:36:32,884 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:32,886 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:32,893 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:32,895 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:32,896 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:32,897 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,897 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,898 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:32,900 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:32,905 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:32,908 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:32,909 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:32,910 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,911 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,911 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,911 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-7f906c98-fd19-4a72-b6e6-24c8141a3e20-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:32,913 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:32,917 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:32,920 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:32,920 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:32,921 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,921 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,921 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,922 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:32,924 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:32,929 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:32,933 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:32,933 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:32,934 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,934 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,935 WARN [Executor task launch worker for task 92] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:32,935 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:32,936 INFO [Executor task launch worker for task 92] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:32,942 INFO [Executor task launch worker for task 92] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:32,948 INFO [Executor task launch worker for task 92] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 57.0 (TID 92). 1651 bytes result sent to driver
2018-10-11 21:36:32,949 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 57.0 (TID 92) in 77 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:32,949 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 57.0, whose tasks have all completed, from pool 
2018-10-11 21:36:32,949 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 57 (count at SparkDataManager.scala:47) finished in 0.082 s
2018-10-11 21:36:32,949 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:32,950 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:32,950 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 58)
2018-10-11 21:36:32,950 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:32,950 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 58 (MapPartitionsRDD[186] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:32,951 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_90 stored as values in memory (estimated size 7.4 KB, free 1389.0 MB)
2018-10-11 21:36:32,952 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_90_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1389.0 MB)
2018-10-11 21:36:32,957 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_90_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:32,958 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 90 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:32,958 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[186] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:32,959 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 58.0 with 1 tasks
2018-10-11 21:36:32,968 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 58.0 (TID 93, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:32,968 INFO [Executor task launch worker for task 93] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 58.0 (TID 93)
2018-10-11 21:36:32,970 INFO [Executor task launch worker for task 93] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:32,971 INFO [Executor task launch worker for task 93] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:32,973 INFO [Executor task launch worker for task 93] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 58.0 (TID 93). 1775 bytes result sent to driver
2018-10-11 21:36:32,973 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 58.0 (TID 93) in 11 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:32,973 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 58.0, whose tasks have all completed, from pool 
2018-10-11 21:36:32,974 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 58 (count at SparkDataManager.scala:47) finished in 0.023 s
2018-10-11 21:36:32,974 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 42 finished: count at SparkDataManager.scala:47, took 0.109764 s
2018-10-11 21:36:32,976 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:32,976 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:32,979 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:32,980 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:32,996 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:33,003 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:33,258 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:33,259 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:33,260 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:33,260 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:33,315 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 41.051076 ms
2018-10-11 21:36:33,320 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_91 stored as values in memory (estimated size 230.0 KB, free 1388.7 MB)
2018-10-11 21:36:33,328 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_91_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1388.7 MB)
2018-10-11 21:36:33,329 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_91_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.0 MB)
2018-10-11 21:36:33,331 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 91 from count at SparkDataManager.scala:47
2018-10-11 21:36:33,332 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 20990639 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:33,350 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: count at SparkDataManager.scala:47
2018-10-11 21:36:33,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 189 (count at SparkDataManager.scala:47)
2018-10-11 21:36:33,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 43 (count at SparkDataManager.scala:47) with 1 output partitions
2018-10-11 21:36:33,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 60 (count at SparkDataManager.scala:47)
2018-10-11 21:36:33,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 59)
2018-10-11 21:36:33,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 59)
2018-10-11 21:36:33,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 59 (MapPartitionsRDD[189] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:33,353 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_92 stored as values in memory (estimated size 26.4 KB, free 1388.7 MB)
2018-10-11 21:36:33,353 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_92_piece0 stored as bytes in memory (estimated size 9.8 KB, free 1388.7 MB)
2018-10-11 21:36:33,354 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_92_piece0 in memory on 192.168.0.206:34485 (size: 9.8 KB, free: 1391.0 MB)
2018-10-11 21:36:33,354 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 92 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:33,355 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 59 (MapPartitionsRDD[189] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:33,355 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 59.0 with 1 tasks
2018-10-11 21:36:33,356 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 59.0 (TID 94, localhost, executor driver, partition 0, PROCESS_LOCAL, 8979 bytes)
2018-10-11 21:36:33,356 INFO [Executor task launch worker for task 94] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 59.0 (TID 94)
2018-10-11 21:36:33,361 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:33,363 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:33,368 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:33,371 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:33,371 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:33,373 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,373 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,374 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:33,378 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:33,382 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:33,384 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:33,385 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:33,385 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,385 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,385 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,385 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-7f906c98-fd19-4a72-b6e6-24c8141a3e20-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:33,388 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:33,393 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:33,395 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:33,395 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:33,396 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,396 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,396 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,396 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:33,398 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:33,402 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:33,405 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:33,406 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:33,407 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,407 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,407 WARN [Executor task launch worker for task 94] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:33,407 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:33,408 INFO [Executor task launch worker for task 94] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:33,411 INFO [Executor task launch worker for task 94] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:33,414 INFO [Executor task launch worker for task 94] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 59.0 (TID 94). 1651 bytes result sent to driver
2018-10-11 21:36:33,415 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 59.0 (TID 94) in 59 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:33,415 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 59.0, whose tasks have all completed, from pool 
2018-10-11 21:36:33,415 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 59 (count at SparkDataManager.scala:47) finished in 0.063 s
2018-10-11 21:36:33,415 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:33,415 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:33,415 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 60)
2018-10-11 21:36:33,415 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:33,416 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 60 (MapPartitionsRDD[192] at count at SparkDataManager.scala:47), which has no missing parents
2018-10-11 21:36:33,417 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_93 stored as values in memory (estimated size 7.4 KB, free 1388.7 MB)
2018-10-11 21:36:33,418 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_93_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1388.7 MB)
2018-10-11 21:36:33,418 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_93_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.0 MB)
2018-10-11 21:36:33,419 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 93 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:33,419 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[192] at count at SparkDataManager.scala:47) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:33,419 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 60.0 with 1 tasks
2018-10-11 21:36:33,420 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 60.0 (TID 95, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:33,421 INFO [Executor task launch worker for task 95] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 60.0 (TID 95)
2018-10-11 21:36:33,425 INFO [Executor task launch worker for task 95] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:33,425 INFO [Executor task launch worker for task 95] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:33,427 INFO [Executor task launch worker for task 95] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 60.0 (TID 95). 1775 bytes result sent to driver
2018-10-11 21:36:33,427 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 60.0 (TID 95) in 7 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:33,428 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 60.0, whose tasks have all completed, from pool 
2018-10-11 21:36:33,428 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 60 (count at SparkDataManager.scala:47) finished in 0.012 s
2018-10-11 21:36:33,429 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 43 finished: count at SparkDataManager.scala:47, took 0.078117 s
2018-10-11 21:36:33,533 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 43.857323 ms
2018-10-11 21:36:33,589 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:33,589 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:33,625 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,627 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,627 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,653 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:55
2018-10-11 21:36:33,654 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 44 (insertInto at SparkDataManager.scala:55) with 1 output partitions
2018-10-11 21:36:33,654 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 61 (insertInto at SparkDataManager.scala:55)
2018-10-11 21:36:33,654 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:33,654 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:33,654 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 61 (MapPartitionsRDD[194] at insertInto at SparkDataManager.scala:55), which has no missing parents
2018-10-11 21:36:33,670 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_94 stored as values in memory (estimated size 131.8 KB, free 1388.5 MB)
2018-10-11 21:36:33,672 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_94_piece0 stored as bytes in memory (estimated size 46.7 KB, free 1388.5 MB)
2018-10-11 21:36:33,672 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_94_piece0 in memory on 192.168.0.206:34485 (size: 46.7 KB, free: 1391.0 MB)
2018-10-11 21:36:33,673 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 94 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:33,673 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[194] at insertInto at SparkDataManager.scala:55) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:33,673 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 61.0 with 1 tasks
2018-10-11 21:36:33,674 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 61.0 (TID 96, localhost, executor driver, partition 0, PROCESS_LOCAL, 9044 bytes)
2018-10-11 21:36:33,674 INFO [Executor task launch worker for task 96] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 61.0 (TID 96)
2018-10-11 21:36:33,682 INFO [Executor task launch worker for task 96] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,683 INFO [Executor task launch worker for task 96] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,683 INFO [Executor task launch worker for task 96] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:33,683 INFO [Executor task launch worker for task 96] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:33,684 INFO [Executor task launch worker for task 96] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:33,685 INFO [Executor task launch worker for task 96] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "database",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "executionDateTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "sparkConfig",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "queries",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "id",
          "type" : "short",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "businessQuestion",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "queryClass",
          "type" : "integer",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "statements",
          "type" : {
            "type" : "array",
            "elementType" : {
              "type" : "struct",
              "fields" : [ {
                "name" : "id",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              }, {
                "name" : "text",
                "type" : "string",
                "nullable" : true,
                "metadata" : { }
              } ]
            },
            "containsNull" : true
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

       
2018-10-11 21:36:33,694 INFO [Executor task launch worker for task 96] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 902
2018-10-11 21:36:33,697 INFO [Executor task launch worker for task 96] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213633_0061_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_runs/_temporary/0/task_20181011213633_0061_m_000000
2018-10-11 21:36:33,697 INFO [Executor task launch worker for task 96] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213633_0061_m_000000_0: Committed
2018-10-11 21:36:33,698 INFO [Executor task launch worker for task 96] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 61.0 (TID 96). 1996 bytes result sent to driver
2018-10-11 21:36:33,698 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 61.0 (TID 96) in 24 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:33,698 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 61.0, whose tasks have all completed, from pool 
2018-10-11 21:36:33,699 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 61 (insertInto at SparkDataManager.scala:55) finished in 0.044 s
2018-10-11 21:36:33,699 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 44 finished: insertInto at SparkDataManager.scala:55, took 0.046231 s
2018-10-11 21:36:33,706 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:33,707 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:33,709 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:33,709 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:33,712 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:33,713 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:33,722 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:33,723 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:33,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:33,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:33,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs
2018-10-11 21:36:33,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_runs newtbl=spark_tpcds_runs	
2018-10-11 21:36:33,776 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_runs
2018-10-11 21:36:33,776 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_runs to 23471
2018-10-11 21:36:33,818 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 21:36:33,822 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:33,823 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:33,825 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:36:33,826 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:33,827 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:33,830 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:33,832 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:33,859 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:33,860 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:33,860 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:33,861 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:33,871 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_95 stored as values in memory (estimated size 226.1 KB, free 1388.3 MB)
2018-10-11 21:36:33,882 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_95_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1388.2 MB)
2018-10-11 21:36:33,883 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_95_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1390.9 MB)
2018-10-11 21:36:33,884 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 95 from collect at Statement.scala:24
2018-10-11 21:36:33,884 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:33,899 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:33,900 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 198 (collect at Statement.scala:24)
2018-10-11 21:36:33,900 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 45 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:33,900 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 63 (collect at Statement.scala:24)
2018-10-11 21:36:33,900 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 62)
2018-10-11 21:36:33,900 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 62)
2018-10-11 21:36:33,901 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 62 (MapPartitionsRDD[198] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:33,902 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_96 stored as values in memory (estimated size 11.1 KB, free 1388.2 MB)
2018-10-11 21:36:33,905 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_96_piece0 stored as bytes in memory (estimated size 5.5 KB, free 1388.2 MB)
2018-10-11 21:36:33,905 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_96_piece0 in memory on 192.168.0.206:34485 (size: 5.5 KB, free: 1390.9 MB)
2018-10-11 21:36:33,906 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 96 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:33,906 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[198] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:33,906 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 62.0 with 1 tasks
2018-10-11 21:36:33,907 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 62.0 (TID 97, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:33,907 INFO [Executor task launch worker for task 97] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 62.0 (TID 97)
2018-10-11 21:36:33,909 INFO [Executor task launch worker for task 97] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:33,911 INFO [Executor task launch worker for task 97] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 62.0 (TID 97). 1723 bytes result sent to driver
2018-10-11 21:36:33,911 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 62.0 (TID 97) in 4 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:33,911 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 62.0, whose tasks have all completed, from pool 
2018-10-11 21:36:33,912 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 62 (collect at Statement.scala:24) finished in 0.011 s
2018-10-11 21:36:33,912 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:33,912 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:33,912 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 63)
2018-10-11 21:36:33,912 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:33,912 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 63 (MapPartitionsRDD[201] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:33,913 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_97 stored as values in memory (estimated size 7.4 KB, free 1388.2 MB)
2018-10-11 21:36:33,916 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_97_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1388.2 MB)
2018-10-11 21:36:33,916 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_97_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1390.9 MB)
2018-10-11 21:36:33,917 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 97 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:33,917 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[201] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:33,917 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 63.0 with 1 tasks
2018-10-11 21:36:33,918 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 63.0 (TID 98, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:33,918 INFO [Executor task launch worker for task 98] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 63.0 (TID 98)
2018-10-11 21:36:33,920 INFO [Executor task launch worker for task 98] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:33,920 INFO [Executor task launch worker for task 98] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 0 ms
2018-10-11 21:36:33,921 INFO [Executor task launch worker for task 98] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 63.0 (TID 98). 1782 bytes result sent to driver
2018-10-11 21:36:33,922 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 63.0 (TID 98) in 4 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:33,922 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 63.0, whose tasks have all completed, from pool 
2018-10-11 21:36:33,923 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 63 (collect at Statement.scala:24) finished in 0.010 s
2018-10-11 21:36:33,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 45 finished: collect at Statement.scala:24, took 0.023675 s
2018-10-11 21:36:33,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:36:33,954 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:33,955 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:33,998 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,999 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:33,999 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,045 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1866
2018-10-11 21:36:34,045 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1642
2018-10-11 21:36:34,045 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1582
2018-10-11 21:36:34,045 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1576
2018-10-11 21:36:34,046 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1561
2018-10-11 21:36:34,046 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1647
2018-10-11 21:36:34,046 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1708
2018-10-11 21:36:34,046 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1778
2018-10-11 21:36:34,046 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1899
2018-10-11 21:36:34,047 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_81_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.0 MB)
2018-10-11 21:36:34,048 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1948
2018-10-11 21:36:34,048 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2009
2018-10-11 21:36:34,048 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1822
2018-10-11 21:36:34,049 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1989
2018-10-11 21:36:34,049 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1940
2018-10-11 21:36:34,049 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_72_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.0 MB)
2018-10-11 21:36:34,050 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1550
2018-10-11 21:36:34,051 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_96_piece0 on 192.168.0.206:34485 in memory (size: 5.5 KB, free: 1391.0 MB)
2018-10-11 21:36:34,054 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1662
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1588
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1635
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2110
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2102
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1683
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1755
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1759
2018-10-11 21:36:34,055 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2104
2018-10-11 21:36:34,056 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 46 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2049
2018-10-11 21:36:34,056 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 64 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1693
2018-10-11 21:36:34,056 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1586
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1894
2018-10-11 21:36:34,056 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1626
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1553
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1652
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1944
2018-10-11 21:36:34,056 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1650
2018-10-11 21:36:34,056 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 64 (MapPartitionsRDD[203] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 12
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1667
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1859
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2039
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1776
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2070
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 11
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1936
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1980
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1587
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1533
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1920
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2069
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1495
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1536
2018-10-11 21:36:34,057 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1918
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1518
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1812
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1523
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1728
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1481
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1785
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1954
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1714
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1962
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1555
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1716
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1735
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2061
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2017
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2063
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1791
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2085
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1527
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1711
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1764
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1567
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1504
2018-10-11 21:36:34,058 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2011
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1733
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1499
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1634
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1706
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2038
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2050
2018-10-11 21:36:34,059 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1488
2018-10-11 21:36:34,059 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_71_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.0 MB)
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1837
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1676
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2008
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1800
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2107
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1590
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1834
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2106
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1722
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1840
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1796
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2098
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1875
2018-10-11 21:36:34,060 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2006
2018-10-11 21:36:34,061 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1664
2018-10-11 21:36:34,061 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1858
2018-10-11 21:36:34,061 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1884
2018-10-11 21:36:34,061 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1982
2018-10-11 21:36:34,061 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1815
2018-10-11 21:36:34,061 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1889
2018-10-11 21:36:34,061 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_84_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.0 MB)
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1881
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1832
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1694
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1927
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1482
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2005
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1604
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1646
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1698
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1802
2018-10-11 21:36:34,062 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1932
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2078
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2086
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1996
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1599
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1557
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1537
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1974
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2064
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1836
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1871
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1577
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1546
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1614
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2022
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1574
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1702
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1648
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1845
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1934
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1615
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1565
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1909
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1487
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1900
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1492
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1511
2018-10-11 21:36:34,063 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1931
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1921
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1682
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1997
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1747
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1543
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1483
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1830
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1730
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1842
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1825
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1841
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1687
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1695
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1938
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1532
2018-10-11 21:36:34,064 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1661
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2111
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1862
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1594
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2059
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1591
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1958
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1829
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1689
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1809
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1628
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1721
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1957
2018-10-11 21:36:34,065 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1573
2018-10-11 21:36:34,066 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1984
2018-10-11 21:36:34,066 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_78_piece0 on 192.168.0.206:34485 in memory (size: 5.4 KB, free: 1391.0 MB)
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1945
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1937
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2021
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1607
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1782
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1750
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1946
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2091
2018-10-11 21:36:34,067 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1869
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1556
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2018
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2096
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2027
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1925
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2012
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1787
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2083
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1814
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1770
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2090
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1973
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1500
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1763
2018-10-11 21:36:34,068 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1538
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2052
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1844
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1968
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1617
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2105
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1852
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1873
2018-10-11 21:36:34,069 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1622
2018-10-11 21:36:34,070 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_76_piece0 on 192.168.0.206:34485 in memory (size: 46.7 KB, free: 1391.1 MB)
2018-10-11 21:36:34,070 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_98 stored as values in memory (estimated size 131.0 KB, free 1388.7 MB)
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1726
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1903
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1939
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1839
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1757
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2073
2018-10-11 21:36:34,071 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1534
2018-10-11 21:36:34,072 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_98_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1388.8 MB)
2018-10-11 21:36:34,072 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2084
2018-10-11 21:36:34,072 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_98_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.0 MB)
2018-10-11 21:36:34,072 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 98 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,073 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_94_piece0 on 192.168.0.206:34485 in memory (size: 46.7 KB, free: 1391.1 MB)
2018-10-11 21:36:34,073 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[203] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:34,073 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 64.0 with 1 tasks
2018-10-11 21:36:34,073 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1780
2018-10-11 21:36:34,073 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1602
2018-10-11 21:36:34,073 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1601
2018-10-11 21:36:34,073 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1478
2018-10-11 21:36:34,073 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1609
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1535
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1760
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1480
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1659
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1625
2018-10-11 21:36:34,074 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 64.0 (TID 99, localhost, executor driver, partition 0, PROCESS_LOCAL, 10313 bytes)
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1501
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1995
2018-10-11 21:36:34,074 INFO [Executor task launch worker for task 99] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 64.0 (TID 99)
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1541
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1516
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1685
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1981
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1496
2018-10-11 21:36:34,074 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1956
2018-10-11 21:36:34,075 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1530
2018-10-11 21:36:34,075 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1734
2018-10-11 21:36:34,075 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1906
2018-10-11 21:36:34,075 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2002
2018-10-11 21:36:34,075 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1701
2018-10-11 21:36:34,075 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2045
2018-10-11 21:36:34,076 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_74_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.1 MB)
2018-10-11 21:36:34,080 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1998
2018-10-11 21:36:34,080 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1966
2018-10-11 21:36:34,080 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1783
2018-10-11 21:36:34,080 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1700
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1725
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1529
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2044
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1710
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2043
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1616
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1743
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1564
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1883
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2056
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1549
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1643
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1795
2018-10-11 21:36:34,081 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1559
2018-10-11 21:36:34,082 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_85_piece0 on 192.168.0.206:34485 in memory (size: 8.1 KB, free: 1391.1 MB)
2018-10-11 21:36:34,082 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2047
2018-10-11 21:36:34,082 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1663
2018-10-11 21:36:34,082 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1895
2018-10-11 21:36:34,082 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1554
2018-10-11 21:36:34,083 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1772
2018-10-11 21:36:34,083 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2092
2018-10-11 21:36:34,083 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1738
2018-10-11 21:36:34,083 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1860
2018-10-11 21:36:34,083 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1678
2018-10-11 21:36:34,084 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_91_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.1 MB)
2018-10-11 21:36:34,084 INFO [Executor task launch worker for task 99] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,084 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1761
2018-10-11 21:36:34,084 INFO [Executor task launch worker for task 99] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,084 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2007
2018-10-11 21:36:34,084 INFO [Executor task launch worker for task 99] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:34,084 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1922
2018-10-11 21:36:34,084 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1690
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:34,085 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1765
2018-10-11 21:36:34,085 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1633
2018-10-11 21:36:34,085 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2071
2018-10-11 21:36:34,085 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1888
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:34,085 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:34,086 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:34,086 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:34,086 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:34,086 INFO [Executor task launch worker for task 99] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:34,087 INFO [Executor task launch worker for task 99] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:34,085 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 15
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1753
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1833
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1611
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1639
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1959
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1603
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1484
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1977
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1774
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1846
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1657
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1865
2018-10-11 21:36:34,089 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1673
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1741
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1872
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1816
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1929
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1477
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1655
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1768
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2026
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1592
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1960
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2109
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1568
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1740
2018-10-11 21:36:34,090 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1789
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1843
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2079
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1848
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1566
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1933
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1502
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1910
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2067
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1994
2018-10-11 21:36:34,091 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2010
2018-10-11 21:36:34,092 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_93_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1835
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1748
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1739
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1605
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1878
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1704
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1520
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1593
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1808
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1705
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1983
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1498
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1709
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2000
2018-10-11 21:36:34,093 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1641
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1493
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1917
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1579
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2108
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1679
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1521
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1786
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1905
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1581
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1600
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1924
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1506
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1632
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1781
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1788
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1792
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1674
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1930
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1898
2018-10-11 21:36:34,094 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1672
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1863
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1514
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1544
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2020
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2075
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1877
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1915
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1612
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1630
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2072
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1545
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1596
2018-10-11 21:36:34,095 INFO [Executor task launch worker for task 99] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2249
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2053
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1649
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1744
2018-10-11 21:36:34,095 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1790
2018-10-11 21:36:34,096 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1621
2018-10-11 21:36:34,096 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1746
2018-10-11 21:36:34,096 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2103
2018-10-11 21:36:34,096 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1817
2018-10-11 21:36:34,096 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1509
2018-10-11 21:36:34,096 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1771
2018-10-11 21:36:34,096 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_97_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1767
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1942
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1955
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1803
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1629
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2082
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2100
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1570
2018-10-11 21:36:34,097 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1970
2018-10-11 21:36:34,097 INFO [Executor task launch worker for task 99] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213634_0064_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213634_0064_m_000000
2018-10-11 21:36:34,097 INFO [Executor task launch worker for task 99] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213634_0064_m_000000_0: Committed
2018-10-11 21:36:34,098 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_77_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.1 MB)
2018-10-11 21:36:34,098 INFO [Executor task launch worker for task 99] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 64.0 (TID 99). 1996 bytes result sent to driver
2018-10-11 21:36:34,098 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1491
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1820
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1901
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1640
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1745
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2094
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1867
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2046
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1638
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1666
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2042
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2058
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1519
2018-10-11 21:36:34,099 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 64.0 (TID 99) in 26 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:34,099 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1965
2018-10-11 21:36:34,099 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 64.0, whose tasks have all completed, from pool 
2018-10-11 21:36:34,100 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 64 (insertInto at SparkDataManager.scala:61) finished in 0.043 s
2018-10-11 21:36:34,100 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_75_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:34,100 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 46 finished: insertInto at SparkDataManager.scala:61, took 0.045018 s
2018-10-11 21:36:34,100 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1515
2018-10-11 21:36:34,100 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1610
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1669
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1737
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2015
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1585
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1886
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1828
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1548
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1713
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1773
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2076
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1686
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2089
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1854
2018-10-11 21:36:34,101 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1670
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1823
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2074
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2093
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1732
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1891
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1972
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1531
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1719
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1885
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1964
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1547
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2029
2018-10-11 21:36:34,102 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1914
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1476
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1479
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1620
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1928
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2003
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1813
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1827
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1849
2018-10-11 21:36:34,103 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1807
2018-10-11 21:36:34,104 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2055
2018-10-11 21:36:34,104 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1916
2018-10-11 21:36:34,104 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1952
2018-10-11 21:36:34,104 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_90_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.1 MB)
2018-10-11 21:36:34,105 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1654
2018-10-11 21:36:34,105 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1665
2018-10-11 21:36:34,105 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1723
2018-10-11 21:36:34,105 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1882
2018-10-11 21:36:34,105 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1986
2018-10-11 21:36:34,106 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_88_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.1 MB)
2018-10-11 21:36:34,106 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1624
2018-10-11 21:36:34,106 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_80_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1838
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1853
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1584
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1684
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1766
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2057
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2028
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1896
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1926
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1751
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1558
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1864
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1724
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1777
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1806
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1912
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1935
2018-10-11 21:36:34,107 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1598
2018-10-11 21:36:34,108 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:34,108 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_95_piece0 on 192.168.0.206:34485 in memory (size: 21.4 KB, free: 1391.2 MB)
2018-10-11 21:36:34,108 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:34,108 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 17
2018-10-11 21:36:34,109 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1512
2018-10-11 21:36:34,109 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1618
2018-10-11 21:36:34,109 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1528
2018-10-11 21:36:34,109 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_87_piece0 on 192.168.0.206:34485 in memory (size: 46.3 KB, free: 1391.3 MB)
2018-10-11 21:36:34,110 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2036
2018-10-11 21:36:34,110 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1540
2018-10-11 21:36:34,110 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1551
2018-10-11 21:36:34,110 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1571
2018-10-11 21:36:34,111 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_79_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:34,112 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1692
2018-10-11 21:36:34,112 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1623
2018-10-11 21:36:34,112 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1681
2018-10-11 21:36:34,113 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1644
2018-10-11 21:36:34,113 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1805
2018-10-11 21:36:34,114 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_86_piece0 on 192.168.0.206:34485 in memory (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:34,114 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1742
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2054
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1919
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1490
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1513
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1824
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1608
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1525
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1580
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1754
2018-10-11 21:36:34,115 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1718
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2095
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1497
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1729
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1489
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1758
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1874
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1517
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1736
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1976
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1826
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1904
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1951
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1627
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1552
2018-10-11 21:36:34,116 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1597
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1908
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1671
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1819
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1821
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1971
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1749
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1987
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1494
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1539
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2025
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1923
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1524
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1762
2018-10-11 21:36:34,117 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1868
2018-10-11 21:36:34,118 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1636
2018-10-11 21:36:34,118 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1941
2018-10-11 21:36:34,118 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1720
2018-10-11 21:36:34,118 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_89_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.3 MB)
2018-10-11 21:36:34,119 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1991
2018-10-11 21:36:34,119 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 13
2018-10-11 21:36:34,119 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1699
2018-10-11 21:36:34,120 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_83_piece0 on 192.168.0.206:34485 in memory (size: 201.0 B, free: 1391.3 MB)
2018-10-11 21:36:34,120 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2060
2018-10-11 21:36:34,120 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1572
2018-10-11 21:36:34,120 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2019
2018-10-11 21:36:34,120 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1999
2018-10-11 21:36:34,120 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2031
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1798
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1799
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1794
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1892
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1651
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1752
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1811
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1575
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2014
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1784
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1856
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1658
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2041
2018-10-11 21:36:34,121 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1583
2018-10-11 21:36:34,122 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1595
2018-10-11 21:36:34,122 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1510
2018-10-11 21:36:34,122 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1818
2018-10-11 21:36:34,122 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1911
2018-10-11 21:36:34,122 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1943
2018-10-11 21:36:34,122 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1963
2018-10-11 21:36:34,123 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:34,123 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2016
2018-10-11 21:36:34,123 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:34,123 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2097
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1950
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1769
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2035
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1526
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1947
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1637
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2080
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1949
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1992
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1870
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1668
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1879
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1975
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1569
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1902
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1775
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1656
2018-10-11 21:36:34,124 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1688
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1897
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1560
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1851
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1847
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1988
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1978
2018-10-11 21:36:34,125 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1861
2018-10-11 21:36:34,125 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_73_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2032
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2040
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2004
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1893
2018-10-11 21:36:34,126 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2037
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1967
2018-10-11 21:36:34,126 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,126 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2001
2018-10-11 21:36:34,127 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1589
2018-10-11 21:36:34,127 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2088
2018-10-11 21:36:34,127 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1562
2018-10-11 21:36:34,127 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1680
2018-10-11 21:36:34,127 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2024
2018-10-11 21:36:34,127 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1831
2018-10-11 21:36:34,127 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_70_piece0 on 192.168.0.206:34485 in memory (size: 22.2 KB, free: 1391.3 MB)
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1507
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1961
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2077
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1563
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1715
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1756
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2068
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1731
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1887
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1578
2018-10-11 21:36:34,128 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1660
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1619
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 14
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2023
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2062
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1613
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2033
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1505
2018-10-11 21:36:34,129 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1522
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1793
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1857
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2013
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1542
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1913
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1486
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1606
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1631
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1727
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2051
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1969
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1691
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2065
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1696
2018-10-11 21:36:34,130 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1653
2018-10-11 21:36:34,131 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1810
2018-10-11 21:36:34,131 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1985
2018-10-11 21:36:34,131 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_92_piece0 on 192.168.0.206:34485 in memory (size: 9.8 KB, free: 1391.3 MB)
2018-10-11 21:36:34,132 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1855
2018-10-11 21:36:34,132 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1890
2018-10-11 21:36:34,132 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned shuffle 16
2018-10-11 21:36:34,132 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2101
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2034
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1675
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1677
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1712
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1907
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2030
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2099
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1697
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1993
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1703
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1707
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1717
2018-10-11 21:36:34,133 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1779
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2066
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1645
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1503
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1850
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1880
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1508
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1804
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2048
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2081
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1797
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1801
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1979
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1953
2018-10-11 21:36:34,134 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1990
2018-10-11 21:36:34,135 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Removed broadcast_82_piece0 on 192.168.0.206:34485 in memory (size: 3.9 KB, free: 1391.3 MB)
2018-10-11 21:36:34,137 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,138 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 2087
2018-10-11 21:36:34,138 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,138 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1485
2018-10-11 21:36:34,138 INFO [Spark Context Cleaner] o.a.s.ContextCleaner [Logging.scala:54] Cleaned accumulator 1876
2018-10-11 21:36:34,145 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,145 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,155 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:34,155 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:34,178 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:34,178 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 3001645
2018-10-11 21:36:34,219 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:34,219 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:34,222 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:34,222 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:34,232 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=test1
2018-10-11 21:36:34,233 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=test1	
2018-10-11 21:36:34,256 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:34,257 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:34,257 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:34,257 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:34,257 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:34,258 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:34,258 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<>
2018-10-11 21:36:34,258 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:34,265 INFO [broadcast-exchange-3] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_99 stored as values in memory (estimated size 226.1 KB, free 1390.8 MB)
2018-10-11 21:36:34,274 INFO [broadcast-exchange-3] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_99_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.7 MB)
2018-10-11 21:36:34,274 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_99_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:34,275 INFO [broadcast-exchange-3] o.a.s.SparkContext [Logging.scala:54] Created broadcast 99 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:34,275 INFO [broadcast-exchange-3] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:34,281 INFO [broadcast-exchange-3] o.a.s.SparkContext [Logging.scala:54] Starting job: run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:34,282 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 47 (run at ThreadPoolExecutor.java:1149) with 1 output partitions
2018-10-11 21:36:34,282 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 65 (run at ThreadPoolExecutor.java:1149)
2018-10-11 21:36:34,282 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:34,283 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:34,283 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 65 (MapPartitionsRDD[207] at run at ThreadPoolExecutor.java:1149), which has no missing parents
2018-10-11 21:36:34,284 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_100 stored as values in memory (estimated size 7.5 KB, free 1390.7 MB)
2018-10-11 21:36:34,285 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_100_piece0 stored as bytes in memory (estimated size 3.9 KB, free 1390.7 MB)
2018-10-11 21:36:34,286 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_100_piece0 in memory on 192.168.0.206:34485 (size: 3.9 KB, free: 1391.3 MB)
2018-10-11 21:36:34,289 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 100 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,289 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[207] at run at ThreadPoolExecutor.java:1149) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:34,290 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 65.0 with 1 tasks
2018-10-11 21:36:34,291 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 65.0 (TID 100, localhost, executor driver, partition 0, PROCESS_LOCAL, 8375 bytes)
2018-10-11 21:36:34,291 INFO [Executor task launch worker for task 100] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 65.0 (TID 100)
2018-10-11 21:36:34,293 INFO [Executor task launch worker for task 100] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:34,295 INFO [Executor task launch worker for task 100] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 65.0 (TID 100). 1460 bytes result sent to driver
2018-10-11 21:36:34,296 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 65.0 (TID 100) in 5 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:34,296 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 65.0, whose tasks have all completed, from pool 
2018-10-11 21:36:34,297 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 65 (run at ThreadPoolExecutor.java:1149) finished in 0.014 s
2018-10-11 21:36:34,298 INFO [broadcast-exchange-3] o.a.s.s.DAGScheduler [Logging.scala:54] Job 47 finished: run at ThreadPoolExecutor.java:1149, took 0.016332 s
2018-10-11 21:36:34,300 INFO [broadcast-exchange-3] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_101 stored as values in memory (estimated size 5.8 KB, free 1390.7 MB)
2018-10-11 21:36:34,301 INFO [broadcast-exchange-3] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_101_piece0 stored as bytes in memory (estimated size 201.0 B, free 1390.7 MB)
2018-10-11 21:36:34,301 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_101_piece0 in memory on 192.168.0.206:34485 (size: 201.0 B, free: 1391.3 MB)
2018-10-11 21:36:34,302 INFO [broadcast-exchange-3] o.a.s.SparkContext [Logging.scala:54] Created broadcast 101 from run at ThreadPoolExecutor.java:1149
2018-10-11 21:36:34,304 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_102 stored as values in memory (estimated size 226.1 KB, free 1390.5 MB)
2018-10-11 21:36:34,313 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_102_piece0 stored as bytes in memory (estimated size 21.4 KB, free 1390.5 MB)
2018-10-11 21:36:34,314 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_102_piece0 in memory on 192.168.0.206:34485 (size: 21.4 KB, free: 1391.3 MB)
2018-10-11 21:36:34,314 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 102 from collect at Statement.scala:24
2018-10-11 21:36:34,315 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 4195071 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:34,335 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at Statement.scala:24
2018-10-11 21:36:34,335 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Registering RDD 213 (collect at Statement.scala:24)
2018-10-11 21:36:34,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 48 (collect at Statement.scala:24) with 1 output partitions
2018-10-11 21:36:34,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 67 (collect at Statement.scala:24)
2018-10-11 21:36:34,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List(ShuffleMapStage 66)
2018-10-11 21:36:34,336 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List(ShuffleMapStage 66)
2018-10-11 21:36:34,337 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ShuffleMapStage 66 (MapPartitionsRDD[213] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:34,340 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_103 stored as values in memory (estimated size 17.5 KB, free 1390.5 MB)
2018-10-11 21:36:34,341 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_103_piece0 stored as bytes in memory (estimated size 8.1 KB, free 1390.5 MB)
2018-10-11 21:36:34,342 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_103_piece0 in memory on 192.168.0.206:34485 (size: 8.1 KB, free: 1391.3 MB)
2018-10-11 21:36:34,342 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 103 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,343 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ShuffleMapStage 66 (MapPartitionsRDD[213] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:34,343 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 66.0 with 1 tasks
2018-10-11 21:36:34,343 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 66.0 (TID 101, localhost, executor driver, partition 0, PROCESS_LOCAL, 8364 bytes)
2018-10-11 21:36:34,344 INFO [Executor task launch worker for task 101] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 66.0 (TID 101)
2018-10-11 21:36:34,346 INFO [Executor task launch worker for task 101] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/test1/part-00000-cf15e448-0381-4af1-8586-c740d4667e96-c000.snappy.parquet, range: 0-767, partition values: [empty row]
2018-10-11 21:36:34,350 INFO [Executor task launch worker for task 101] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 66.0 (TID 101). 2669 bytes result sent to driver
2018-10-11 21:36:34,350 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 66.0 (TID 101) in 7 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:34,350 INFO [task-result-getter-1] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 66.0, whose tasks have all completed, from pool 
2018-10-11 21:36:34,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ShuffleMapStage 66 (collect at Statement.scala:24) finished in 0.014 s
2018-10-11 21:36:34,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] looking for newly runnable stages
2018-10-11 21:36:34,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] running: Set()
2018-10-11 21:36:34,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] waiting: Set(ResultStage 67)
2018-10-11 21:36:34,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] failed: Set()
2018-10-11 21:36:34,351 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 67 (MapPartitionsRDD[216] at collect at Statement.scala:24), which has no missing parents
2018-10-11 21:36:34,353 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_104 stored as values in memory (estimated size 7.4 KB, free 1390.4 MB)
2018-10-11 21:36:34,353 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_104_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1390.4 MB)
2018-10-11 21:36:34,354 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_104_piece0 in memory on 192.168.0.206:34485 (size: 3.8 KB, free: 1391.3 MB)
2018-10-11 21:36:34,354 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 104 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,355 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[216] at collect at Statement.scala:24) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:34,355 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 67.0 with 1 tasks
2018-10-11 21:36:34,355 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 67.0 (TID 102, localhost, executor driver, partition 0, ANY, 7754 bytes)
2018-10-11 21:36:34,356 INFO [Executor task launch worker for task 102] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 67.0 (TID 102)
2018-10-11 21:36:34,357 INFO [Executor task launch worker for task 102] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Getting 1 non-empty blocks out of 1 blocks
2018-10-11 21:36:34,358 INFO [Executor task launch worker for task 102] o.a.s.s.ShuffleBlockFetcherIterator [Logging.scala:54] Started 0 remote fetches in 1 ms
2018-10-11 21:36:34,359 INFO [Executor task launch worker for task 102] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 67.0 (TID 102). 1782 bytes result sent to driver
2018-10-11 21:36:34,359 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 67.0 (TID 102) in 4 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:34,359 INFO [task-result-getter-2] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 67.0, whose tasks have all completed, from pool 
2018-10-11 21:36:34,360 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 67 (collect at Statement.scala:24) finished in 0.008 s
2018-10-11 21:36:34,360 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 48 finished: collect at Statement.scala:24, took 0.025232 s
2018-10-11 21:36:34,361 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:36:34,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,416 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.p.ParquetFileFormat [Logging.scala:54] Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,431 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: insertInto at SparkDataManager.scala:61
2018-10-11 21:36:34,432 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 49 (insertInto at SparkDataManager.scala:61) with 1 output partitions
2018-10-11 21:36:34,432 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 68 (insertInto at SparkDataManager.scala:61)
2018-10-11 21:36:34,432 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:34,432 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:34,432 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 68 (MapPartitionsRDD[218] at insertInto at SparkDataManager.scala:61), which has no missing parents
2018-10-11 21:36:34,445 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_105 stored as values in memory (estimated size 131.0 KB, free 1390.3 MB)
2018-10-11 21:36:34,446 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_105_piece0 stored as bytes in memory (estimated size 46.3 KB, free 1390.3 MB)
2018-10-11 21:36:34,447 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_105_piece0 in memory on 192.168.0.206:34485 (size: 46.3 KB, free: 1391.2 MB)
2018-10-11 21:36:34,447 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 105 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,448 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[218] at insertInto at SparkDataManager.scala:61) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:34,448 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 68.0 with 1 tasks
2018-10-11 21:36:34,448 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 68.0 (TID 103, localhost, executor driver, partition 0, PROCESS_LOCAL, 10852 bytes)
2018-10-11 21:36:34,449 INFO [Executor task launch worker for task 103] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 68.0 (TID 103)
2018-10-11 21:36:34,457 INFO [Executor task launch worker for task 103] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,457 INFO [Executor task launch worker for task 103] o.a.s.s.e.d.SQLHadoopMapReduceCommitProtocol [Logging.scala:54] Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-10-11 21:36:34,457 INFO [Executor task launch worker for task 103] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:34,457 INFO [Executor task launch worker for task 103] o.a.p.h.c.CodecConfig [CodecConfig.java:95] Compression: SNAPPY
2018-10-11 21:36:34,458 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:329] Parquet block size to 134217728
2018-10-11 21:36:34,458 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:330] Parquet page size to 1048576
2018-10-11 21:36:34,458 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:331] Parquet dictionary page size to 1048576
2018-10-11 21:36:34,458 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:332] Dictionary is on
2018-10-11 21:36:34,458 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:333] Validation is off
2018-10-11 21:36:34,459 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:334] Writer version is: PARQUET_1_0
2018-10-11 21:36:34,459 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:335] Maximum row group padding size is 0 bytes
2018-10-11 21:36:34,459 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:336] Page size checking is: estimated
2018-10-11 21:36:34,459 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:337] Min row count for page size check is: 100
2018-10-11 21:36:34,459 INFO [Executor task launch worker for task 103] o.a.p.h.ParquetOutputFormat [ParquetOutputFormat.java:338] Max row count for page size check is: 10000
2018-10-11 21:36:34,460 INFO [Executor task launch worker for task 103] o.a.s.s.e.d.p.ParquetWriteSupport [Logging.scala:54] Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "statement",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "text",
        "type" : "string",
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "physicalPlanDescription",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

       
2018-10-11 21:36:34,471 INFO [Executor task launch worker for task 103] o.a.p.h.InternalParquetRecordWriter [InternalParquetRecordWriter.java:160] Flushing mem columnStore to file. allocated memory: 2789
2018-10-11 21:36:34,473 INFO [Executor task launch worker for task 103] o.a.h.m.l.o.FileOutputCommitter [FileOutputCommitter.java:439] Saved output of task 'attempt_20181011213634_0068_m_000000_0' to file:/user/hive/warehouse/tpcds.db/spark_tpcds_statements/_temporary/0/task_20181011213634_0068_m_000000
2018-10-11 21:36:34,474 INFO [Executor task launch worker for task 103] o.a.s.m.SparkHadoopMapRedUtil [Logging.scala:54] attempt_20181011213634_0068_m_000000_0: Committed
2018-10-11 21:36:34,474 INFO [Executor task launch worker for task 103] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 68.0 (TID 103). 1996 bytes result sent to driver
2018-10-11 21:36:34,475 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 68.0 (TID 103) in 27 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:34,475 INFO [task-result-getter-3] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 68.0, whose tasks have all completed, from pool 
2018-10-11 21:36:34,475 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 68 (insertInto at SparkDataManager.scala:61) finished in 0.042 s
2018-10-11 21:36:34,476 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 49 finished: insertInto at SparkDataManager.scala:61, took 0.044510 s
2018-10-11 21:36:34,482 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Job null committed.
2018-10-11 21:36:34,483 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileFormatWriter [Logging.scala:54] Finished processing stats for job null.
2018-10-11 21:36:34,496 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:34,496 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:34,499 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,499 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,509 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,510 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,518 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,519 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,531 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements
2018-10-11 21:36:34,531 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=alter_table: db=tpcds tbl=spark_tpcds_statements newtbl=spark_tpcds_statements	
2018-10-11 21:36:34,561 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:217] Updating table stats fast for spark_tpcds_statements
2018-10-11 21:36:34,561 INFO [ScalaTest-run-running-SparkTPCDSTest] hive.log [MetaStoreUtils.java:219] Updated size of table spark_tpcds_statements to 3015347
2018-10-11 21:36:34,602 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:36:34,602 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 21:36:34,603 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,628 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:34,629 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:34,651 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,651 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,660 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:34,660 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:34,662 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:34,662 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:34,671 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_runs
2018-10-11 21:36:34,672 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_runs	
2018-10-11 21:36:34,768 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:34,769 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:34,769 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<name: string, description: string, database: string, executionDateTime: bigint, sparkConfig: map<string,string> ... 1 more field>
2018-10-11 21:36:34,770 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:34,787 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 10.524613 ms
2018-10-11 21:36:34,813 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.c.e.c.CodeGenerator [Logging.scala:54] Code generated in 19.088146 ms
2018-10-11 21:36:34,815 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_106 stored as values in memory (estimated size 230.0 KB, free 1390.0 MB)
2018-10-11 21:36:34,822 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_106_piece0 stored as bytes in memory (estimated size 22.2 KB, free 1390.0 MB)
2018-10-11 21:36:34,822 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_106_piece0 in memory on 192.168.0.206:34485 (size: 22.2 KB, free: 1391.2 MB)
2018-10-11 21:36:34,823 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 106 from collect at SparkDataManager.scala:66
2018-10-11 21:36:34,823 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 25189295 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:34,829 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:66
2018-10-11 21:36:34,830 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 50 (collect at SparkDataManager.scala:66) with 1 output partitions
2018-10-11 21:36:34,830 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 69 (collect at SparkDataManager.scala:66)
2018-10-11 21:36:34,830 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:34,830 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:34,830 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 69 (MapPartitionsRDD[222] at collect at SparkDataManager.scala:66), which has no missing parents
2018-10-11 21:36:34,831 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_107 stored as values in memory (estimated size 31.8 KB, free 1390.0 MB)
2018-10-11 21:36:34,832 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_107_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1390.0 MB)
2018-10-11 21:36:34,832 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_107_piece0 in memory on 192.168.0.206:34485 (size: 10.5 KB, free: 1391.2 MB)
2018-10-11 21:36:34,833 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 107 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,833 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[222] at collect at SparkDataManager.scala:66) (first 15 tasks are for partitions Vector(0))
2018-10-11 21:36:34,833 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 69.0 with 1 tasks
2018-10-11 21:36:34,833 INFO [dispatcher-event-loop-1] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 69.0 (TID 104, localhost, executor driver, partition 0, PROCESS_LOCAL, 9141 bytes)
2018-10-11 21:36:34,834 INFO [Executor task launch worker for task 104] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 69.0 (TID 104)
2018-10-11 21:36:34,837 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-4c42319b-54bf-43db-b419-0d31fcd62ac7-c000.snappy.parquet, range: 0-4578, partition values: [empty row]
2018-10-11 21:36:34,838 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:34,842 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,844 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,844 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,845 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,845 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,845 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f444620a-4c60-4bf8-ba73-d14b8a582171-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:34,846 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:34,849 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,851 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,851 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,852 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,852 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,852 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,852 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-f0dd9d60-31a5-4d88-b4ca-739dc3766876-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:34,854 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:34,858 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,860 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,860 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,860 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,860 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,861 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,861 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-7f906c98-fd19-4a72-b6e6-24c8141a3e20-c000.snappy.parquet, range: 0-4352, partition values: [empty row]
2018-10-11 21:36:34,862 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:34,865 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,867 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,867 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,868 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,868 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,868 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,868 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-e9ec2d02-f00a-4127-979f-465cfc3b4292-c000.snappy.parquet, range: 0-4336, partition values: [empty row]
2018-10-11 21:36:34,869 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:34,873 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,875 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,875 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,876 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,876 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,876 WARN [Executor task launch worker for task 104] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,876 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_runs/part-00000-6d8d8960-9879-4827-9e2b-595e45cfe0f4-c000.snappy.parquet, range: 0-1501, partition values: [empty row]
2018-10-11 21:36:34,877 INFO [Executor task launch worker for task 104] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary name (UTF8);
  optional binary description (UTF8);
  optional binary database (UTF8);
  required int64 executionDateTime;
  optional group sparkConfig (MAP) {
    repeated group key_value {
      required binary key (UTF8);
      optional binary value (UTF8);
    }
  }
  optional group queries (LIST) {
    repeated group list {
      optional group element {
        required int32 id (INT_16);
        optional binary businessQuestion (UTF8);
        required int32 queryClass;
        optional group statements (LIST) {
          repeated group list {
            optional group element {
              optional binary id (UTF8);
              optional binary text (UTF8);
            }
          }
        }
      }
    }
  }
}

Catalyst form:
StructType(StructField(name,StringType,true), StructField(description,StringType,true), StructField(database,StringType,true), StructField(executionDateTime,LongType,true), StructField(sparkConfig,MapType(StringType,StringType,true),true), StructField(queries,ArrayType(StructType(StructField(id,ShortType,true), StructField(businessQuestion,StringType,true), StructField(queryClass,IntegerType,true), StructField(statements,ArrayType(StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true),true)),true),true))
       
2018-10-11 21:36:34,880 INFO [Executor task launch worker for task 104] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 0 records.
2018-10-11 21:36:34,883 INFO [Executor task launch worker for task 104] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 69.0 (TID 104). 2173 bytes result sent to driver
2018-10-11 21:36:34,883 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 69.0 (TID 104) in 50 ms on localhost (executor driver) (1/1)
2018-10-11 21:36:34,883 INFO [task-result-getter-0] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Removed TaskSet 69.0, whose tasks have all completed, from pool 
2018-10-11 21:36:34,883 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] ResultStage 69 (collect at SparkDataManager.scala:66) finished in 0.052 s
2018-10-11 21:36:34,884 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.DAGScheduler [Logging.scala:54] Job 50 finished: collect at SparkDataManager.scala:66, took 0.054124 s
2018-10-11 21:36:34,884 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,885 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,885 WARN [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,886 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_database: tpcds
2018-10-11 21:36:34,886 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_database: tpcds	
2018-10-11 21:36:34,889 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,890 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,897 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.HiveMetaStore [HiveMetaStore.java:746] 0: get_table : db=tpcds tbl=spark_tpcds_statements
2018-10-11 21:36:34,898 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.h.h.m.H.audit [HiveMetaStore.java:371] ugi=aaronvanhecken	ip=unknown-ip-addr	cmd=get_table : db=tpcds tbl=spark_tpcds_statements	
2018-10-11 21:36:34,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Pruning directories with: 
2018-10-11 21:36:34,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Post-Scan Filters: 
2018-10-11 21:36:34,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.d.FileSourceStrategy [Logging.scala:54] Output Data Schema: struct<statement: struct<id: string, text: string>, startTime: bigint, endTime: bigint, description: string, details: string ... 1 more field>
2018-10-11 21:36:34,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Pushed Filters: 
2018-10-11 21:36:34,963 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_108 stored as values in memory (estimated size 228.2 KB, free 1389.8 MB)
2018-10-11 21:36:34,971 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_108_piece0 stored as bytes in memory (estimated size 21.7 KB, free 1389.7 MB)
2018-10-11 21:36:34,971 INFO [dispatcher-event-loop-0] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_108_piece0 in memory on 192.168.0.206:34485 (size: 21.7 KB, free: 1391.2 MB)
2018-10-11 21:36:34,972 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Created broadcast 108 from collect at SparkDataManager.scala:77
2018-10-11 21:36:34,973 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.s.e.FileSourceScanExec [Logging.scala:54] Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.
2018-10-11 21:36:34,980 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.s.SparkContext [Logging.scala:54] Starting job: collect at SparkDataManager.scala:77
2018-10-11 21:36:34,981 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Got job 51 (collect at SparkDataManager.scala:77) with 8 output partitions
2018-10-11 21:36:34,981 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Final stage: ResultStage 70 (collect at SparkDataManager.scala:77)
2018-10-11 21:36:34,981 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Parents of final stage: List()
2018-10-11 21:36:34,981 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Missing parents: List()
2018-10-11 21:36:34,981 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting ResultStage 70 (MapPartitionsRDD[225] at collect at SparkDataManager.scala:77), which has no missing parents
2018-10-11 21:36:34,982 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_109 stored as values in memory (estimated size 16.2 KB, free 1389.7 MB)
2018-10-11 21:36:34,983 INFO [dag-scheduler-event-loop] o.a.s.s.m.MemoryStore [Logging.scala:54] Block broadcast_109_piece0 stored as bytes in memory (estimated size 6.7 KB, free 1389.7 MB)
2018-10-11 21:36:34,983 INFO [dispatcher-event-loop-1] o.a.s.s.BlockManagerInfo [Logging.scala:54] Added broadcast_109_piece0 in memory on 192.168.0.206:34485 (size: 6.7 KB, free: 1391.2 MB)
2018-10-11 21:36:34,984 INFO [dag-scheduler-event-loop] o.a.s.SparkContext [Logging.scala:54] Created broadcast 109 from broadcast at DAGScheduler.scala:1039
2018-10-11 21:36:34,984 INFO [dag-scheduler-event-loop] o.a.s.s.DAGScheduler [Logging.scala:54] Submitting 8 missing tasks from ResultStage 70 (MapPartitionsRDD[225] at collect at SparkDataManager.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
2018-10-11 21:36:34,984 INFO [dag-scheduler-event-loop] o.a.s.s.TaskSchedulerImpl [Logging.scala:54] Adding task set 70.0 with 8 tasks
2018-10-11 21:36:34,984 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 0.0 in stage 70.0 (TID 105, localhost, executor driver, partition 0, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:34,985 INFO [Executor task launch worker for task 105] o.a.s.e.Executor [Logging.scala:54] Running task 0.0 in stage 70.0 (TID 105)
2018-10-11 21:36:34,986 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e31abc9-a228-4133-b02e-0f6cd7638510-c000.snappy.parquet, range: 0-13723, partition values: [empty row]
2018-10-11 21:36:34,987 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:34,988 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,989 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,989 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,990 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,990 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b1982672-2d41-4bff-8f45-9e6f8c921d5d-c000.snappy.parquet, range: 0-13722, partition values: [empty row]
2018-10-11 21:36:34,990 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:34,991 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,992 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,992 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,993 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,993 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e5316a0-c580-49fd-91d0-550955cd9bbc-c000.snappy.parquet, range: 0-13721, partition values: [empty row]
2018-10-11 21:36:34,993 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:34,994 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,995 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,995 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,995 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,996 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e15f7475-849c-45f2-9dcf-5bb2d5b69bd4-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:34,996 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:34,997 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:34,998 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:34,998 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:34,998 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:34,998 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a46c4b1e-6e4f-4382-a659-662347269774-c000.snappy.parquet, range: 0-13719, partition values: [empty row]
2018-10-11 21:36:34,999 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,000 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,001 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,001 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,001 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,001 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c7514033-0170-45b0-ad15-cb3097418dfb-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:35,002 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,004 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,006 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,006 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,006 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,006 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3723b14f-e12a-4f5a-b016-809b54de139c-c000.snappy.parquet, range: 0-13706, partition values: [empty row]
2018-10-11 21:36:35,007 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,008 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,009 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,009 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,009 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,009 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-07c85578-afbb-490a-bd9d-3837ab2a7baa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:35,010 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,011 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,011 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,012 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,012 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,012 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-603f4016-bd22-4ed6-b2c7-be69003d48fa-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:35,013 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,014 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,014 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,014 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,015 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,015 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a6043ae9-7f4d-4793-ad20-a149ce2339bb-c000.snappy.parquet, range: 0-13704, partition values: [empty row]
2018-10-11 21:36:35,015 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,016 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,017 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,017 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,017 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,018 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b8d65fb3-afa1-45c2-9354-709d30d7ae22-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:35,018 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,019 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,020 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,020 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,021 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,021 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c8f8e344-f81f-44df-b175-bab47acc8b64-c000.snappy.parquet, range: 0-13702, partition values: [empty row]
2018-10-11 21:36:35,022 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,023 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,023 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,024 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,024 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,024 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0e52f33e-2792-451e-bfca-4fb2393d1315-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:35,024 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,025 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,026 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,026 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,026 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,027 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e001f918-e226-4b11-a792-e057310c71dc-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:35,027 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,028 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,029 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,029 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,029 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,029 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a17d8d1-3cec-4462-90f4-c0c77d76c5ce-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:35,030 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,031 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,032 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,032 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,032 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,032 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7992780f-0fa1-4c14-beca-90350009f018-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:35,033 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,034 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,035 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,035 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,035 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,035 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d57f919b-2b39-4c28-b8f3-61f31041cb83-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:35,036 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,037 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,038 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,038 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,038 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,038 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-735e4a0c-71b2-4dfe-9652-eddb95823bd0-c000.snappy.parquet, range: 0-13701, partition values: [empty row]
2018-10-11 21:36:35,039 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,040 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,041 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,041 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,042 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,042 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6e132893-66ca-44f4-810d-b31bd307f3bf-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:35,043 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,044 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,044 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,045 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,045 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,045 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e32d4e0b-152b-41f5-9d5d-6319207c046a-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:35,046 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,047 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,047 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,048 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,048 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,048 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d24eaa68-02d1-43c1-b2da-ef94148610ed-c000.snappy.parquet, range: 0-13700, partition values: [empty row]
2018-10-11 21:36:35,049 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,049 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,050 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,050 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,051 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,051 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad3d0b86-19cf-474c-b3d4-796d7afaa4ae-c000.snappy.parquet, range: 0-13699, partition values: [empty row]
2018-10-11 21:36:35,052 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,053 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,054 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,054 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,055 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,055 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0247bc41-a2af-4b30-a5e0-5bc327f47cc2-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:35,056 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,057 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,058 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,058 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,058 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,058 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-26318166-52ec-4313-a4e5-3dc130f82d18-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:35,059 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,060 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,060 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,061 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,061 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,061 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2227df14-fd69-4e52-88fc-ffa906e8e96a-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:35,061 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,063 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,063 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,063 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,064 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,064 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-16e0e1c7-4724-40d4-8fe9-64c573377aac-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:35,065 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,066 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,066 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,067 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,067 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,067 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-12cc2ef7-5c98-45c7-bfca-9f6852b4f9cb-c000.snappy.parquet, range: 0-13698, partition values: [empty row]
2018-10-11 21:36:35,068 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,069 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,069 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,070 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,070 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,070 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a42f91ed-6325-465e-92ab-b01b65ee69f2-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:35,071 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,072 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,073 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,073 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,073 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,073 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c5704978-3346-4cf0-a83a-fd67411e60c0-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:35,074 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,075 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,075 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,076 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,076 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,076 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-611aa927-9eff-4acc-a956-1419efe00229-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:35,077 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,078 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,079 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,079 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,079 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,079 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b6173355-0a8b-414c-9ed0-27985f88914d-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:35,080 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,082 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,083 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,083 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,084 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,084 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb54fd41-c4a7-41e8-a051-2493b4fc4fed-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:35,085 INFO [Executor task launch worker for task 105] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,087 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,088 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,088 INFO [Executor task launch worker for task 105] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,088 WARN [Executor task launch worker for task 105] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,089 INFO [Executor task launch worker for task 105] o.a.s.e.Executor [Logging.scala:54] Finished task 0.0 in stage 70.0 (TID 105). 1423 bytes result sent to driver
2018-10-11 21:36:35,090 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 1.0 in stage 70.0 (TID 106, localhost, executor driver, partition 1, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:35,090 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 0.0 in stage 70.0 (TID 105) in 106 ms on localhost (executor driver) (1/8)
2018-10-11 21:36:35,090 INFO [Executor task launch worker for task 106] o.a.s.e.Executor [Logging.scala:54] Running task 1.0 in stage 70.0 (TID 106)
2018-10-11 21:36:35,092 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9af47b28-32d6-4d91-b198-acee5bfe01dd-c000.snappy.parquet, range: 0-13697, partition values: [empty row]
2018-10-11 21:36:35,093 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,094 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,095 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,096 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,096 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,096 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-db7aecb7-d25c-4a59-ad0e-2bacf5772cb8-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,097 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,098 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,099 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,099 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,099 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,099 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ae6870f1-1f08-443f-8701-2a8a8fc509da-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,100 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,102 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,103 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,103 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,103 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,103 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d164cc8-eee4-4f35-920d-9fa0984dbb40-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,104 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,105 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,106 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,107 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,107 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,107 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b48d131c-6bd6-478f-9abc-96bd891b0584-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,108 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,109 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,110 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,110 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,111 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,111 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fa252a4c-1080-4e94-9dc5-d3e35b8e70ea-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,112 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,113 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,113 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,114 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,114 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,114 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-586baf62-c0dd-443b-a8db-5822599f893a-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,115 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,116 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,117 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,117 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,117 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,117 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5ae7e692-d21c-4b25-9cdf-83f59ef40407-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,118 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,120 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,121 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,121 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,122 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,122 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5d04bbb7-c918-4d38-9348-7833d532fb6c-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,123 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,124 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,125 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,126 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,126 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,126 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-178dd25a-2181-4b65-970b-0683491eca83-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,127 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,128 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,129 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,129 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,129 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,129 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-90d7046c-97fc-4066-9720-fc9230ba0c54-c000.snappy.parquet, range: 0-13696, partition values: [empty row]
2018-10-11 21:36:35,130 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,131 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,132 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,132 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,133 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,133 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2bc1210e-eaa6-47cf-ba41-7e0b223d190f-c000.snappy.parquet, range: 0-13695, partition values: [empty row]
2018-10-11 21:36:35,134 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,135 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,136 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,137 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,137 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,137 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e7cb913a-50de-4609-aada-ae4eafd1de31-c000.snappy.parquet, range: 0-13694, partition values: [empty row]
2018-10-11 21:36:35,138 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,139 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,140 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,140 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,141 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,141 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f1d46819-9d92-46ba-9025-2adb0818719c-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:35,142 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,143 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,144 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,144 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,144 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,144 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b85e87fe-5382-4ea3-895f-13f9f032ef4b-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:35,145 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,146 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,147 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,147 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,147 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,147 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3831987b-b4b7-4617-a261-5bf721ece358-c000.snappy.parquet, range: 0-13693, partition values: [empty row]
2018-10-11 21:36:35,148 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,149 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,150 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,150 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,150 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,150 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-21e4180a-5c27-419b-ab1c-78aa6b2cb324-c000.snappy.parquet, range: 0-13691, partition values: [empty row]
2018-10-11 21:36:35,151 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,152 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,153 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,154 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,154 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,154 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c33bde23-fc3e-4548-a230-3eb78aac57fb-c000.snappy.parquet, range: 0-13690, partition values: [empty row]
2018-10-11 21:36:35,155 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,156 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,157 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,157 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,157 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,158 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0232792-2703-4cd0-a57b-e87871b66a88-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,158 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,160 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,160 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,161 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,161 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,161 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6f55e7b2-ed9f-4a3f-9a54-06887558e22d-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,162 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,163 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,164 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,165 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,165 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,165 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4384b7b2-8b67-4077-a182-89d2e37a4833-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,166 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,167 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,168 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,168 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,169 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,169 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19b79ab5-3105-4028-bbd0-c35b902de4a4-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,170 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,172 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,173 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,174 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,174 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,174 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6d920ff1-e3fc-40ac-a84e-386f13c16e67-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,175 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,176 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,177 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,177 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,177 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,177 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d63edb8d-fac0-4840-b019-b0b6e7a639d6-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,178 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,179 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,180 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,181 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,181 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,181 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-09613236-c7ea-4e03-a2f6-c2dd85f36bed-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,182 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,183 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,184 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,184 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,184 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,184 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aafd0534-3203-4405-a282-194c9a55812b-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,185 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,186 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,188 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,188 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,188 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,188 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-018f2c55-3be6-41b0-b9cd-c3e9619e0cda-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,189 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,191 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,192 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,192 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,193 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,193 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b9578903-dcf4-4d51-9126-5a90dd27a1ef-c000.snappy.parquet, range: 0-13673, partition values: [empty row]
2018-10-11 21:36:35,194 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,195 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,195 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,196 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,196 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,196 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9f7a9a62-4bad-4d7b-8940-549d39238770-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:35,197 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,198 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,199 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,199 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,199 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,199 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8b0e504c-c3cf-496a-8730-d46b0b7df1b9-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:35,200 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,201 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,202 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,202 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,202 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,202 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-79b8fe3f-ef98-4c64-bedc-86c3fb9283fd-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:35,203 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,205 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,206 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,206 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,206 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,206 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57dddecf-aa33-4874-99e6-9748f780fe78-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:35,207 INFO [Executor task launch worker for task 106] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,208 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,209 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,209 INFO [Executor task launch worker for task 106] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,209 WARN [Executor task launch worker for task 106] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,210 INFO [Executor task launch worker for task 106] o.a.s.e.Executor [Logging.scala:54] Finished task 1.0 in stage 70.0 (TID 106). 1423 bytes result sent to driver
2018-10-11 21:36:35,210 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 2.0 in stage 70.0 (TID 107, localhost, executor driver, partition 2, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:35,210 INFO [Executor task launch worker for task 107] o.a.s.e.Executor [Logging.scala:54] Running task 2.0 in stage 70.0 (TID 107)
2018-10-11 21:36:35,210 INFO [task-result-getter-2] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 1.0 in stage 70.0 (TID 106) in 120 ms on localhost (executor driver) (2/8)
2018-10-11 21:36:35,212 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7bfc2fc2-2b64-4ee4-83ae-ad0b645e1322-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:35,212 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,214 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,214 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,215 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,215 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,215 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6a928c40-d93a-4600-a43e-d6259a8a7c26-c000.snappy.parquet, range: 0-13672, partition values: [empty row]
2018-10-11 21:36:35,216 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,217 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,218 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,218 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,218 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,218 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-62170ceb-3786-48a8-93d4-b567247e8037-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:35,219 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,220 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,221 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,221 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,221 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,221 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8643ee33-f4b3-4b9d-a32e-39e43cfe4ba7-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:35,222 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,223 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,224 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,224 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,224 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,224 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0680d8e3-d6a3-46d1-b2d8-76df28d3b02a-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:35,225 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,226 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,227 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,227 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,227 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,227 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-aae2da02-ce8a-4dad-87c4-6a12978949be-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:35,228 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,229 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,230 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,230 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,230 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,230 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8eae07df-593a-43cc-ac4a-2be464a53d3f-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:35,231 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,232 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,233 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,233 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,233 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,234 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a663e89f-b19d-4f56-bfd7-d844367b5dc2-c000.snappy.parquet, range: 0-13671, partition values: [empty row]
2018-10-11 21:36:35,234 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,236 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,236 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,237 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,237 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,237 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2318bf98-30f1-4cb7-89ff-f54e8f26ee77-c000.snappy.parquet, range: 0-13670, partition values: [empty row]
2018-10-11 21:36:35,238 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,239 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,239 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,240 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,240 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,240 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-854fe582-8eec-43ce-b79a-f5a9b836d2fd-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,241 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,242 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,243 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,243 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,243 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,244 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5793a580-0e06-4393-afec-1a4eefbbd626-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,244 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,245 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,246 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,246 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,247 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,247 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e7b564a-5bd2-4701-bdea-b107946c9b0c-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,247 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,248 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,249 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,249 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,249 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,249 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d10c8980-7d22-4a6d-99ec-053805bdf9ec-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,250 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,251 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,252 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,252 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,252 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,252 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-59858d75-788e-4333-9577-bd4947258eff-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,253 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,255 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,256 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,256 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,256 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,257 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-274749d2-43f1-4c5d-81fe-d888dc9dff47-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,257 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,258 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,259 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,259 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,260 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,260 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ddde392-a677-4c60-92fc-49cd2be2c0f1-c000.snappy.parquet, range: 0-13669, partition values: [empty row]
2018-10-11 21:36:35,260 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,262 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,262 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,262 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,263 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,263 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-b273f389-8ffb-4cbf-b108-6506095ae632-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:35,263 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,264 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,265 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,265 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,266 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,266 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7233e2d1-9862-45dc-9824-4e41f2c2867e-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:35,266 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,267 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,268 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,268 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,269 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,269 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-86a299de-f8fc-46fe-86a9-60a4b26a9b38-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:35,270 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,271 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,271 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,272 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,272 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,272 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-defaedb6-dcf3-4709-916c-a3e955107f58-c000.snappy.parquet, range: 0-13668, partition values: [empty row]
2018-10-11 21:36:35,273 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,274 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,275 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,275 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,275 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,275 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a837d014-9d5d-454e-a7f6-8f2fb06c37d0-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:35,276 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,277 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,278 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,278 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,278 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,278 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-45a99307-69fd-4a6c-8a0c-91d96b83817a-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:35,279 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,280 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,281 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,281 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,281 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,281 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-29b6c682-0e21-4836-904c-33bbd018067c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:35,282 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,284 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,285 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,285 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,285 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,285 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bbb741a3-045d-451c-a803-96b4d0493b04-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:35,286 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,287 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,288 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,288 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,289 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,289 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3e33d9f4-1cd5-43f7-b792-6d744d0a7505-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:35,290 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,292 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,293 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,293 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,294 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,294 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-71b8e55a-40c5-4b5c-9dae-af4d5551ef2c-c000.snappy.parquet, range: 0-13661, partition values: [empty row]
2018-10-11 21:36:35,295 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,296 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,297 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,297 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,297 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,298 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e2d6c34-9f3a-4d0c-9b3f-d0afa5e7e097-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,298 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,299 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,300 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,300 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,301 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,301 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8aadda0f-0463-4bfe-9f32-62f122ef3ff4-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,302 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,304 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,305 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,305 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,306 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,306 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-bea86d33-775a-4e73-97cd-927d76b9352f-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,307 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,309 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,310 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,310 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,311 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,311 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9e0852e5-e63c-45eb-912a-d3964270da43-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,312 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,314 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,315 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,315 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,316 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,316 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4cc621ee-7546-4d9f-8ba0-c4c66f0adeec-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,317 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,319 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,320 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,320 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,321 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,321 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-58131256-2685-47d1-abb5-a4357c095a67-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,322 INFO [Executor task launch worker for task 107] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,324 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,325 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,326 INFO [Executor task launch worker for task 107] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,326 WARN [Executor task launch worker for task 107] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,326 INFO [Executor task launch worker for task 107] o.a.s.e.Executor [Logging.scala:54] Finished task 2.0 in stage 70.0 (TID 107). 1423 bytes result sent to driver
2018-10-11 21:36:35,327 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 3.0 in stage 70.0 (TID 108, localhost, executor driver, partition 3, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:35,327 INFO [task-result-getter-3] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 2.0 in stage 70.0 (TID 107) in 117 ms on localhost (executor driver) (3/8)
2018-10-11 21:36:35,327 INFO [Executor task launch worker for task 108] o.a.s.e.Executor [Logging.scala:54] Running task 3.0 in stage 70.0 (TID 108)
2018-10-11 21:36:35,329 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a5a52bbe-0c0c-41d6-a8be-9839d1629b37-c000.snappy.parquet, range: 0-13660, partition values: [empty row]
2018-10-11 21:36:35,330 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,331 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,333 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,333 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,333 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,334 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-869a108e-1223-444d-9a58-a852afc71fdc-c000.snappy.parquet, range: 0-13658, partition values: [empty row]
2018-10-11 21:36:35,335 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,336 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,338 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,338 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,339 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,339 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-666a2e39-edd2-4992-85ee-cd58cc019c85-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:35,340 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,341 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,342 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,343 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,343 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,343 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca11e9c0-3060-4a86-91dd-db9ce2ca7e5f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:35,344 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,345 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,346 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,346 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,347 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,347 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4a6558fd-1e62-4e07-9df1-400871fc5cec-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:35,348 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,349 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,350 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,350 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,350 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,350 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-19122b21-1ceb-4cf4-8593-9fa7c8624d2d-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:35,351 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,353 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,354 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,355 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,355 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,355 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d7ef5b02-cddb-495a-aad3-f3679ca76b8f-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:35,357 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,359 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,360 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,360 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,361 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,361 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-694f93cb-34fe-4247-821f-f5ca506e1459-c000.snappy.parquet, range: 0-13657, partition values: [empty row]
2018-10-11 21:36:35,362 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,363 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,364 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,365 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,365 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,365 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-701ae0c0-29df-4b97-839c-9c83df4e1b68-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:35,367 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,368 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,370 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,371 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,371 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,371 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5b6171fb-919a-4c7b-98aa-5dfe9d454475-c000.snappy.parquet, range: 0-11395, partition values: [empty row]
2018-10-11 21:36:35,372 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,374 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,375 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,375 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,376 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,376 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d21bcc-601f-4e42-b182-081c470c659d-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:35,376 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,378 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,379 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,379 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,379 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,380 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-600d8f03-7aac-4957-baf4-c554ff43494a-c000.snappy.parquet, range: 0-11394, partition values: [empty row]
2018-10-11 21:36:35,380 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,381 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,382 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,382 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,383 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,383 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e02ee93e-e796-42b0-bf02-5f75dfdc1c6c-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:35,384 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,385 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,386 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,386 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,387 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,387 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a90b0f72-7d9a-4a49-a68b-f290f02aa529-c000.snappy.parquet, range: 0-11393, partition values: [empty row]
2018-10-11 21:36:35,388 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,389 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,390 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,391 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,391 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,391 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f41d2183-74d9-4df5-be81-a2fb23bf9de7-c000.snappy.parquet, range: 0-11392, partition values: [empty row]
2018-10-11 21:36:35,392 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,394 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,394 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,395 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,395 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,395 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3fa69d1c-3edf-45d2-b0ac-1272168f52c9-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:35,396 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,397 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,398 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,398 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,399 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,399 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a0b202fc-2bf1-495f-9764-acfb0429a5c8-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:35,400 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,401 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,402 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,402 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,403 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,403 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-936820e3-b496-4cc2-8571-9635f4fdbfa3-c000.snappy.parquet, range: 0-11391, partition values: [empty row]
2018-10-11 21:36:35,405 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,406 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,408 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,409 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,409 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,409 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-eb4c07ef-088b-4a15-be2f-b273fd27d040-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:35,410 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,412 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,413 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,414 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,414 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,414 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c4d67cb5-e9dc-4b19-acea-905574bf8522-c000.snappy.parquet, range: 0-11381, partition values: [empty row]
2018-10-11 21:36:35,416 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,417 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,419 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,419 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,419 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,420 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-224b809f-76f3-4919-a9b8-43b820662112-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:35,421 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,422 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,423 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,423 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,424 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,424 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-7501282c-59f0-4b1a-88f5-82366eca768c-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:35,425 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,426 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,426 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,427 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,427 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,427 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3cad3b64-e9c9-41b4-a422-dc665b78c2c1-c000.snappy.parquet, range: 0-11380, partition values: [empty row]
2018-10-11 21:36:35,428 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,429 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,430 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,430 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,430 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,430 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-390c15b5-d721-4535-be3f-12b05cfbe30a-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:35,431 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,432 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,433 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,433 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,434 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,434 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05d28ee0-4831-43b1-a27f-dab415ede7a0-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:35,435 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,437 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,438 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,438 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,439 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,439 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-56056fe1-8392-42d6-b2c1-4463f7381740-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:35,440 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,441 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,442 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,442 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,442 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,442 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dd3999fe-68a8-43bd-b968-d1abe12b7d68-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:35,443 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,444 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,445 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,445 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,446 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,446 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-40f0422a-c31f-4b28-870e-1da30f3d4173-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:35,447 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,449 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,450 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,451 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,451 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,451 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-81102eef-ee91-4905-a219-3a7d6ad68001-c000.snappy.parquet, range: 0-11379, partition values: [empty row]
2018-10-11 21:36:35,452 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,453 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,454 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,455 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,455 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,455 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a7708ac5-ff0d-4d9b-b829-432325d537bd-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:35,456 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,457 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,458 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,458 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,458 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,458 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-de923178-e27e-43df-9376-a8765b42b212-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:35,459 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,460 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,461 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,461 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,462 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,462 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2366af80-dffe-477b-8217-7f704825188b-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:35,462 INFO [Executor task launch worker for task 108] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,463 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,464 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,464 INFO [Executor task launch worker for task 108] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,465 WARN [Executor task launch worker for task 108] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,465 INFO [Executor task launch worker for task 108] o.a.s.e.Executor [Logging.scala:54] Finished task 3.0 in stage 70.0 (TID 108). 2708 bytes result sent to driver
2018-10-11 21:36:35,466 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 4.0 in stage 70.0 (TID 109, localhost, executor driver, partition 4, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:35,466 INFO [task-result-getter-0] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 3.0 in stage 70.0 (TID 108) in 139 ms on localhost (executor driver) (4/8)
2018-10-11 21:36:35,466 INFO [Executor task launch worker for task 109] o.a.s.e.Executor [Logging.scala:54] Running task 4.0 in stage 70.0 (TID 109)
2018-10-11 21:36:35,468 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ad60e729-defb-4a4e-99d9-9ca74fcf6a2d-c000.snappy.parquet, range: 0-11378, partition values: [empty row]
2018-10-11 21:36:35,469 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,471 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,472 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,472 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,473 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,473 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0cf40412-95c9-405c-9299-90b8afa2a414-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,474 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,475 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,475 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,476 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,476 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,476 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-8f84cc33-f1d5-478f-87ce-0663d825408c-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,477 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,478 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,478 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,479 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,479 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,479 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64602a48-9868-4fdb-bbce-e23fdb1ff4ad-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,480 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,481 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,482 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,482 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,482 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,482 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-22885ce6-f042-455f-87e9-76587998ab27-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,483 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,484 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,485 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,485 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,485 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,486 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4f0d15a7-be71-41c6-9bcc-c559eb6c8a2a-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,487 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,488 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,489 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,490 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,490 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,490 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1d69df48-f907-4504-ad20-f42db26b5389-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,491 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,492 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,493 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,493 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,493 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,493 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3bf97aa1-dd22-42f0-8f17-b66b1cd270a5-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,494 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,496 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,496 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,497 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,497 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,497 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-2f7c556f-9946-4dc8-99ba-e17d1301b602-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,498 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,499 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,500 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,500 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,500 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,500 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-57c3815d-7209-4ed6-90cc-5597db2fc409-c000.snappy.parquet, range: 0-11377, partition values: [empty row]
2018-10-11 21:36:35,501 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,502 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,503 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,503 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,504 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,504 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-fff88882-0587-48ef-95f1-5dd0508833d6-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,504 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,506 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,507 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,507 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,507 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,507 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0df7b64b-c870-4b00-b1dc-59fec5d6c208-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,508 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,509 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,511 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,511 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,511 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,511 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-384bf5ec-e9b8-429f-8281-82432437fb02-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,512 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,514 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,515 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,515 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,516 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,516 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f08e80af-add7-42b2-b4ad-2a22d1ce2a56-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,516 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,517 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,518 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,518 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,519 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,519 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-615e7057-357a-4616-a738-affe25bf8b3a-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,519 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,521 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,522 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,522 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,522 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,523 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-73bd7a91-071c-42d6-a80a-c13db79d72d2-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,524 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,526 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,527 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,527 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,528 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,528 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dbf5cfde-c2ae-4ddf-bdee-0e703dcab699-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,529 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,531 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,532 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,532 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,533 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,533 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-70719782-4616-4b3d-82dd-7bc33b67022f-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,534 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,536 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,537 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,537 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,537 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,538 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-1e620604-bee6-486e-b171-860be8b8b75e-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,538 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,539 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,540 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,540 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,541 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,541 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-05b626a4-35ba-4e5f-9c93-d7f62d4d3027-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,542 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,543 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,545 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,545 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,545 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,545 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-d0ac09a5-d2c0-4844-95ed-3706f6eb6930-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,547 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,548 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,549 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,549 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,549 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,549 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e6fcf0f4-d681-401e-b5b6-7075100f24ee-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,550 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,551 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,552 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,552 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,552 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,552 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-00ef3320-f515-47a4-b26f-5b5caf94f65d-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,553 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,555 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,556 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,556 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,556 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,557 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ff4df6d7-b381-401d-987b-233a02e4d909-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,558 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,559 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,560 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,561 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,561 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,561 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f5fe52bb-22d0-4408-b9d4-9b3a37c89088-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,562 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,563 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,564 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,565 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,565 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,565 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a3db9e49-1db1-4e94-a28e-f73df6d27fa8-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,566 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,568 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,569 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,570 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,570 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,570 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-66b4c136-f4b2-4428-9331-2adf6a097511-c000.snappy.parquet, range: 0-11376, partition values: [empty row]
2018-10-11 21:36:35,571 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,572 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,573 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,574 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,574 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,574 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e3141f79-7a0f-4730-8468-e54a6c54c533-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:35,575 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,576 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,577 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,578 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,578 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,578 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-0b0c8187-cf2f-44d8-88fc-49ce4dd62874-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:35,579 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,581 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,582 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,582 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,582 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,583 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-08d62d99-4b8f-4e06-8620-bba67557d59d-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:35,583 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,584 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,585 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,585 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,586 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,586 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-64403a21-b2b7-43f7-a22d-5f7706bad082-c000.snappy.parquet, range: 0-11374, partition values: [empty row]
2018-10-11 21:36:35,586 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,588 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,589 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,589 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,590 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,590 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-c1e3a260-62f4-4aeb-8a26-82616f973030-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:35,591 INFO [Executor task launch worker for task 109] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,593 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,594 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,594 INFO [Executor task launch worker for task 109] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,594 WARN [Executor task launch worker for task 109] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,595 INFO [Executor task launch worker for task 109] o.a.s.e.Executor [Logging.scala:54] Finished task 4.0 in stage 70.0 (TID 109). 3161 bytes result sent to driver
2018-10-11 21:36:35,595 INFO [dispatcher-event-loop-0] o.a.s.s.TaskSetManager [Logging.scala:54] Starting task 5.0 in stage 70.0 (TID 110, localhost, executor driver, partition 5, PROCESS_LOCAL, 13259 bytes)
2018-10-11 21:36:35,595 INFO [Executor task launch worker for task 110] o.a.s.e.Executor [Logging.scala:54] Running task 5.0 in stage 70.0 (TID 110)
2018-10-11 21:36:35,595 INFO [task-result-getter-1] o.a.s.s.TaskSetManager [Logging.scala:54] Finished task 4.0 in stage 70.0 (TID 109) in 129 ms on localhost (executor driver) (5/8)
2018-10-11 21:36:35,596 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9bff7912-93ac-4fc7-a1db-f8e566a70655-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:35,597 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,598 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,599 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,599 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,600 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,600 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-47728647-7a80-4592-ba7c-c3ebcb1e5224-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:35,601 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,602 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,604 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,604 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,605 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,605 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-9b3bccf5-ad25-404d-9310-88a915f9b7a2-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:35,606 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,608 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,609 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,609 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,610 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,610 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-510a5951-088e-4dab-8dd0-c02c47869d79-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:35,611 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,612 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,613 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,613 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,614 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,614 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-6650061f-a0ab-42f3-be0d-e29bba951689-c000.snappy.parquet, range: 0-11373, partition values: [empty row]
2018-10-11 21:36:35,615 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,616 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,617 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,617 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,617 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,617 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a73de2ae-2a91-46fe-a61e-d06104068675-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:35,618 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,619 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,620 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,620 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,621 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,621 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-60348a41-e7a8-4c02-9dbf-a4848152bf0a-c000.snappy.parquet, range: 0-11372, partition values: [empty row]
2018-10-11 21:36:35,622 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,623 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,624 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,624 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,624 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,624 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-ca9d3388-a32e-42aa-9b9a-f2ba826d1b2e-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:35,625 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,626 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,627 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,627 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,628 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,628 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-46ee92fd-a03b-4ad7-a847-fac273921f57-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:35,628 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,629 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,630 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,630 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,631 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,631 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4317e6e3-7c07-49f4-9bea-564768382089-c000.snappy.parquet, range: 0-11371, partition values: [empty row]
2018-10-11 21:36:35,631 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,632 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,633 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,633 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,633 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,634 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-dc5fe75d-ae31-4bd0-a866-d3274b2d629b-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:35,634 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,635 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,636 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,636 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,637 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,637 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-49c1ab9d-a225-4ba8-911f-7c8138662881-c000.snappy.parquet, range: 0-11363, partition values: [empty row]
2018-10-11 21:36:35,637 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,638 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,639 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,639 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,640 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,640 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a910a7f7-d2c2-4fb9-9907-6f0a12ef5816-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,640 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,641 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,642 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,643 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,643 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,643 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-af526506-da3a-461a-b7d6-7576a7ca2dc6-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,644 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,645 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,646 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,647 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,647 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,647 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f7492e12-3b8e-43e6-b84c-e42d363116f9-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,648 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,650 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,651 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,651 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,651 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,652 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-272bed24-7d51-4fc6-9a8a-aba3fb69c4b8-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,653 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,655 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,656 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,656 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,656 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,656 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-4ecac671-fc2e-4df4-9315-bf4c516db607-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,657 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,659 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,660 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,660 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,660 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,660 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-f2c3b673-16d6-4469-a5ed-13de2cc92f10-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,661 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,663 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,664 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,664 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,664 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,665 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-5a6947b4-4ad6-4ab6-8907-a8320e284bcb-c000.snappy.parquet, range: 0-11362, partition values: [empty row]
2018-10-11 21:36:35,666 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,667 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,668 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,669 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,669 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,669 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-3a0fc732-ce1b-4da0-9cd5-162ab4573e6e-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:35,670 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,672 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,673 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,673 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,673 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,673 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-a087b5c1-75ef-4738-bcac-2ee86aa294bd-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:35,674 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,676 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,677 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,677 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,678 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,678 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-e0eae261-dec5-4554-b126-6e1829f78050-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:35,679 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,681 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,682 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,683 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,683 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,684 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-987c95c1-23dd-407a-98e6-0e238fe1473b-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:35,685 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,687 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,688 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,689 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 1 ms. row count = 1
2018-10-11 21:36:35,689 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,689 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-cb60375b-4e46-4d45-99c6-540adc7208c3-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:35,690 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.p.ParquetReadSupport [Logging.scala:54] Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional group statement {
    optional binary id (UTF8);
    optional binary text (UTF8);
  }
  required int64 startTime;
  required int64 endTime;
  optional binary description (UTF8);
  optional binary details (UTF8);
  optional binary physicalPlanDescription (UTF8);
}

Catalyst form:
StructType(StructField(statement,StructType(StructField(id,StringType,true), StructField(text,StringType,true)),true), StructField(startTime,LongType,true), StructField(endTime,LongType,true), StructField(description,StringType,true), StructField(details,StringType,true), StructField(physicalPlanDescription,StringType,true))
       
2018-10-11 21:36:35,691 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:183] RecordReader initialized will read a total of 1 records.
2018-10-11 21:36:35,693 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:124] at row 0. reading next block
2018-10-11 21:36:35,693 INFO [Executor task launch worker for task 110] o.a.p.h.InternalParquetRecordReader [InternalParquetRecordReader.java:133] block read in memory in 0 ms. row count = 1
2018-10-11 21:36:35,693 WARN [Executor task launch worker for task 110] o.a.s.s.SparkSession$Builder [Logging.scala:66] Using an existing SparkSession; some configuration may not take effect.
2018-10-11 21:36:35,693 INFO [Executor task launch worker for task 110] o.a.s.s.e.d.FileScanRDD [Logging.scala:54] Reading File path: file:///user/hive/warehouse/tpcds.db/spark_tpcds_statements/part-00000-319157ef-58f7-4406-b9b1-6baf5b53b0a8-c000.snappy.parquet, range: 0-11361, partition values: [empty row]
2018-10-11 21:36:54,694 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test' ...
2018-10-11 21:36:54,710 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:36:54,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:36:55,713 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:36:56,050 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:36:56,051 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test'.
2018-10-11 21:36:57,362 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test1' ...
2018-10-11 21:36:57,369 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:36:57,449 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:36:57,863 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:36:58,140 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:36:58,140 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test1'.
2018-10-11 21:36:59,125 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test2' ...
2018-10-11 21:36:59,131 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 21:36:59,218 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 21:36:59,487 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 21:36:59,487 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test2'.
2018-10-11 21:37:01,587 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 21:37:03,907 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 21:37:06,184 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 21:37:06,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 21:37:06,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:37:06,979 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:37:07,283 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:37:07,507 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:37:07,508 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 21:37:08,259 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 21:37:08,264 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:37:08,324 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:37:08,642 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:37:08,836 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:37:08,836 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 21:37:12,727 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 21:37:12,728 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 21:37:12,729 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,729 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,729 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,729 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,730 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,730 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,730 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,730 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,730 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,730 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,731 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,732 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,734 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,734 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,734 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,734 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,735 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,735 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,735 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,735 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,735 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,736 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,736 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,736 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,736 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,737 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,738 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,738 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,738 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,738 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,739 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,739 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,739 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,739 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,739 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,740 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,740 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:37:12,740 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,740 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,741 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,741 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:37:12,741 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,741 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,741 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,741 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,742 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:37:12,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,748 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,749 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,750 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,751 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,752 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,753 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,754 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,755 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,758 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,759 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:12,760 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:37:12,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:37:12,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:37:12,761 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:37:13,478 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test5' ...
2018-10-11 21:37:13,487 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:37:13,566 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:37:14,055 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:37:14,264 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:37:14,265 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test5'.
2018-10-11 21:37:14,974 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test6' ...
2018-10-11 21:37:14,980 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 21:37:15,031 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 21:37:15,626 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 21:37:15,626 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test6'.
2018-10-11 21:37:18,359 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 21:37:18,360 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:60]   No result found in run 2.
2018-10-11 21:37:18,360 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 2
2018-10-11 21:37:18,360 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:59]   No result found in run 1.
2018-10-11 21:38:13,703 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test3' does not exist.
2018-10-11 21:38:13,707 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 21:38:14,716 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test3'.
2018-10-11 21:38:14,716 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 21:38:14,733 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:38:14,734 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 21:38:14,735 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:38:14,736 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 21:38:14,980 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 21:38:14,981 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:38:14,982 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:38:14,982 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:38:14,982 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:29] Statement 'test3.1.0' finished.
2018-10-11 21:38:14,986 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:37] Remove listener.
2018-10-11 21:38:14,991 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:41] Saving statement 'test3.1.0' result ...
2018-10-11 21:38:15,440 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:43] Saved statement 'test3.1.0' result.
2018-10-11 21:38:15,440 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 21:38:15,441 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:38:15,441 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 21:38:15,781 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 21:38:15,782 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:38:15,782 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:38:15,783 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:38:15,783 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:29] Statement 'test3.1.1' finished.
2018-10-11 21:38:15,783 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:37] Remove listener.
2018-10-11 21:38:15,783 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:41] Saving statement 'test3.1.1' result ...
2018-10-11 21:38:16,158 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:43] Saved statement 'test3.1.1' result.
2018-10-11 21:38:16,158 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:38:16,159 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 21:38:16,596 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test4' does not exist.
2018-10-11 21:38:16,596 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 21:38:17,302 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test4'.
2018-10-11 21:38:17,302 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 21:38:17,309 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:38:17,309 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 21:38:17,310 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:38:17,310 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 21:38:17,395 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 21:38:17,395 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:38:17,395 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:38:17,396 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:38:17,396 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:29] Statement 'test4.1.0' finished.
2018-10-11 21:38:17,396 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:37] Remove listener.
2018-10-11 21:38:17,397 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:41] Saving statement 'test4.1.0' result ...
2018-10-11 21:38:17,686 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:43] Saved statement 'test4.1.0' result.
2018-10-11 21:38:17,687 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 21:38:17,687 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:38:17,687 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 21:38:17,832 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 21:38:17,832 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:38:17,832 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:38:17,832 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:38:17,833 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:29] Statement 'test4.1.1' finished.
2018-10-11 21:38:17,833 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:37] Remove listener.
2018-10-11 21:38:17,833 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:41] Saving statement 'test4.1.1' result ...
2018-10-11 21:38:18,129 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:43] Saved statement 'test4.1.1' result.
2018-10-11 21:38:18,130 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:38:18,130 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 21:38:18,225 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test3'
2018-10-11 21:38:18,619 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 21:38:21,396 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test4'
2018-10-11 21:38:21,653 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 21:38:23,916 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 21:38:23,917 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 21:38:23,918 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,918 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,918 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,918 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,918 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,918 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,919 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,920 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,921 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,922 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,922 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,922 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,922 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,923 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,924 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,925 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,926 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:38:23,927 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,928 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,930 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,931 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,932 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,932 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,932 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,932 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,932 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,933 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,933 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,933 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,933 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,934 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,934 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,934 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 21:38:23,934 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,934 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,935 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,936 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,936 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,936 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,937 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,937 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,937 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,937 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,937 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,938 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,938 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,938 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,938 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,938 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,939 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,940 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,941 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,941 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,941 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,941 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,941 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,942 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,942 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,942 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,942 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,943 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,943 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,943 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,943 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,943 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,944 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,944 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,944 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,944 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,945 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,945 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,945 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,945 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,945 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:38:23,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,946 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 21:38:23,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,947 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,948 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,949 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,950 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,950 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,950 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,950 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,950 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,951 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,951 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,951 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,951 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,951 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,952 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,952 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,952 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,953 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,954 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,954 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,954 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,955 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,955 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,955 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,955 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,956 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,956 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:38:23,956 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 21:38:23,956 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 21:38:23,957 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 21:38:23,957 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 21:43:24,536 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test3' does not exist.
2018-10-11 21:43:24,541 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 21:44:36,992 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test3'.
2018-10-11 21:44:36,992 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 21:44:37,034 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:44:45,992 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 21:44:45,993 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:44:45,993 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 21:44:46,258 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 21:44:46,258 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:44:46,259 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:44:46,260 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:44:46,265 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:44:46,269 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 21:44:46,855 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 21:44:48,125 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 21:44:48,126 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:44:48,127 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 21:44:48,450 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 21:44:48,451 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:44:48,451 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:44:48,452 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:44:48,452 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:44:48,452 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 21:44:50,342 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 21:44:50,343 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:44:50,343 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 21:44:50,865 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test4' does not exist.
2018-10-11 21:44:50,865 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 21:44:53,338 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test4'.
2018-10-11 21:44:53,338 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 21:44:53,347 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:44:55,874 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 21:44:55,875 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:44:55,875 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 21:44:55,966 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 21:44:55,966 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:44:55,966 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:44:55,967 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:44:55,967 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:44:55,967 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 21:44:57,060 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 21:44:58,157 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 21:44:58,157 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:44:58,158 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 21:44:58,318 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 21:44:58,319 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:44:58,319 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:44:58,319 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:44:58,320 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:44:58,320 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 21:44:58,674 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 21:44:58,674 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:44:58,675 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 21:44:58,789 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test3'
2018-10-11 21:44:59,236 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 21:45:02,535 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test4'
2018-10-11 21:45:02,796 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 21:45:05,654 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 21:45:05,656 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 21:50:01,243 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test3' does not exist.
2018-10-11 21:50:01,248 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 21:50:06,259 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test3'.
2018-10-11 21:50:06,259 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 21:50:06,273 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:50:37,107 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 21:50:37,108 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:50:37,109 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 21:50:37,396 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 21:50:37,396 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:50:37,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:50:37,398 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:50:37,402 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:50:37,406 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 21:50:37,889 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 21:50:38,893 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 21:50:38,894 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:50:38,895 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 21:50:39,303 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 21:50:39,304 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:50:39,305 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:50:39,306 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:50:39,307 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:50:39,308 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 21:50:39,635 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 21:50:39,636 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:50:39,637 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 21:50:40,169 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test4' does not exist.
2018-10-11 21:50:40,170 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 21:50:40,957 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test4'.
2018-10-11 21:50:40,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 21:50:40,968 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:50:44,130 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 21:50:44,130 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:50:44,130 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 21:50:44,219 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 21:50:44,219 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:50:44,219 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:50:44,220 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:50:44,220 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:50:44,220 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 21:50:44,537 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 21:50:45,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 21:50:45,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:50:45,298 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 21:50:45,468 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 21:50:45,468 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:50:45,469 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:50:45,469 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:50:45,469 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:50:45,470 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 21:50:45,790 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 21:50:45,790 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:50:45,791 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 21:50:49,315 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test3'
2018-10-11 21:51:09,362 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 21:54:17,612 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test3' does not exist.
2018-10-11 21:54:17,615 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 21:54:18,626 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test3'.
2018-10-11 21:54:18,626 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 21:54:18,641 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:54:25,499 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 21:54:25,500 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:54:25,501 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 21:54:25,781 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 21:54:25,782 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:54:25,783 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:54:25,784 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:54:25,788 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:54:25,793 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 21:54:26,836 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 21:54:26,837 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 21:54:26,837 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:54:26,837 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 21:54:27,191 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 21:54:27,192 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:54:27,192 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:54:27,193 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:54:27,193 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:54:27,193 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 21:54:27,601 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 21:54:27,601 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:54:27,602 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 21:54:28,235 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test4' does not exist.
2018-10-11 21:54:28,267 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 21:54:29,138 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test4'.
2018-10-11 21:54:29,138 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 21:54:29,146 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 21:54:29,146 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 21:54:29,146 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:54:29,146 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 21:54:29,247 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 21:54:29,247 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:54:29,247 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 21:54:29,248 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:54:29,248 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:54:29,248 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 21:54:30,000 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 21:54:30,001 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 21:54:30,001 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 21:54:30,001 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 21:54:30,143 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 21:54:30,144 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 21:54:30,144 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 21:54:30,144 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 21:54:30,144 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 21:54:30,145 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 21:54:30,427 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 21:54:30,427 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 21:54:30,428 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 21:54:57,179 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test3'
2018-10-11 21:57:39,171 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 22:04:11,990 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test3' does not exist.
2018-10-11 22:04:12,015 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 22:04:13,124 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test3'.
2018-10-11 22:04:13,124 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 22:04:13,168 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:04:13,182 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 22:04:13,183 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:04:13,184 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 22:04:13,472 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 22:04:13,472 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:04:13,491 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:04:13,492 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:04:13,507 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:04:13,510 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 22:04:14,031 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 22:04:14,032 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 22:04:14,032 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:04:14,032 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 22:04:14,547 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 22:04:14,548 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:04:14,548 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:04:14,549 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:04:14,549 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:04:14,550 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 22:04:15,026 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 22:04:15,026 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:04:15,027 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 22:04:15,564 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test4' does not exist.
2018-10-11 22:04:15,565 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 22:04:16,174 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test4'.
2018-10-11 22:04:16,174 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 22:04:16,183 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:04:16,184 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 22:04:16,184 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:04:16,184 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 22:04:16,273 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 22:04:16,274 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:04:16,274 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:04:16,275 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:04:16,275 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:04:16,275 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 22:04:16,583 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 22:04:16,583 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 22:04:16,583 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:04:16,584 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 22:04:16,722 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 22:04:16,722 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:04:16,722 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:04:16,722 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:04:16,723 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:04:16,723 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 22:04:17,042 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 22:04:17,042 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:04:17,043 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 22:04:17,127 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test3'
2018-10-11 22:04:17,429 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 22:04:22,669 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test4'
2018-10-11 22:04:22,906 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 22:04:25,366 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:04:25,378 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 22:04:25,379 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,380 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,380 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,381 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,382 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,382 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,382 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,382 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,382 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,383 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,384 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,385 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,386 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,387 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,387 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,387 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,387 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,387 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,388 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,388 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,389 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,389 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,389 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,389 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,389 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,390 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,391 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,392 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,393 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,393 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,393 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 22:04:25,394 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,395 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,395 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,395 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 22:04:25,395 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,395 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,396 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,397 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,398 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,399 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,400 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,401 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,402 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,403 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,404 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,404 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,404 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,404 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,404 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,404 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,405 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,405 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,405 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,405 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,405 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,407 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,407 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,407 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,407 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,407 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,408 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,408 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,408 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,408 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,408 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,409 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,409 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,409 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,409 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,410 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,410 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,410 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,410 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,411 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,411 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,411 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,411 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,411 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,412 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,412 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,412 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,412 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,412 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 22:04:25,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,413 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,414 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,414 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,414 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,414 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,414 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : false
2018-10-11 22:04:25,415 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,415 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,415 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,415 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,415 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,416 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,416 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,416 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,416 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,416 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,417 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,418 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,418 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,418 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,418 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,418 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,419 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,419 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,419 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,419 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,421 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,421 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,421 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,421 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,422 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,423 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,424 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,424 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,424 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,424 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,424 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,424 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:04:25,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:04:25,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:04:25,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:04:25,425 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:06:08,062 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test3' does not exist.
2018-10-11 22:06:08,065 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 22:06:09,330 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test3'.
2018-10-11 22:06:09,330 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test3' ...
2018-10-11 22:06:09,354 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:06:09,368 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 22:06:09,369 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:06:09,370 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 22:06:09,671 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 22:06:09,672 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:06:09,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:06:09,685 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:06:09,700 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:06:09,705 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 22:06:10,091 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 22:06:10,091 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 22:06:10,092 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:06:10,092 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 22:06:10,593 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 22:06:10,594 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:06:10,595 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:06:10,595 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:06:10,595 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:06:10,596 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 22:06:11,034 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 22:06:11,035 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:06:11,035 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test3'.
2018-10-11 22:06:11,855 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:16] The run 'test4' does not exist.
2018-10-11 22:06:11,855 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 22:06:12,921 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:25] Saved run 'test4'.
2018-10-11 22:06:12,922 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:27] Start run 'test4' ...
2018-10-11 22:06:12,929 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:06:12,930 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 22:06:12,930 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:06:12,930 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 22:06:13,020 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 22:06:13,020 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:06:13,020 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:06:13,021 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:06:13,021 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:06:13,021 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 22:06:13,756 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 22:06:13,757 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 22:06:13,757 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:06:13,757 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 22:06:13,903 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 22:06:13,903 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:06:13,904 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:06:13,904 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:06:13,904 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:06:13,905 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 22:06:14,196 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 22:06:14,196 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:06:14,197 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:30] Finished run 'test4'.
2018-10-11 22:06:14,282 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test3'
2018-10-11 22:06:14,612 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 22:06:15,121 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:65] Filter run 'test4'
2018-10-11 22:06:15,434 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:70] Get query results
2018-10-11 22:06:15,743 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:06:15,744 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 22:06:15,745 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:06:15,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:06:15,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:06:15,746 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:06:15,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:06:15,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:06:15,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:06:15,747 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:20:39,482 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 22:20:44,984 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test3'.
2018-10-11 22:20:44,985 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test3' ...
2018-10-11 22:20:45,053 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:20:45,063 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 22:20:45,074 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:20:45,075 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 22:20:46,004 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 22:20:46,005 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:20:46,030 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:20:46,030 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:20:46,079 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:20:46,089 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 22:20:46,729 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 22:20:46,729 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 22:20:46,729 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:20:46,730 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 22:20:47,327 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 22:20:47,328 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:20:47,328 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:20:47,328 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:20:47,329 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:20:47,329 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 22:20:47,889 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 22:20:47,890 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:20:47,890 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test3'.
2018-10-11 22:20:47,970 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 22:20:48,784 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test4'.
2018-10-11 22:20:48,784 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test4' ...
2018-10-11 22:20:48,791 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:20:48,792 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 22:20:48,792 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:20:48,792 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 22:20:48,859 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 22:20:48,859 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:20:48,859 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:20:48,859 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:20:48,859 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:20:48,860 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 22:20:49,253 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 22:20:49,253 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 22:20:49,254 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:20:49,254 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 22:20:49,378 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 22:20:49,378 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:20:49,379 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:20:49,379 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:20:49,379 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:20:49,379 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 22:20:49,762 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 22:20:49,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:20:49,762 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test4'.
2018-10-11 22:20:49,826 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test3'
2018-10-11 22:20:50,125 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:20:50,399 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test4'
2018-10-11 22:20:51,533 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:20:52,504 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:20:52,516 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 22:20:52,517 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:20:52,517 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:20:52,517 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:20:52,517 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:20:52,517 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:20:52,518 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:20:52,518 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:20:52,518 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:23:01,345 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test' ...
2018-10-11 22:23:08,690 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test'.
2018-10-11 22:23:08,691 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test' ...
2018-10-11 22:23:09,274 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:23:09,317 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test.1.0'.
2018-10-11 22:23:09,379 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:09,379 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test.1.0' ...
2018-10-11 22:23:13,556 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test.1.0' finished.
2018-10-11 22:23:13,557 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:13,693 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:23:13,694 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:14,663 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:14,731 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test.1.0' result ...
2018-10-11 22:23:15,709 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test.1.0' result.
2018-10-11 22:23:15,710 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test.1.1'.
2018-10-11 22:23:15,710 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:15,710 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test.1.1' ...
2018-10-11 22:23:17,131 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test.1.1' finished.
2018-10-11 22:23:17,131 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:17,132 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:23:17,132 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:17,132 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:17,133 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test.1.1' result ...
2018-10-11 22:23:17,878 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test.1.1' result.
2018-10-11 22:23:17,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:23:17,878 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test'.
2018-10-11 22:23:18,690 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test1' ...
2018-10-11 22:23:19,290 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test1'.
2018-10-11 22:23:19,290 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test1' ...
2018-10-11 22:23:19,296 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:23:19,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test1.1.0'.
2018-10-11 22:23:19,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:19,297 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test1.1.0' ...
2018-10-11 22:23:19,366 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test1.1.0' finished.
2018-10-11 22:23:19,367 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:19,367 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:23:19,368 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:19,368 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:19,368 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test1.1.0' result ...
2018-10-11 22:23:19,740 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test1.1.0' result.
2018-10-11 22:23:19,740 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test1.1.1'.
2018-10-11 22:23:19,741 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:19,741 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test1.1.1' ...
2018-10-11 22:23:19,853 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test1.1.1' finished.
2018-10-11 22:23:19,853 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:19,853 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:23:19,853 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:19,854 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:19,854 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test1.1.1' result ...
2018-10-11 22:23:20,350 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test1.1.1' result.
2018-10-11 22:23:20,351 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:23:20,351 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test1'.
2018-10-11 22:23:20,446 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test2' ...
2018-10-11 22:23:21,067 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test2'.
2018-10-11 22:23:21,068 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test2' ...
2018-10-11 22:23:21,075 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 22:23:21,075 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test2.2.0'.
2018-10-11 22:23:21,075 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:21,075 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test2.2.0' ...
2018-10-11 22:23:21,227 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test2.2.0' finished.
2018-10-11 22:23:21,227 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:21,227 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 22:23:21,228 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:21,228 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:21,228 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test2.2.0' result ...
2018-10-11 22:23:21,644 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test2.2.0' result.
2018-10-11 22:23:21,644 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 22:23:21,644 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test2'.
2018-10-11 22:23:22,259 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test2'
2018-10-11 22:23:23,700 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:24,032 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 22:23:24,033 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test1'
2018-10-11 22:23:24,286 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:24,616 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 22:23:24,616 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test'
2018-10-11 22:23:24,864 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:25,144 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] %-25s %25s

2018-10-11 22:23:25,232 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 22:23:25,756 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test3'.
2018-10-11 22:23:25,756 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test3' ...
2018-10-11 22:23:25,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:23:25,765 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 22:23:25,765 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:25,765 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 22:23:25,847 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 22:23:25,848 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:25,848 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:23:25,848 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:25,848 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:25,848 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 22:23:26,214 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 22:23:26,214 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 22:23:26,215 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:26,215 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 22:23:26,333 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 22:23:26,334 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:26,334 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:23:26,334 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:26,335 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:26,335 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 22:23:29,951 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 22:23:29,952 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:23:29,976 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test3'.
2018-10-11 22:23:30,066 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 22:23:31,588 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test4'.
2018-10-11 22:23:31,588 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test4' ...
2018-10-11 22:23:31,595 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:23:31,596 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 22:23:31,596 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:31,596 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 22:23:31,717 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 22:23:31,717 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:31,718 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:23:31,718 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:31,718 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:31,719 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 22:23:34,271 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 22:23:34,272 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 22:23:34,272 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:34,273 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 22:23:34,697 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 22:23:34,697 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:34,698 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:23:34,698 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:34,698 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:34,698 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 22:23:36,164 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 22:23:36,165 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:23:36,165 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test4'.
2018-10-11 22:23:36,222 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test3'
2018-10-11 22:23:36,433 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:36,676 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test4'
2018-10-11 22:23:36,883 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:37,227 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:23:37,281 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 22:23:37,419 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:23:37,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:23:37,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:23:37,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:23:37,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:23:37,420 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:23:37,421 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:23:37,421 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:23:37,474 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test5' ...
2018-10-11 22:23:38,592 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test5'.
2018-10-11 22:23:38,593 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test5' ...
2018-10-11 22:23:38,598 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:23:38,599 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test5.1.0'.
2018-10-11 22:23:38,599 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:38,599 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test5.1.0' ...
2018-10-11 22:23:38,656 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test5.1.0' finished.
2018-10-11 22:23:38,656 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:38,657 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:23:38,657 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:38,657 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:38,657 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test5.1.0' result ...
2018-10-11 22:23:39,920 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test5.1.0' result.
2018-10-11 22:23:39,920 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test5.1.1'.
2018-10-11 22:23:39,920 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:39,920 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test5.1.1' ...
2018-10-11 22:23:40,033 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test5.1.1' finished.
2018-10-11 22:23:40,033 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:40,033 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:23:40,033 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:40,033 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:40,034 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test5.1.1' result ...
2018-10-11 22:23:41,222 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test5.1.1' result.
2018-10-11 22:23:41,223 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:23:41,276 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test5'.
2018-10-11 22:23:41,589 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test6' ...
2018-10-11 22:23:42,557 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test6'.
2018-10-11 22:23:42,557 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test6' ...
2018-10-11 22:23:42,564 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 22:23:42,565 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test6.2.0'.
2018-10-11 22:23:42,565 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:23:42,565 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test6.2.0' ...
2018-10-11 22:23:42,660 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test6.2.0' finished.
2018-10-11 22:23:42,661 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:23:42,661 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 22:23:42,661 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:23:42,661 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:23:42,662 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test6.2.0' result ...
2018-10-11 22:23:42,958 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test6.2.0' result.
2018-10-11 22:23:42,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 22:23:42,958 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test6'.
2018-10-11 22:23:43,020 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test5'
2018-10-11 22:23:43,280 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:43,527 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test6'
2018-10-11 22:23:43,830 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:23:43,968 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:23:43,969 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:60]   No result found in run 2.
2018-10-11 22:23:43,969 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 2
2018-10-11 22:23:43,969 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:59]   No result found in run 1.
2018-10-11 22:32:34,613 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test' ...
2018-10-11 22:32:37,976 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test'.
2018-10-11 22:32:37,977 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test' ...
2018-10-11 22:32:38,025 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:32:38,035 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test.1.0'.
2018-10-11 22:32:38,047 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:38,047 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test.1.0' ...
2018-10-11 22:32:38,573 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test.1.0' finished.
2018-10-11 22:32:38,574 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:38,644 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:32:38,644 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:38,658 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:38,660 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test.1.0' result ...
2018-10-11 22:32:39,236 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test.1.0' result.
2018-10-11 22:32:39,237 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test.1.1'.
2018-10-11 22:32:39,237 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:39,237 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test.1.1' ...
2018-10-11 22:32:39,690 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test.1.1' finished.
2018-10-11 22:32:39,691 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:39,691 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:32:39,691 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:39,691 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:39,692 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test.1.1' result ...
2018-10-11 22:32:40,222 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test.1.1' result.
2018-10-11 22:32:40,222 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:32:40,222 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test'.
2018-10-11 22:32:40,657 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test1' ...
2018-10-11 22:32:41,290 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test1'.
2018-10-11 22:32:41,291 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test1' ...
2018-10-11 22:32:41,296 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:32:41,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test1.1.0'.
2018-10-11 22:32:41,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:41,297 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test1.1.0' ...
2018-10-11 22:32:41,370 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test1.1.0' finished.
2018-10-11 22:32:41,371 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:41,371 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:32:41,372 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:41,372 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:41,372 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test1.1.0' result ...
2018-10-11 22:32:41,743 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test1.1.0' result.
2018-10-11 22:32:41,743 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test1.1.1'.
2018-10-11 22:32:41,744 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:41,744 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test1.1.1' ...
2018-10-11 22:32:41,856 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test1.1.1' finished.
2018-10-11 22:32:41,856 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:41,857 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:32:41,857 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:41,857 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:41,857 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test1.1.1' result ...
2018-10-11 22:32:42,219 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test1.1.1' result.
2018-10-11 22:32:42,219 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:32:42,220 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test1'.
2018-10-11 22:32:42,313 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test2' ...
2018-10-11 22:32:42,803 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test2'.
2018-10-11 22:32:42,803 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test2' ...
2018-10-11 22:32:42,810 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 22:32:42,811 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test2.2.0'.
2018-10-11 22:32:42,811 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:42,811 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test2.2.0' ...
2018-10-11 22:32:42,882 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test2.2.0' finished.
2018-10-11 22:32:42,882 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:42,883 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 22:32:42,883 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:42,883 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:42,884 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test2.2.0' result ...
2018-10-11 22:32:43,288 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test2.2.0' result.
2018-10-11 22:32:43,288 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 22:32:43,288 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test2'.
2018-10-11 22:32:43,696 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test2'
2018-10-11 22:32:43,951 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:44,132 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] test2                           11/10/2018 10:32:42
2018-10-11 22:32:44,133 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test1'
2018-10-11 22:32:44,369 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:44,703 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] test1                           11/10/2018 10:32:40
2018-10-11 22:32:44,704 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test'
2018-10-11 22:32:44,974 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:45,291 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:38] test                            11/10/2018 10:32:34
2018-10-11 22:32:45,355 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test3' ...
2018-10-11 22:32:45,899 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test3'.
2018-10-11 22:32:45,899 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test3' ...
2018-10-11 22:32:45,908 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:32:45,908 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.0'.
2018-10-11 22:32:45,909 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:45,909 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.0' ...
2018-10-11 22:32:45,990 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.0' finished.
2018-10-11 22:32:45,991 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:45,991 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:32:45,991 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:45,992 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:45,992 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.0' result ...
2018-10-11 22:32:46,291 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.0' result.
2018-10-11 22:32:46,292 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test3.1.1'.
2018-10-11 22:32:46,292 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:46,292 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test3.1.1' ...
2018-10-11 22:32:46,405 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test3.1.1' finished.
2018-10-11 22:32:46,406 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:46,406 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:32:46,406 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:46,407 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:46,407 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test3.1.1' result ...
2018-10-11 22:32:46,726 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test3.1.1' result.
2018-10-11 22:32:46,726 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:32:46,726 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test3'.
2018-10-11 22:32:46,783 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test4' ...
2018-10-11 22:32:47,284 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test4'.
2018-10-11 22:32:47,285 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test4' ...
2018-10-11 22:32:47,290 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:32:47,290 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.0'.
2018-10-11 22:32:47,290 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:47,291 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.0' ...
2018-10-11 22:32:47,343 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.0' finished.
2018-10-11 22:32:47,343 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:47,344 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:32:47,344 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:47,344 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:47,344 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.0' result ...
2018-10-11 22:32:47,704 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.0' result.
2018-10-11 22:32:47,704 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test4.1.1'.
2018-10-11 22:32:47,705 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:47,705 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test4.1.1' ...
2018-10-11 22:32:47,804 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test4.1.1' finished.
2018-10-11 22:32:47,804 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:47,805 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:32:47,805 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:47,805 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:47,806 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test4.1.1' result ...
2018-10-11 22:32:48,180 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test4.1.1' result.
2018-10-11 22:32:48,180 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:32:48,180 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test4'.
2018-10-11 22:32:48,237 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test3'
2018-10-11 22:32:48,476 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:48,730 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test4'
2018-10-11 22:32:48,971 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:49,213 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:32:49,222 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:62]   Elapsed time matches               : false
2018-10-11 22:32:49,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.0' and 'test4.1.0'
2018-10-11 22:32:49,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:32:49,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:32:49,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:32:49,230 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:65]   Statement 'test3.1.1' and 'test4.1.1'
2018-10-11 22:32:49,231 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:66]     Description matches              : true
2018-10-11 22:32:49,231 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:67]     Details matches                  : false
2018-10-11 22:32:49,231 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:68]     Physical plan description matches: false
2018-10-11 22:32:49,293 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test5' ...
2018-10-11 22:32:50,757 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test5'.
2018-10-11 22:32:50,757 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test5' ...
2018-10-11 22:32:50,764 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '1' ... 
2018-10-11 22:32:50,764 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test5.1.0'.
2018-10-11 22:32:50,764 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:50,765 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test5.1.0' ...
2018-10-11 22:32:50,830 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test5.1.0' finished.
2018-10-11 22:32:50,831 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:50,831 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 99
2018-10-11 22:32:50,831 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:50,831 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:50,832 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test5.1.0' result ...
2018-10-11 22:32:51,192 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test5.1.0' result.
2018-10-11 22:32:51,192 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test5.1.1'.
2018-10-11 22:32:51,192 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:51,192 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test5.1.1' ...
2018-10-11 22:32:51,296 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test5.1.1' finished.
2018-10-11 22:32:51,296 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:51,296 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9801
2018-10-11 22:32:51,297 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:51,297 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:51,297 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test5.1.1' result ...
2018-10-11 22:32:51,617 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test5.1.1' result.
2018-10-11 22:32:51,617 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '1'.
2018-10-11 22:32:51,617 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test5'.
2018-10-11 22:32:51,705 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:23] Saving run 'test6' ...
2018-10-11 22:32:52,127 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:26] Saved run 'test6'.
2018-10-11 22:32:52,127 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:28] Start run 'test6' ...
2018-10-11 22:32:52,133 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:17] Run query '2' ... 
2018-10-11 22:32:52,134 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:10] Create listener for statement 'test6.2.0'.
2018-10-11 22:32:52,134 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:14] Add listener.
2018-10-11 22:32:52,134 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:16] Execute statement 'test6.2.0' ...
2018-10-11 22:32:52,228 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:25] Statement 'test6.2.0' finished.
2018-10-11 22:32:52,228 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:26] Print result:
2018-10-11 22:32:52,228 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:27] 9999
2018-10-11 22:32:52,228 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:28] Result count is '1'
2018-10-11 22:32:52,229 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:36] Remove listener.
2018-10-11 22:32:52,229 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:40] Saving statement 'test6.2.0' result ...
2018-10-11 22:32:52,662 DEBUG [ScalaTest-run-running-SparkTPCDSTest] o.a.t.s.Statement [Statement.scala:42] Saved statement 'test6.2.0' result.
2018-10-11 22:32:52,663 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.q.Query [Query.scala:19] Finished query '2'.
2018-10-11 22:32:52,663 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.r.Run [Run.scala:31] Finished run 'test6'.
2018-10-11 22:32:52,722 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test5'
2018-10-11 22:32:53,005 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:53,295 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:67] Filter run 'test6'
2018-10-11 22:32:53,570 TRACE [ScalaTest-run-running-SparkTPCDSTest] o.a.t.d.SparkDataManager [SparkDataManager.scala:72] Get query results
2018-10-11 22:32:53,684 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 1
2018-10-11 22:32:53,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:60]   No result found in run 2.
2018-10-11 22:32:53,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:56] Query 2
2018-10-11 22:32:53,685 INFO [ScalaTest-run-running-SparkTPCDSTest] o.a.t.SparkTPCDS$ [SparkTPCDS.scala:59]   No result found in run 1.
